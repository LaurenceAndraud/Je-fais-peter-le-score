{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "649ce345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "from features_functions import compute_features\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from scipy.io import wavfile\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c892235b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the paths to the files \n",
    "data_path = \"Data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "79c9e363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of the classes\n",
    "classes_paths = [\"Cars/\", \"Trucks/\"]\n",
    "classes_names = [\"car\", \"truck\"]\n",
    "cars_list = [4,5,7,9,10,15,20,21,23,26,30,38,39,44,46,48,51,52,53,57] # ne pas changer\n",
    "trucks_list = [2,4,10,11,13,20,22,25,27,30,31,32,33,35,36,39,40,45,47,48] # ne pas changer\n",
    "nbr_of_sigs = 20 # Nbr of sigs in each class, Ã  ne pas changer\n",
    "seq_length = 0.6 # Nbr of second of signal for one sequence\n",
    "nbr_of_obs = int(nbr_of_sigs*10/seq_length) # Each signal is 10 s long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "72787277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to search for the files\n",
    "learning_labels = []\n",
    "for i in range(2*nbr_of_sigs):\n",
    "    if i < nbr_of_sigs:\n",
    "        name = f\"{classes_names[0]}{cars_list[i]}.wav\"\n",
    "        class_path = classes_paths[0]\n",
    "    else:\n",
    "        name = f\"{classes_names[1]}{trucks_list[i - nbr_of_sigs]}.wav\"\n",
    "        class_path = classes_paths[1]\n",
    "\n",
    "    # Read the data and scale them between -1 and 1\n",
    "    fs, data = sio.wavfile.read(data_path + class_path + name)\n",
    "    data = data.astype(float)\n",
    "    data = data/32768\n",
    "\n",
    "    # Cut the data into sequences (we take off the last bits)\n",
    "    data_length = data.shape[0]\n",
    "    nbr_blocks = int((data_length/fs)/seq_length)\n",
    "    seqs = data[:int(nbr_blocks*seq_length*fs)].reshape((nbr_blocks, int(seq_length*fs)))\n",
    "\n",
    "    for k_seq, seq in enumerate(seqs):\n",
    "        # Compute the signal in three domains\n",
    "        sig_sq = seq**2\n",
    "        sig_t = seq / np.sqrt(sig_sq.sum())\n",
    "        sig_f = np.absolute(np.fft.fft(sig_t))\n",
    "        sig_c = np.absolute(np.fft.fft(sig_f))\n",
    "\n",
    "        # Compute the features and store them\n",
    "        features_list = []\n",
    "        N_feat, features_list = compute_features(sig_t, sig_f[:sig_t.shape[0]//2], sig_c[:sig_t.shape[0]//2], fs)\n",
    "        features_vector = np.array(features_list)[np.newaxis,:]\n",
    "\n",
    "        if k_seq == 0 and i == 0:\n",
    "            learning_features = features_vector\n",
    "            learning_labels.append(classes_names[0])\n",
    "        elif i < nbr_of_sigs:\n",
    "            learning_features = np.vstack((learning_features, features_vector))\n",
    "            learning_labels.append(classes_names[0])\n",
    "        else:\n",
    "            learning_features = np.vstack((learning_features, features_vector))\n",
    "            learning_labels.append(classes_names[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "db51294f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1983, 71)\n",
      "1983\n"
     ]
    }
   ],
   "source": [
    "print(learning_features.shape)\n",
    "print(len(learning_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "44359312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive Bayes: 0.6120906801007556\n",
      "Accuracy of Logistic Regression: 0.7380352644836272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree: 0.7355163727959698\n",
      "Accuracy of Random Forest: 0.8664987405541562\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Separate data in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(learning_features, learning_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the labels\n",
    "labelEncoder = preprocessing.LabelEncoder().fit(y_train)\n",
    "learningLabelsStd = labelEncoder.transform(y_train)\n",
    "testLabelsStd = labelEncoder.transform(y_test)\n",
    "\n",
    "# Learn the model\n",
    "models = [GaussianNB(), LogisticRegression(), DecisionTreeClassifier(), RandomForestClassifier()]\n",
    "model_names = ['Naive Bayes', 'Logistic Regression', 'Decision Tree', 'Random Forest']\n",
    "\n",
    "for model, model_name in zip(models, model_names):\n",
    "    clf = model.fit(learningFeatures_scaled, learningLabelsStd)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    scaler = preprocessing.StandardScaler(with_mean=True).fit(X_train)\n",
    "    learningFeatures_scaled = scaler.transform(X_train)\n",
    "    model.fit(learningFeatures_scaled, learningLabelsStd)\n",
    "# Test the model\n",
    "    testFeatures_scaled = scaler.transform(X_test)\n",
    "    accuracy = model.score(testFeatures_scaled, testLabelsStd)\n",
    "    #accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy of {model_name}:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b7de2ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.62899356\n",
      "Iteration 2, loss = 0.53768392\n",
      "Iteration 3, loss = 0.49896909\n",
      "Iteration 4, loss = 0.45783498\n",
      "Iteration 5, loss = 0.41100026\n",
      "Iteration 6, loss = 0.39011391\n",
      "Iteration 7, loss = 0.38050009\n",
      "Iteration 8, loss = 0.33857277\n",
      "Iteration 9, loss = 0.32694607\n",
      "Iteration 10, loss = 0.36854084\n",
      "Iteration 11, loss = 0.28813745\n",
      "Iteration 12, loss = 0.28569674\n",
      "Iteration 13, loss = 0.23632204\n",
      "Iteration 14, loss = 0.23096080\n",
      "Iteration 15, loss = 0.20520117\n",
      "Iteration 16, loss = 0.19863932\n",
      "Iteration 17, loss = 0.24598938\n",
      "Iteration 18, loss = 0.20457289\n",
      "Iteration 19, loss = 0.16788800\n",
      "Iteration 20, loss = 0.14993393\n",
      "Iteration 21, loss = 0.18024816\n",
      "Iteration 22, loss = 0.13813263\n",
      "Iteration 23, loss = 0.13365025\n",
      "Iteration 24, loss = 0.26542658\n",
      "Iteration 25, loss = 0.37748752\n",
      "Iteration 26, loss = 0.20143012\n",
      "Iteration 27, loss = 0.16280367\n",
      "Iteration 28, loss = 0.13945990\n",
      "Iteration 29, loss = 0.12742604\n",
      "Iteration 30, loss = 0.11043916\n",
      "Iteration 31, loss = 0.10571442\n",
      "Iteration 32, loss = 0.09359850\n",
      "Iteration 33, loss = 0.08612783\n",
      "Iteration 34, loss = 0.07155552\n",
      "Iteration 35, loss = 0.08391276\n",
      "Iteration 36, loss = 0.07136258\n",
      "Iteration 37, loss = 0.12139960\n",
      "Iteration 38, loss = 0.06725943\n",
      "Iteration 39, loss = 0.05796424\n",
      "Iteration 40, loss = 0.04755895\n",
      "Iteration 41, loss = 0.04537464\n",
      "Iteration 42, loss = 0.04279827\n",
      "Iteration 43, loss = 0.04397386\n",
      "Iteration 44, loss = 0.05809418\n",
      "Iteration 45, loss = 0.04463685\n",
      "Iteration 46, loss = 0.03115435\n",
      "Iteration 47, loss = 0.03095751\n",
      "Iteration 48, loss = 0.03195654\n",
      "Iteration 49, loss = 0.02357880\n",
      "Iteration 50, loss = 0.02203906\n",
      "Iteration 51, loss = 0.01749696\n",
      "Iteration 52, loss = 0.01803749\n",
      "Iteration 53, loss = 0.01621175\n",
      "Iteration 54, loss = 0.01375701\n",
      "Iteration 55, loss = 0.01204291\n",
      "Iteration 56, loss = 0.01373030\n",
      "Iteration 57, loss = 0.01323310\n",
      "Iteration 58, loss = 0.01141278\n",
      "Iteration 59, loss = 0.01068377\n",
      "Iteration 60, loss = 0.01130722\n",
      "Iteration 61, loss = 0.01157770\n",
      "Iteration 62, loss = 0.01417086\n",
      "Iteration 63, loss = 0.01203334\n",
      "Iteration 64, loss = 0.00876277\n",
      "Iteration 65, loss = 0.00808708\n",
      "Iteration 66, loss = 0.00729998\n",
      "Iteration 67, loss = 0.00654715\n",
      "Iteration 68, loss = 0.00668834\n",
      "Iteration 69, loss = 0.00563382\n",
      "Iteration 70, loss = 0.00509887\n",
      "Iteration 71, loss = 0.00470097\n",
      "Iteration 72, loss = 0.00432494\n",
      "Iteration 73, loss = 0.00435687\n",
      "Iteration 74, loss = 0.00413324\n",
      "Iteration 75, loss = 0.00389403\n",
      "Iteration 76, loss = 0.00367850\n",
      "Iteration 77, loss = 0.00359322\n",
      "Iteration 78, loss = 0.00344605\n",
      "Iteration 79, loss = 0.00332601\n",
      "Iteration 80, loss = 0.00332753\n",
      "Iteration 81, loss = 0.00318691\n",
      "Iteration 82, loss = 0.00310780\n",
      "Iteration 83, loss = 0.00294267\n",
      "Iteration 84, loss = 0.00304910\n",
      "Iteration 85, loss = 0.00275902\n",
      "Iteration 86, loss = 0.00289221\n",
      "Iteration 87, loss = 0.00270979\n",
      "Iteration 88, loss = 0.00256488\n",
      "Iteration 89, loss = 0.00252018\n",
      "Iteration 90, loss = 0.00250354\n",
      "Iteration 91, loss = 0.00239243\n",
      "Iteration 92, loss = 0.00236930\n",
      "Iteration 93, loss = 0.00228813\n",
      "Iteration 94, loss = 0.00221061\n",
      "Iteration 95, loss = 0.00218423\n",
      "Iteration 96, loss = 0.00211566\n",
      "Iteration 97, loss = 0.00216807\n",
      "Iteration 98, loss = 0.00202600\n",
      "Iteration 99, loss = 0.00205247\n",
      "Iteration 100, loss = 0.00198152\n",
      "Iteration 101, loss = 0.00197867\n",
      "Iteration 102, loss = 0.00187084\n",
      "Iteration 103, loss = 0.00184471\n",
      "Iteration 104, loss = 0.00180575\n",
      "Iteration 105, loss = 0.00180824\n",
      "Iteration 106, loss = 0.00176607\n",
      "Iteration 107, loss = 0.00174690\n",
      "Iteration 108, loss = 0.00169402\n",
      "Iteration 109, loss = 0.00163483\n",
      "Iteration 110, loss = 0.00164914\n",
      "Iteration 111, loss = 0.00166496\n",
      "Iteration 112, loss = 0.00163689\n",
      "Iteration 113, loss = 0.00156693\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 0.8967254408060453\n"
     ]
    }
   ],
   "source": [
    "# test other model\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Separate data in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(learning_features, learning_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the labels\n",
    "labelEncoder = preprocessing.LabelEncoder().fit(y_train)\n",
    "learningLabelsStd = labelEncoder.transform(y_train)\n",
    "testLabelsStd = labelEncoder.transform(y_test)\n",
    "\n",
    "# Learn the model\n",
    "model = MLPClassifier(hidden_layer_sizes=(100,  100), max_iter=1000, alpha=1e-4, solver='sgd', verbose=10, random_state=1, learning_rate_init=.1)\n",
    "scaler = preprocessing.StandardScaler(with_mean=True).fit(X_train)\n",
    "learningFeatures_scaled = scaler.transform(X_train)\n",
    "\n",
    "model.fit(learningFeatures_scaled, learningLabelsStd)\n",
    "\n",
    "# Test the model\n",
    "testFeatures_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = model.score(testFeatures_scaled, testLabelsStd)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b7d93a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.76700322\n",
      "Iteration 2, loss = 0.65019717\n",
      "Iteration 3, loss = 0.60684778\n",
      "Iteration 4, loss = 0.55903652\n",
      "Iteration 5, loss = 0.52414630\n",
      "Iteration 6, loss = 0.50166696\n",
      "Iteration 7, loss = 0.48434058\n",
      "Iteration 8, loss = 0.46953822\n",
      "Iteration 9, loss = 0.45482008\n",
      "Iteration 10, loss = 0.43943126\n",
      "Iteration 11, loss = 0.42349687\n",
      "Iteration 12, loss = 0.40720203\n",
      "Iteration 13, loss = 0.39043952\n",
      "Iteration 14, loss = 0.37372436\n",
      "Iteration 15, loss = 0.35690307\n",
      "Iteration 16, loss = 0.34014430\n",
      "Iteration 17, loss = 0.32351894\n",
      "Iteration 18, loss = 0.30646359\n",
      "Iteration 19, loss = 0.28949203\n",
      "Iteration 20, loss = 0.27259490\n",
      "Iteration 21, loss = 0.25571365\n",
      "Iteration 22, loss = 0.23907320\n",
      "Iteration 23, loss = 0.22261699\n",
      "Iteration 24, loss = 0.20657307\n",
      "Iteration 25, loss = 0.19103483\n",
      "Iteration 26, loss = 0.17598480\n",
      "Iteration 27, loss = 0.16165651\n",
      "Iteration 28, loss = 0.14798217\n",
      "Iteration 29, loss = 0.13496998\n",
      "Iteration 30, loss = 0.12274297\n",
      "Iteration 31, loss = 0.11126938\n",
      "Iteration 32, loss = 0.10069858\n",
      "Iteration 33, loss = 0.09072373\n",
      "Iteration 34, loss = 0.08146077\n",
      "Iteration 35, loss = 0.07292678\n",
      "Iteration 36, loss = 0.06511961\n",
      "Iteration 37, loss = 0.05799194\n",
      "Iteration 38, loss = 0.05158044\n",
      "Iteration 39, loss = 0.04583629\n",
      "Iteration 40, loss = 0.04086722\n",
      "Iteration 41, loss = 0.03640125\n",
      "Iteration 42, loss = 0.03247366\n",
      "Iteration 43, loss = 0.02905880\n",
      "Iteration 44, loss = 0.02606454\n",
      "Iteration 45, loss = 0.02343521\n",
      "Iteration 46, loss = 0.02115482\n",
      "Iteration 47, loss = 0.01914117\n",
      "Iteration 48, loss = 0.01738181\n",
      "Iteration 49, loss = 0.01583856\n",
      "Iteration 50, loss = 0.01447198\n",
      "Iteration 51, loss = 0.01326717\n",
      "Iteration 52, loss = 0.01220247\n",
      "Iteration 53, loss = 0.01126217\n",
      "Iteration 54, loss = 0.01042563\n",
      "Iteration 55, loss = 0.00968629\n",
      "Iteration 56, loss = 0.00902990\n",
      "Iteration 57, loss = 0.00844028\n",
      "Iteration 58, loss = 0.00790886\n",
      "Iteration 59, loss = 0.00743356\n",
      "Iteration 60, loss = 0.00700495\n",
      "Iteration 61, loss = 0.00661661\n",
      "Iteration 62, loss = 0.00626728\n",
      "Iteration 63, loss = 0.00595123\n",
      "Iteration 64, loss = 0.00566466\n",
      "Iteration 65, loss = 0.00540354\n",
      "Iteration 66, loss = 0.00516383\n",
      "Iteration 67, loss = 0.00494386\n",
      "Iteration 68, loss = 0.00474262\n",
      "Iteration 69, loss = 0.00455773\n",
      "Iteration 70, loss = 0.00438596\n",
      "Iteration 71, loss = 0.00422779\n",
      "Iteration 72, loss = 0.00408130\n",
      "Iteration 73, loss = 0.00394563\n",
      "Iteration 74, loss = 0.00382022\n",
      "Iteration 75, loss = 0.00370339\n",
      "Iteration 76, loss = 0.00359507\n",
      "Iteration 77, loss = 0.00349298\n",
      "Iteration 78, loss = 0.00339820\n",
      "Iteration 79, loss = 0.00330892\n",
      "Iteration 80, loss = 0.00322542\n",
      "Iteration 81, loss = 0.00314706\n",
      "Iteration 82, loss = 0.00307247\n",
      "Iteration 83, loss = 0.00300215\n",
      "Iteration 84, loss = 0.00293536\n",
      "Iteration 85, loss = 0.00287193\n",
      "Iteration 86, loss = 0.00281184\n",
      "Iteration 87, loss = 0.00275501\n",
      "Iteration 88, loss = 0.00270003\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70416745\n",
      "Iteration 2, loss = 0.64625320\n",
      "Iteration 3, loss = 0.59924317\n",
      "Iteration 4, loss = 0.55934083\n",
      "Iteration 5, loss = 0.56222930\n",
      "Iteration 6, loss = 0.53198837\n",
      "Iteration 7, loss = 0.47535387\n",
      "Iteration 8, loss = 0.46434171\n",
      "Iteration 9, loss = 0.42997171\n",
      "Iteration 10, loss = 0.49161943\n",
      "Iteration 11, loss = 0.42133422\n",
      "Iteration 12, loss = 0.41624618\n",
      "Iteration 13, loss = 0.48023797\n",
      "Iteration 14, loss = 0.42120362\n",
      "Iteration 15, loss = 0.41113535\n",
      "Iteration 16, loss = 0.40672447\n",
      "Iteration 17, loss = 0.34396158\n",
      "Iteration 18, loss = 0.39221699\n",
      "Iteration 19, loss = 0.31425690\n",
      "Iteration 20, loss = 0.33123951\n",
      "Iteration 21, loss = 0.33746977\n",
      "Iteration 22, loss = 0.30960205\n",
      "Iteration 23, loss = 0.36801544\n",
      "Iteration 24, loss = 0.27987454\n",
      "Iteration 25, loss = 0.22611235\n",
      "Iteration 26, loss = 0.23597022\n",
      "Iteration 27, loss = 0.23932777\n",
      "Iteration 28, loss = 0.36976492\n",
      "Iteration 29, loss = 0.31302358\n",
      "Iteration 30, loss = 0.30648881\n",
      "Iteration 31, loss = 0.40675195\n",
      "Iteration 32, loss = 0.43517432\n",
      "Iteration 33, loss = 0.32652768\n",
      "Iteration 34, loss = 0.20018868\n",
      "Iteration 35, loss = 0.20836698\n",
      "Iteration 36, loss = 0.20449937\n",
      "Iteration 37, loss = 0.22823127\n",
      "Iteration 38, loss = 0.16775179\n",
      "Iteration 39, loss = 0.13416643\n",
      "Iteration 40, loss = 0.29814527\n",
      "Iteration 41, loss = 0.13930385\n",
      "Iteration 42, loss = 0.14361891\n",
      "Iteration 43, loss = 0.14464236\n",
      "Iteration 44, loss = 0.12246250\n",
      "Iteration 45, loss = 0.16014803\n",
      "Iteration 46, loss = 0.11045694\n",
      "Iteration 47, loss = 0.14914517\n",
      "Iteration 48, loss = 0.08649026\n",
      "Iteration 49, loss = 0.21104988\n",
      "Iteration 50, loss = 0.11809230\n",
      "Iteration 51, loss = 0.10721109\n",
      "Iteration 52, loss = 0.07988746\n",
      "Iteration 53, loss = 0.10091290\n",
      "Iteration 54, loss = 0.05637626\n",
      "Iteration 55, loss = 0.05034478\n",
      "Iteration 56, loss = 0.06358645\n",
      "Iteration 57, loss = 0.05557356\n",
      "Iteration 58, loss = 0.06024782\n",
      "Iteration 59, loss = 0.06561325\n",
      "Iteration 60, loss = 0.06962181\n",
      "Iteration 61, loss = 0.04423632\n",
      "Iteration 62, loss = 0.04548524\n",
      "Iteration 63, loss = 0.09639605\n",
      "Iteration 64, loss = 0.04050122\n",
      "Iteration 65, loss = 0.44288442\n",
      "Iteration 66, loss = 3.19067826\n",
      "Iteration 67, loss = 1.08279713\n",
      "Iteration 68, loss = 0.44493120\n",
      "Iteration 69, loss = 0.42963329\n",
      "Iteration 70, loss = 0.43011301\n",
      "Iteration 71, loss = 0.40310403\n",
      "Iteration 72, loss = 0.35608887\n",
      "Iteration 73, loss = 1.03349504\n",
      "Iteration 74, loss = 1.06458722\n",
      "Iteration 75, loss = 0.53168399\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67728151\n",
      "Iteration 2, loss = 0.58346714\n",
      "Iteration 3, loss = 0.55467790\n",
      "Iteration 4, loss = 0.51372641\n",
      "Iteration 5, loss = 0.48787277\n",
      "Iteration 6, loss = 0.45712386\n",
      "Iteration 7, loss = 0.46038261\n",
      "Iteration 8, loss = 0.40449344\n",
      "Iteration 9, loss = 0.38250511\n",
      "Iteration 10, loss = 0.35592226\n",
      "Iteration 11, loss = 0.34551814\n",
      "Iteration 12, loss = 0.39101488\n",
      "Iteration 13, loss = 0.29904949\n",
      "Iteration 14, loss = 0.27558043\n",
      "Iteration 15, loss = 0.29438233\n",
      "Iteration 16, loss = 0.46329646\n",
      "Iteration 17, loss = 0.28409733\n",
      "Iteration 18, loss = 0.24414610\n",
      "Iteration 19, loss = 0.22873544\n",
      "Iteration 20, loss = 0.20740776\n",
      "Iteration 21, loss = 0.19193677\n",
      "Iteration 22, loss = 0.17670289\n",
      "Iteration 23, loss = 0.17976821\n",
      "Iteration 24, loss = 0.14495786\n",
      "Iteration 25, loss = 0.14529758\n",
      "Iteration 26, loss = 0.12511096\n",
      "Iteration 27, loss = 0.13857311\n",
      "Iteration 28, loss = 0.28456170\n",
      "Iteration 29, loss = 0.86275407\n",
      "Iteration 30, loss = 0.21897522\n",
      "Iteration 31, loss = 0.19723698\n",
      "Iteration 32, loss = 0.17680092\n",
      "Iteration 33, loss = 0.15699615\n",
      "Iteration 34, loss = 0.13173423\n",
      "Iteration 35, loss = 0.12360918\n",
      "Iteration 36, loss = 0.10565937\n",
      "Iteration 37, loss = 0.10471184\n",
      "Iteration 38, loss = 0.08779762\n",
      "Iteration 39, loss = 0.07845990\n",
      "Iteration 40, loss = 0.07726870\n",
      "Iteration 41, loss = 0.06286703\n",
      "Iteration 42, loss = 0.05690554\n",
      "Iteration 43, loss = 0.05154365\n",
      "Iteration 44, loss = 0.04565605\n",
      "Iteration 45, loss = 0.04082902\n",
      "Iteration 46, loss = 0.03836166\n",
      "Iteration 47, loss = 0.03707719\n",
      "Iteration 48, loss = 0.03191358\n",
      "Iteration 49, loss = 0.03956794\n",
      "Iteration 50, loss = 0.02795322\n",
      "Iteration 51, loss = 0.02576782\n",
      "Iteration 52, loss = 0.02378115\n",
      "Iteration 53, loss = 0.02099508\n",
      "Iteration 54, loss = 0.02190639\n",
      "Iteration 55, loss = 0.02001281\n",
      "Iteration 56, loss = 0.01970382\n",
      "Iteration 57, loss = 0.01699802\n",
      "Iteration 58, loss = 0.01515492\n",
      "Iteration 59, loss = 0.01583096\n",
      "Iteration 60, loss = 0.01265336\n",
      "Iteration 61, loss = 0.01230132\n",
      "Iteration 62, loss = 0.01198722\n",
      "Iteration 63, loss = 0.01111574\n",
      "Iteration 64, loss = 0.00994471\n",
      "Iteration 65, loss = 0.01013124\n",
      "Iteration 66, loss = 0.00939824\n",
      "Iteration 67, loss = 0.00884024\n",
      "Iteration 68, loss = 0.00980802\n",
      "Iteration 69, loss = 0.00909232\n",
      "Iteration 70, loss = 0.00863082\n",
      "Iteration 71, loss = 0.00756566\n",
      "Iteration 72, loss = 0.00784928\n",
      "Iteration 73, loss = 0.00726870\n",
      "Iteration 74, loss = 0.00863291\n",
      "Iteration 75, loss = 0.00804572\n",
      "Iteration 76, loss = 0.00755036\n",
      "Iteration 77, loss = 0.00604968\n",
      "Iteration 78, loss = 0.00584676\n",
      "Iteration 79, loss = 0.00573417\n",
      "Iteration 80, loss = 0.00580477\n",
      "Iteration 81, loss = 0.00570537\n",
      "Iteration 82, loss = 0.00494779\n",
      "Iteration 83, loss = 0.00537358\n",
      "Iteration 84, loss = 0.00475772\n",
      "Iteration 85, loss = 0.00446005\n",
      "Iteration 86, loss = 0.00440603\n",
      "Iteration 87, loss = 0.00425855\n",
      "Iteration 88, loss = 0.00406907\n",
      "Iteration 89, loss = 0.00399919\n",
      "Iteration 90, loss = 0.00383939\n",
      "Iteration 91, loss = 0.00376297\n",
      "Iteration 92, loss = 0.00378726\n",
      "Iteration 93, loss = 0.00356760\n",
      "Iteration 94, loss = 0.00342825\n",
      "Iteration 95, loss = 0.00343511\n",
      "Iteration 96, loss = 0.00349484\n",
      "Iteration 97, loss = 0.00333035\n",
      "Iteration 98, loss = 0.00317054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 99, loss = 0.00307185\n",
      "Iteration 100, loss = 0.00304704\n",
      "Iteration 101, loss = 0.00296862\n",
      "Iteration 102, loss = 0.00300455\n",
      "Iteration 103, loss = 0.00277900\n",
      "Iteration 104, loss = 0.00273086\n",
      "Iteration 105, loss = 0.00268978\n",
      "Iteration 106, loss = 0.00268266\n",
      "Iteration 107, loss = 0.00261322\n",
      "Iteration 108, loss = 0.00250232\n",
      "Iteration 109, loss = 0.00265493\n",
      "Iteration 110, loss = 0.00242662\n",
      "Iteration 111, loss = 0.00239337\n",
      "Iteration 112, loss = 0.00233860\n",
      "Iteration 113, loss = 0.00229926\n",
      "Iteration 114, loss = 0.00229570\n",
      "Iteration 115, loss = 0.00221212\n",
      "Iteration 116, loss = 0.00218131\n",
      "Iteration 117, loss = 0.00214919\n",
      "Iteration 118, loss = 0.00211280\n",
      "Iteration 119, loss = 0.00206391\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66271183\n",
      "Iteration 2, loss = 0.56674444\n",
      "Iteration 3, loss = 0.52605325\n",
      "Iteration 4, loss = 0.49042063\n",
      "Iteration 5, loss = 0.46319638\n",
      "Iteration 6, loss = 0.43172216\n",
      "Iteration 7, loss = 0.39492742\n",
      "Iteration 8, loss = 0.41700880\n",
      "Iteration 9, loss = 0.38164784\n",
      "Iteration 10, loss = 0.38472239\n",
      "Iteration 11, loss = 0.34601445\n",
      "Iteration 12, loss = 0.31571298\n",
      "Iteration 13, loss = 0.29445816\n",
      "Iteration 14, loss = 0.26635437\n",
      "Iteration 15, loss = 0.26505164\n",
      "Iteration 16, loss = 0.38499658\n",
      "Iteration 17, loss = 0.26692382\n",
      "Iteration 18, loss = 0.23280668\n",
      "Iteration 19, loss = 0.21222014\n",
      "Iteration 20, loss = 0.20383166\n",
      "Iteration 21, loss = 0.21235375\n",
      "Iteration 22, loss = 0.23281273\n",
      "Iteration 23, loss = 0.18070208\n",
      "Iteration 24, loss = 0.17222597\n",
      "Iteration 25, loss = 0.21723239\n",
      "Iteration 26, loss = 0.15545281\n",
      "Iteration 27, loss = 0.13287488\n",
      "Iteration 28, loss = 0.12532408\n",
      "Iteration 29, loss = 0.14504129\n",
      "Iteration 30, loss = 0.10297093\n",
      "Iteration 31, loss = 0.11590567\n",
      "Iteration 32, loss = 0.10066022\n",
      "Iteration 33, loss = 0.08330674\n",
      "Iteration 34, loss = 0.08042780\n",
      "Iteration 35, loss = 0.47374316\n",
      "Iteration 36, loss = 0.56079999\n",
      "Iteration 37, loss = 0.26367316\n",
      "Iteration 38, loss = 0.21479969\n",
      "Iteration 39, loss = 0.19134041\n",
      "Iteration 40, loss = 0.16331163\n",
      "Iteration 41, loss = 0.13066603\n",
      "Iteration 42, loss = 0.11694132\n",
      "Iteration 43, loss = 0.09993224\n",
      "Iteration 44, loss = 0.09359571\n",
      "Iteration 45, loss = 0.07603954\n",
      "Iteration 46, loss = 0.06743832\n",
      "Iteration 47, loss = 0.05832963\n",
      "Iteration 48, loss = 0.05529680\n",
      "Iteration 49, loss = 0.05268387\n",
      "Iteration 50, loss = 0.05044038\n",
      "Iteration 51, loss = 0.04691204\n",
      "Iteration 52, loss = 0.03797667\n",
      "Iteration 53, loss = 0.04015454\n",
      "Iteration 54, loss = 0.03380314\n",
      "Iteration 55, loss = 0.03119310\n",
      "Iteration 56, loss = 0.03018960\n",
      "Iteration 57, loss = 0.02561266\n",
      "Iteration 58, loss = 0.02560677\n",
      "Iteration 59, loss = 0.02400544\n",
      "Iteration 60, loss = 0.02181528\n",
      "Iteration 61, loss = 0.02105111\n",
      "Iteration 62, loss = 0.01812289\n",
      "Iteration 63, loss = 0.01735157\n",
      "Iteration 64, loss = 0.01650181\n",
      "Iteration 65, loss = 0.01594546\n",
      "Iteration 66, loss = 0.01420930\n",
      "Iteration 67, loss = 0.01387544\n",
      "Iteration 68, loss = 0.01223547\n",
      "Iteration 69, loss = 0.01177528\n",
      "Iteration 70, loss = 0.01163980\n",
      "Iteration 71, loss = 0.01083240\n",
      "Iteration 72, loss = 0.00970147\n",
      "Iteration 73, loss = 0.00980479\n",
      "Iteration 74, loss = 0.00917670\n",
      "Iteration 75, loss = 0.00907684\n",
      "Iteration 76, loss = 0.00925520\n",
      "Iteration 77, loss = 0.00924644\n",
      "Iteration 78, loss = 0.00798315\n",
      "Iteration 79, loss = 0.00742521\n",
      "Iteration 80, loss = 0.00741457\n",
      "Iteration 81, loss = 0.00712091\n",
      "Iteration 82, loss = 0.00665962\n",
      "Iteration 83, loss = 0.00654703\n",
      "Iteration 84, loss = 0.00629936\n",
      "Iteration 85, loss = 0.00582386\n",
      "Iteration 86, loss = 0.00576091\n",
      "Iteration 87, loss = 0.00577498\n",
      "Iteration 88, loss = 0.00553522\n",
      "Iteration 89, loss = 0.00531569\n",
      "Iteration 90, loss = 0.00520399\n",
      "Iteration 91, loss = 0.00507063\n",
      "Iteration 92, loss = 0.00473965\n",
      "Iteration 93, loss = 0.00466973\n",
      "Iteration 94, loss = 0.00462550\n",
      "Iteration 95, loss = 0.00430237\n",
      "Iteration 96, loss = 0.00424715\n",
      "Iteration 97, loss = 0.00422813\n",
      "Iteration 98, loss = 0.00409371\n",
      "Iteration 99, loss = 0.00398362\n",
      "Iteration 100, loss = 0.00386171\n",
      "Iteration 101, loss = 0.00380103\n",
      "Iteration 102, loss = 0.00377553\n",
      "Iteration 103, loss = 0.00371385\n",
      "Iteration 104, loss = 0.00345570\n",
      "Iteration 105, loss = 0.00344741\n",
      "Iteration 106, loss = 0.00335383\n",
      "Iteration 107, loss = 0.00335602\n",
      "Iteration 108, loss = 0.00332831\n",
      "Iteration 109, loss = 0.00312160\n",
      "Iteration 110, loss = 0.00313146\n",
      "Iteration 111, loss = 0.00310103\n",
      "Iteration 112, loss = 0.00296469\n",
      "Iteration 113, loss = 0.00293048\n",
      "Iteration 114, loss = 0.00292197\n",
      "Iteration 115, loss = 0.00282655\n",
      "Iteration 116, loss = 0.00273972\n",
      "Iteration 117, loss = 0.00271528\n",
      "Iteration 118, loss = 0.00283661\n",
      "Iteration 119, loss = 0.00271942\n",
      "Iteration 120, loss = 0.00262550\n",
      "Iteration 121, loss = 0.00251572\n",
      "Iteration 122, loss = 0.00245023\n",
      "Iteration 123, loss = 0.00239824\n",
      "Iteration 124, loss = 0.00239209\n",
      "Iteration 125, loss = 0.00234559\n",
      "Iteration 126, loss = 0.00232155\n",
      "Iteration 127, loss = 0.00223283\n",
      "Iteration 128, loss = 0.00222046\n",
      "Iteration 129, loss = 0.00219955\n",
      "Iteration 130, loss = 0.00214759\n",
      "Iteration 131, loss = 0.00212288\n",
      "Iteration 132, loss = 0.00206231\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64309091\n",
      "Iteration 2, loss = 0.55050954\n",
      "Iteration 3, loss = 0.50839977\n",
      "Iteration 4, loss = 0.46691618\n",
      "Iteration 5, loss = 0.42635877\n",
      "Iteration 6, loss = 0.40370117\n",
      "Iteration 7, loss = 0.38049056\n",
      "Iteration 8, loss = 0.35184364\n",
      "Iteration 9, loss = 0.34140580\n",
      "Iteration 10, loss = 0.31429804\n",
      "Iteration 11, loss = 0.31884066\n",
      "Iteration 12, loss = 0.29450551\n",
      "Iteration 13, loss = 0.27773573\n",
      "Iteration 14, loss = 0.23828230\n",
      "Iteration 15, loss = 0.24535279\n",
      "Iteration 16, loss = 0.19802921\n",
      "Iteration 17, loss = 0.22056361\n",
      "Iteration 18, loss = 0.18438137\n",
      "Iteration 19, loss = 0.16825431\n",
      "Iteration 20, loss = 0.23821529\n",
      "Iteration 21, loss = 0.18201314\n",
      "Iteration 22, loss = 0.15083505\n",
      "Iteration 23, loss = 0.12721833\n",
      "Iteration 24, loss = 0.12771552\n",
      "Iteration 25, loss = 0.15032994\n",
      "Iteration 26, loss = 0.12223750\n",
      "Iteration 27, loss = 0.15262054\n",
      "Iteration 28, loss = 0.10394683\n",
      "Iteration 29, loss = 0.09659389\n",
      "Iteration 30, loss = 0.08469243\n",
      "Iteration 31, loss = 0.07878702\n",
      "Iteration 32, loss = 0.09628334\n",
      "Iteration 33, loss = 0.09098659\n",
      "Iteration 34, loss = 0.14671118\n",
      "Iteration 35, loss = 0.08563204\n",
      "Iteration 36, loss = 0.07731975\n",
      "Iteration 37, loss = 0.12218031\n",
      "Iteration 38, loss = 0.06894444\n",
      "Iteration 39, loss = 0.05225713\n",
      "Iteration 40, loss = 0.03962705\n",
      "Iteration 41, loss = 0.04370220\n",
      "Iteration 42, loss = 0.03783981\n",
      "Iteration 43, loss = 0.03967333\n",
      "Iteration 44, loss = 0.82727375\n",
      "Iteration 45, loss = 0.20743776\n",
      "Iteration 46, loss = 0.17877753\n",
      "Iteration 47, loss = 0.13522570\n",
      "Iteration 48, loss = 0.11326309\n",
      "Iteration 49, loss = 0.10194543\n",
      "Iteration 50, loss = 0.07856765\n",
      "Iteration 51, loss = 0.07377441\n",
      "Iteration 52, loss = 0.05959098\n",
      "Iteration 53, loss = 0.05326612\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.78224735\n",
      "Iteration 2, loss = 0.65452007\n",
      "Iteration 3, loss = 0.61015387\n",
      "Iteration 4, loss = 0.56486717\n",
      "Iteration 5, loss = 0.53884752\n",
      "Iteration 6, loss = 0.52093493\n",
      "Iteration 7, loss = 0.50382971\n",
      "Iteration 8, loss = 0.48712940\n",
      "Iteration 9, loss = 0.47137855\n",
      "Iteration 10, loss = 0.45584490\n",
      "Iteration 11, loss = 0.44017168\n",
      "Iteration 12, loss = 0.42421763\n",
      "Iteration 13, loss = 0.40827670\n",
      "Iteration 14, loss = 0.39236506\n",
      "Iteration 15, loss = 0.37663204\n",
      "Iteration 16, loss = 0.36091992\n",
      "Iteration 17, loss = 0.34501964\n",
      "Iteration 18, loss = 0.32919460\n",
      "Iteration 19, loss = 0.31385474\n",
      "Iteration 20, loss = 0.29891194\n",
      "Iteration 21, loss = 0.28446050\n",
      "Iteration 22, loss = 0.26997602\n",
      "Iteration 23, loss = 0.25570498\n",
      "Iteration 24, loss = 0.24175746\n",
      "Iteration 25, loss = 0.22860568\n",
      "Iteration 26, loss = 0.21576954\n",
      "Iteration 27, loss = 0.20339997\n",
      "Iteration 28, loss = 0.19166931\n",
      "Iteration 29, loss = 0.18040330\n",
      "Iteration 30, loss = 0.16962218\n",
      "Iteration 31, loss = 0.15924032\n",
      "Iteration 32, loss = 0.14938156\n",
      "Iteration 33, loss = 0.14002983\n",
      "Iteration 34, loss = 0.13119671\n",
      "Iteration 35, loss = 0.12291197\n",
      "Iteration 36, loss = 0.11507862\n",
      "Iteration 37, loss = 0.10760564\n",
      "Iteration 38, loss = 0.10067990\n",
      "Iteration 39, loss = 0.09416388\n",
      "Iteration 40, loss = 0.08806500\n",
      "Iteration 41, loss = 0.08224247\n",
      "Iteration 42, loss = 0.07681242\n",
      "Iteration 43, loss = 0.07163861\n",
      "Iteration 44, loss = 0.06694301\n",
      "Iteration 45, loss = 0.06253682\n",
      "Iteration 46, loss = 0.05844779\n",
      "Iteration 47, loss = 0.05469971\n",
      "Iteration 48, loss = 0.05124149\n",
      "Iteration 49, loss = 0.04808889\n",
      "Iteration 50, loss = 0.04496171\n",
      "Iteration 51, loss = 0.04201867\n",
      "Iteration 52, loss = 0.03943156\n",
      "Iteration 53, loss = 0.03692338\n",
      "Iteration 54, loss = 0.03460929\n",
      "Iteration 55, loss = 0.03251161\n",
      "Iteration 56, loss = 0.03053271\n",
      "Iteration 57, loss = 0.02873948\n",
      "Iteration 58, loss = 0.02711689\n",
      "Iteration 59, loss = 0.02553216\n",
      "Iteration 60, loss = 0.02406934\n",
      "Iteration 61, loss = 0.02271432\n",
      "Iteration 62, loss = 0.02147607\n",
      "Iteration 63, loss = 0.02030034\n",
      "Iteration 64, loss = 0.01924547\n",
      "Iteration 65, loss = 0.01824996\n",
      "Iteration 66, loss = 0.01731320\n",
      "Iteration 67, loss = 0.01646397\n",
      "Iteration 68, loss = 0.01565421\n",
      "Iteration 69, loss = 0.01490840\n",
      "Iteration 70, loss = 0.01422202\n",
      "Iteration 71, loss = 0.01356880\n",
      "Iteration 72, loss = 0.01296600\n",
      "Iteration 73, loss = 0.01239958\n",
      "Iteration 74, loss = 0.01186976\n",
      "Iteration 75, loss = 0.01138177\n",
      "Iteration 76, loss = 0.01092116\n",
      "Iteration 77, loss = 0.01048748\n",
      "Iteration 78, loss = 0.01008787\n",
      "Iteration 79, loss = 0.00971213\n",
      "Iteration 80, loss = 0.00935723\n",
      "Iteration 81, loss = 0.00902011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 82, loss = 0.00870892\n",
      "Iteration 83, loss = 0.00841204\n",
      "Iteration 84, loss = 0.00813555\n",
      "Iteration 85, loss = 0.00787455\n",
      "Iteration 86, loss = 0.00762467\n",
      "Iteration 87, loss = 0.00738902\n",
      "Iteration 88, loss = 0.00716881\n",
      "Iteration 89, loss = 0.00695886\n",
      "Iteration 90, loss = 0.00675913\n",
      "Iteration 91, loss = 0.00656992\n",
      "Iteration 92, loss = 0.00639025\n",
      "Iteration 93, loss = 0.00622037\n",
      "Iteration 94, loss = 0.00605738\n",
      "Iteration 95, loss = 0.00590285\n",
      "Iteration 96, loss = 0.00575744\n",
      "Iteration 97, loss = 0.00561521\n",
      "Iteration 98, loss = 0.00548297\n",
      "Iteration 99, loss = 0.00535313\n",
      "Iteration 100, loss = 0.00523234\n",
      "Iteration 101, loss = 0.00511402\n",
      "Iteration 102, loss = 0.00500101\n",
      "Iteration 103, loss = 0.00489387\n",
      "Iteration 104, loss = 0.00478984\n",
      "Iteration 105, loss = 0.00468954\n",
      "Iteration 106, loss = 0.00459445\n",
      "Iteration 107, loss = 0.00450155\n",
      "Iteration 108, loss = 0.00441393\n",
      "Iteration 109, loss = 0.00432798\n",
      "Iteration 110, loss = 0.00424505\n",
      "Iteration 111, loss = 0.00416620\n",
      "Iteration 112, loss = 0.00408892\n",
      "Iteration 113, loss = 0.00401532\n",
      "Iteration 114, loss = 0.00394329\n",
      "Iteration 115, loss = 0.00387428\n",
      "Iteration 116, loss = 0.00380786\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74236366\n",
      "Iteration 2, loss = 0.61404492\n",
      "Iteration 3, loss = 0.57993489\n",
      "Iteration 4, loss = 0.56672887\n",
      "Iteration 5, loss = 0.62517840\n",
      "Iteration 6, loss = 0.72993110\n",
      "Iteration 7, loss = 0.58833092\n",
      "Iteration 8, loss = 0.59804799\n",
      "Iteration 9, loss = 0.52918089\n",
      "Iteration 10, loss = 0.50335001\n",
      "Iteration 11, loss = 0.52416105\n",
      "Iteration 12, loss = 0.51022499\n",
      "Iteration 13, loss = 0.50919095\n",
      "Iteration 14, loss = 0.46398607\n",
      "Iteration 15, loss = 0.42263039\n",
      "Iteration 16, loss = 0.42419262\n",
      "Iteration 17, loss = 0.46914133\n",
      "Iteration 18, loss = 0.39813204\n",
      "Iteration 19, loss = 0.43193034\n",
      "Iteration 20, loss = 0.36527321\n",
      "Iteration 21, loss = 0.37419220\n",
      "Iteration 22, loss = 0.34094224\n",
      "Iteration 23, loss = 0.33789191\n",
      "Iteration 24, loss = 0.65100584\n",
      "Iteration 25, loss = 0.38503356\n",
      "Iteration 26, loss = 0.39554816\n",
      "Iteration 27, loss = 0.42767509\n",
      "Iteration 28, loss = 0.47185024\n",
      "Iteration 29, loss = 0.36466931\n",
      "Iteration 30, loss = 0.56936155\n",
      "Iteration 31, loss = 0.35574030\n",
      "Iteration 32, loss = 0.35721894\n",
      "Iteration 33, loss = 0.31226058\n",
      "Iteration 34, loss = 0.31118386\n",
      "Iteration 35, loss = 0.29394200\n",
      "Iteration 36, loss = 0.26688207\n",
      "Iteration 37, loss = 0.32348877\n",
      "Iteration 38, loss = 0.23950636\n",
      "Iteration 39, loss = 0.27665638\n",
      "Iteration 40, loss = 0.43855038\n",
      "Iteration 41, loss = 0.28741992\n",
      "Iteration 42, loss = 0.27322972\n",
      "Iteration 43, loss = 0.22725634\n",
      "Iteration 44, loss = 0.28799028\n",
      "Iteration 45, loss = 0.26440262\n",
      "Iteration 46, loss = 0.35679673\n",
      "Iteration 47, loss = 0.25259585\n",
      "Iteration 48, loss = 0.23375502\n",
      "Iteration 49, loss = 0.37086900\n",
      "Iteration 50, loss = 0.26290113\n",
      "Iteration 51, loss = 0.20142872\n",
      "Iteration 52, loss = 0.21916121\n",
      "Iteration 53, loss = 0.20072100\n",
      "Iteration 54, loss = 0.19443166\n",
      "Iteration 55, loss = 0.19267656\n",
      "Iteration 56, loss = 0.27183453\n",
      "Iteration 57, loss = 0.17048994\n",
      "Iteration 58, loss = 0.16417508\n",
      "Iteration 59, loss = 0.15516095\n",
      "Iteration 60, loss = 0.12538753\n",
      "Iteration 61, loss = 0.12272484\n",
      "Iteration 62, loss = 0.09412797\n",
      "Iteration 63, loss = 0.09458881\n",
      "Iteration 64, loss = 0.08245123\n",
      "Iteration 65, loss = 0.06984349\n",
      "Iteration 66, loss = 0.09129082\n",
      "Iteration 67, loss = 0.08324615\n",
      "Iteration 68, loss = 0.05844737\n",
      "Iteration 69, loss = 0.09834710\n",
      "Iteration 70, loss = 0.07250142\n",
      "Iteration 71, loss = 0.09939951\n",
      "Iteration 72, loss = 0.05882251\n",
      "Iteration 73, loss = 0.12294169\n",
      "Iteration 74, loss = 0.07425743\n",
      "Iteration 75, loss = 0.32141587\n",
      "Iteration 76, loss = 0.08620499\n",
      "Iteration 77, loss = 0.11259864\n",
      "Iteration 78, loss = 0.17448192\n",
      "Iteration 79, loss = 0.35419369\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69626418\n",
      "Iteration 2, loss = 0.59910870\n",
      "Iteration 3, loss = 0.56867362\n",
      "Iteration 4, loss = 0.53852311\n",
      "Iteration 5, loss = 0.51428461\n",
      "Iteration 6, loss = 0.48034481\n",
      "Iteration 7, loss = 0.44703976\n",
      "Iteration 8, loss = 0.43666275\n",
      "Iteration 9, loss = 0.41899141\n",
      "Iteration 10, loss = 0.42613287\n",
      "Iteration 11, loss = 0.38299354\n",
      "Iteration 12, loss = 0.35353384\n",
      "Iteration 13, loss = 0.39333100\n",
      "Iteration 14, loss = 0.36106120\n",
      "Iteration 15, loss = 0.29578395\n",
      "Iteration 16, loss = 0.28063531\n",
      "Iteration 17, loss = 0.31600165\n",
      "Iteration 18, loss = 0.25307373\n",
      "Iteration 19, loss = 0.29098045\n",
      "Iteration 20, loss = 0.27142989\n",
      "Iteration 21, loss = 0.20452418\n",
      "Iteration 22, loss = 0.19719112\n",
      "Iteration 23, loss = 0.19314390\n",
      "Iteration 24, loss = 0.18258654\n",
      "Iteration 25, loss = 0.17377702\n",
      "Iteration 26, loss = 0.24634871\n",
      "Iteration 27, loss = 0.57049640\n",
      "Iteration 28, loss = 0.24164262\n",
      "Iteration 29, loss = 0.19546555\n",
      "Iteration 30, loss = 0.16821170\n",
      "Iteration 31, loss = 0.16310411\n",
      "Iteration 32, loss = 0.13955176\n",
      "Iteration 33, loss = 0.11800761\n",
      "Iteration 34, loss = 0.10881134\n",
      "Iteration 35, loss = 0.10946878\n",
      "Iteration 36, loss = 0.08925184\n",
      "Iteration 37, loss = 0.08486292\n",
      "Iteration 38, loss = 0.18230237\n",
      "Iteration 39, loss = 0.07389186\n",
      "Iteration 40, loss = 0.06653655\n",
      "Iteration 41, loss = 0.05815852\n",
      "Iteration 42, loss = 0.05306343\n",
      "Iteration 43, loss = 0.08688940\n",
      "Iteration 44, loss = 0.05087494\n",
      "Iteration 45, loss = 0.03954916\n",
      "Iteration 46, loss = 0.04833296\n",
      "Iteration 47, loss = 0.04461666\n",
      "Iteration 48, loss = 0.03751864\n",
      "Iteration 49, loss = 0.03565525\n",
      "Iteration 50, loss = 0.02827340\n",
      "Iteration 51, loss = 0.02347291\n",
      "Iteration 52, loss = 0.02058171\n",
      "Iteration 53, loss = 0.02348754\n",
      "Iteration 54, loss = 0.04974432\n",
      "Iteration 55, loss = 0.02921486\n",
      "Iteration 56, loss = 0.03683609\n",
      "Iteration 57, loss = 0.01777101\n",
      "Iteration 58, loss = 0.01687772\n",
      "Iteration 59, loss = 0.01533754\n",
      "Iteration 60, loss = 0.01376028\n",
      "Iteration 61, loss = 0.01186179\n",
      "Iteration 62, loss = 0.01190787\n",
      "Iteration 63, loss = 0.01280336\n",
      "Iteration 64, loss = 0.01046066\n",
      "Iteration 65, loss = 0.01121322\n",
      "Iteration 66, loss = 0.01076844\n",
      "Iteration 67, loss = 0.00936355\n",
      "Iteration 68, loss = 0.00906285\n",
      "Iteration 69, loss = 0.00836192\n",
      "Iteration 70, loss = 0.00820078\n",
      "Iteration 71, loss = 0.00736612\n",
      "Iteration 72, loss = 0.00788159\n",
      "Iteration 73, loss = 0.00686883\n",
      "Iteration 74, loss = 0.00607705\n",
      "Iteration 75, loss = 0.00627692\n",
      "Iteration 76, loss = 0.00638427\n",
      "Iteration 77, loss = 0.00542709\n",
      "Iteration 78, loss = 0.00529055\n",
      "Iteration 79, loss = 0.00540749\n",
      "Iteration 80, loss = 0.00481284\n",
      "Iteration 81, loss = 0.00451554\n",
      "Iteration 82, loss = 0.00436181\n",
      "Iteration 83, loss = 0.00423451\n",
      "Iteration 84, loss = 0.00417902\n",
      "Iteration 85, loss = 0.00393421\n",
      "Iteration 86, loss = 0.00401527\n",
      "Iteration 87, loss = 0.00387914\n",
      "Iteration 88, loss = 0.00375838\n",
      "Iteration 89, loss = 0.00373048\n",
      "Iteration 90, loss = 0.00350556\n",
      "Iteration 91, loss = 0.00344538\n",
      "Iteration 92, loss = 0.00328721\n",
      "Iteration 93, loss = 0.00321789\n",
      "Iteration 94, loss = 0.00314210\n",
      "Iteration 95, loss = 0.00302842\n",
      "Iteration 96, loss = 0.00298373\n",
      "Iteration 97, loss = 0.00299633\n",
      "Iteration 98, loss = 0.00286217\n",
      "Iteration 99, loss = 0.00283216\n",
      "Iteration 100, loss = 0.00277316\n",
      "Iteration 101, loss = 0.00271170\n",
      "Iteration 102, loss = 0.00272097\n",
      "Iteration 103, loss = 0.00260575\n",
      "Iteration 104, loss = 0.00261348\n",
      "Iteration 105, loss = 0.00290980\n",
      "Iteration 106, loss = 0.00253224\n",
      "Iteration 107, loss = 0.00240488\n",
      "Iteration 108, loss = 0.00235229\n",
      "Iteration 109, loss = 0.00231405\n",
      "Iteration 110, loss = 0.00228004\n",
      "Iteration 111, loss = 0.00224421\n",
      "Iteration 112, loss = 0.00221823\n",
      "Iteration 113, loss = 0.00215288\n",
      "Iteration 114, loss = 0.00212611\n",
      "Iteration 115, loss = 0.00211149\n",
      "Iteration 116, loss = 0.00211311\n",
      "Iteration 117, loss = 0.00203288\n",
      "Iteration 118, loss = 0.00199833\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67018031\n",
      "Iteration 2, loss = 0.57860019\n",
      "Iteration 3, loss = 0.53622881\n",
      "Iteration 4, loss = 0.50364313\n",
      "Iteration 5, loss = 0.48208281\n",
      "Iteration 6, loss = 0.45348283\n",
      "Iteration 7, loss = 0.43229166\n",
      "Iteration 8, loss = 0.40884801\n",
      "Iteration 9, loss = 0.37350025\n",
      "Iteration 10, loss = 0.36594629\n",
      "Iteration 11, loss = 0.33435062\n",
      "Iteration 12, loss = 0.44081075\n",
      "Iteration 13, loss = 0.42341154\n",
      "Iteration 14, loss = 0.30933989\n",
      "Iteration 15, loss = 0.28493350\n",
      "Iteration 16, loss = 0.28197108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, loss = 0.26449204\n",
      "Iteration 18, loss = 0.24240134\n",
      "Iteration 19, loss = 0.22496291\n",
      "Iteration 20, loss = 0.21752931\n",
      "Iteration 21, loss = 0.18669087\n",
      "Iteration 22, loss = 0.18888788\n",
      "Iteration 23, loss = 0.18183742\n",
      "Iteration 24, loss = 0.16596525\n",
      "Iteration 25, loss = 0.15266687\n",
      "Iteration 26, loss = 0.14985859\n",
      "Iteration 27, loss = 0.25777510\n",
      "Iteration 28, loss = 0.20690379\n",
      "Iteration 29, loss = 0.25060017\n",
      "Iteration 30, loss = 0.14508018\n",
      "Iteration 31, loss = 0.13348786\n",
      "Iteration 32, loss = 0.12058933\n",
      "Iteration 33, loss = 0.10331718\n",
      "Iteration 34, loss = 0.10015219\n",
      "Iteration 35, loss = 0.08773678\n",
      "Iteration 36, loss = 0.07629321\n",
      "Iteration 37, loss = 0.06828040\n",
      "Iteration 38, loss = 0.06848245\n",
      "Iteration 39, loss = 0.06911406\n",
      "Iteration 40, loss = 0.10240202\n",
      "Iteration 41, loss = 0.07035209\n",
      "Iteration 42, loss = 0.12670413\n",
      "Iteration 43, loss = 0.45406824\n",
      "Iteration 44, loss = 0.16599364\n",
      "Iteration 45, loss = 0.12460947\n",
      "Iteration 46, loss = 0.09747449\n",
      "Iteration 47, loss = 0.08047831\n",
      "Iteration 48, loss = 0.07149584\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65531119\n",
      "Iteration 2, loss = 0.56361581\n",
      "Iteration 3, loss = 0.50991944\n",
      "Iteration 4, loss = 0.47450098\n",
      "Iteration 5, loss = 0.44018313\n",
      "Iteration 6, loss = 0.43361632\n",
      "Iteration 7, loss = 0.38161783\n",
      "Iteration 8, loss = 0.34498432\n",
      "Iteration 9, loss = 0.33646832\n",
      "Iteration 10, loss = 0.31376793\n",
      "Iteration 11, loss = 0.36726728\n",
      "Iteration 12, loss = 0.29577538\n",
      "Iteration 13, loss = 0.26363607\n",
      "Iteration 14, loss = 0.25446893\n",
      "Iteration 15, loss = 0.21649137\n",
      "Iteration 16, loss = 0.22333013\n",
      "Iteration 17, loss = 0.20396090\n",
      "Iteration 18, loss = 0.21536761\n",
      "Iteration 19, loss = 0.19510085\n",
      "Iteration 20, loss = 0.17580583\n",
      "Iteration 21, loss = 0.15955252\n",
      "Iteration 22, loss = 0.15333229\n",
      "Iteration 23, loss = 0.20182144\n",
      "Iteration 24, loss = 0.14647058\n",
      "Iteration 25, loss = 0.12912523\n",
      "Iteration 26, loss = 0.11132748\n",
      "Iteration 27, loss = 0.10549339\n",
      "Iteration 28, loss = 0.11536032\n",
      "Iteration 29, loss = 0.09028997\n",
      "Iteration 30, loss = 0.18311560\n",
      "Iteration 31, loss = 0.28928314\n",
      "Iteration 32, loss = 0.14029170\n",
      "Iteration 33, loss = 0.10484884\n",
      "Iteration 34, loss = 0.09574262\n",
      "Iteration 35, loss = 0.09127073\n",
      "Iteration 36, loss = 0.08795912\n",
      "Iteration 37, loss = 0.07994344\n",
      "Iteration 38, loss = 0.07090315\n",
      "Iteration 39, loss = 0.06885663\n",
      "Iteration 40, loss = 0.05821885\n",
      "Iteration 41, loss = 0.04245059\n",
      "Iteration 42, loss = 0.03846143\n",
      "Iteration 43, loss = 0.05153409\n",
      "Iteration 44, loss = 0.54477515\n",
      "Iteration 45, loss = 0.17593118\n",
      "Iteration 46, loss = 0.12330194\n",
      "Iteration 47, loss = 0.10264758\n",
      "Iteration 48, loss = 0.08634924\n",
      "Iteration 49, loss = 0.06466302\n",
      "Iteration 50, loss = 0.05507732\n",
      "Iteration 51, loss = 0.04723039\n",
      "Iteration 52, loss = 0.05473815\n",
      "Iteration 53, loss = 0.04530339\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.78224735\n",
      "Iteration 2, loss = 0.65452007\n",
      "Iteration 3, loss = 0.61015387\n",
      "Iteration 4, loss = 0.56486717\n",
      "Iteration 5, loss = 0.53884752\n",
      "Iteration 6, loss = 0.52093493\n",
      "Iteration 7, loss = 0.50382971\n",
      "Iteration 8, loss = 0.48712940\n",
      "Iteration 9, loss = 0.47137855\n",
      "Iteration 10, loss = 0.45584490\n",
      "Iteration 11, loss = 0.44017168\n",
      "Iteration 12, loss = 0.42421763\n",
      "Iteration 13, loss = 0.40827670\n",
      "Iteration 14, loss = 0.39236506\n",
      "Iteration 15, loss = 0.37663204\n",
      "Iteration 16, loss = 0.36091992\n",
      "Iteration 17, loss = 0.34501964\n",
      "Iteration 18, loss = 0.32919460\n",
      "Iteration 19, loss = 0.31385474\n",
      "Iteration 20, loss = 0.29891194\n",
      "Iteration 21, loss = 0.28446050\n",
      "Iteration 22, loss = 0.26997602\n",
      "Iteration 23, loss = 0.25570498\n",
      "Iteration 24, loss = 0.24175746\n",
      "Iteration 25, loss = 0.22860568\n",
      "Iteration 26, loss = 0.21576954\n",
      "Iteration 27, loss = 0.20339997\n",
      "Iteration 28, loss = 0.19166931\n",
      "Iteration 29, loss = 0.18040330\n",
      "Iteration 30, loss = 0.16962218\n",
      "Iteration 31, loss = 0.15924032\n",
      "Iteration 32, loss = 0.14938156\n",
      "Iteration 33, loss = 0.14002983\n",
      "Iteration 34, loss = 0.13119671\n",
      "Iteration 35, loss = 0.12291197\n",
      "Iteration 36, loss = 0.11507862\n",
      "Iteration 37, loss = 0.10760564\n",
      "Iteration 38, loss = 0.10067990\n",
      "Iteration 39, loss = 0.09416388\n",
      "Iteration 40, loss = 0.08806500\n",
      "Iteration 41, loss = 0.08224247\n",
      "Iteration 42, loss = 0.07681242\n",
      "Iteration 43, loss = 0.07163861\n",
      "Iteration 44, loss = 0.06694301\n",
      "Iteration 45, loss = 0.06253682\n",
      "Iteration 46, loss = 0.05844779\n",
      "Iteration 47, loss = 0.05469971\n",
      "Iteration 48, loss = 0.05124149\n",
      "Iteration 49, loss = 0.04808889\n",
      "Iteration 50, loss = 0.04496171\n",
      "Iteration 51, loss = 0.04201867\n",
      "Iteration 52, loss = 0.03943156\n",
      "Iteration 53, loss = 0.03692338\n",
      "Iteration 54, loss = 0.03460929\n",
      "Iteration 55, loss = 0.03251161\n",
      "Iteration 56, loss = 0.03053271\n",
      "Iteration 57, loss = 0.02873948\n",
      "Iteration 58, loss = 0.02711689\n",
      "Iteration 59, loss = 0.02553216\n",
      "Iteration 60, loss = 0.02406934\n",
      "Iteration 61, loss = 0.02271432\n",
      "Iteration 62, loss = 0.02147607\n",
      "Iteration 63, loss = 0.02030034\n",
      "Iteration 64, loss = 0.01924547\n",
      "Iteration 65, loss = 0.01824996\n",
      "Iteration 66, loss = 0.01731320\n",
      "Iteration 67, loss = 0.01646397\n",
      "Iteration 68, loss = 0.01565421\n",
      "Iteration 69, loss = 0.01490840\n",
      "Iteration 70, loss = 0.01422202\n",
      "Iteration 71, loss = 0.01356880\n",
      "Iteration 72, loss = 0.01296600\n",
      "Iteration 73, loss = 0.01239958\n",
      "Iteration 74, loss = 0.01186976\n",
      "Iteration 75, loss = 0.01138177\n",
      "Iteration 76, loss = 0.01092116\n",
      "Iteration 77, loss = 0.01048748\n",
      "Iteration 78, loss = 0.01008787\n",
      "Iteration 79, loss = 0.00971213\n",
      "Iteration 80, loss = 0.00935723\n",
      "Iteration 81, loss = 0.00902011\n",
      "Iteration 82, loss = 0.00870892\n",
      "Iteration 83, loss = 0.00841204\n",
      "Iteration 84, loss = 0.00813555\n",
      "Iteration 85, loss = 0.00787455\n",
      "Iteration 86, loss = 0.00762467\n",
      "Iteration 87, loss = 0.00738902\n",
      "Iteration 88, loss = 0.00716881\n",
      "Iteration 89, loss = 0.00695886\n",
      "Iteration 90, loss = 0.00675913\n",
      "Iteration 91, loss = 0.00656992\n",
      "Iteration 92, loss = 0.00639025\n",
      "Iteration 93, loss = 0.00622037\n",
      "Iteration 94, loss = 0.00605738\n",
      "Iteration 95, loss = 0.00590285\n",
      "Iteration 96, loss = 0.00575744\n",
      "Iteration 97, loss = 0.00561521\n",
      "Iteration 98, loss = 0.00548297\n",
      "Iteration 99, loss = 0.00535313\n",
      "Iteration 100, loss = 0.00523234\n",
      "Iteration 101, loss = 0.00511402\n",
      "Iteration 102, loss = 0.00500101\n",
      "Iteration 103, loss = 0.00489387\n",
      "Iteration 104, loss = 0.00478984\n",
      "Iteration 105, loss = 0.00468954\n",
      "Iteration 106, loss = 0.00459445\n",
      "Iteration 107, loss = 0.00450155\n",
      "Iteration 108, loss = 0.00441393\n",
      "Iteration 109, loss = 0.00432798\n",
      "Iteration 110, loss = 0.00424505\n",
      "Iteration 111, loss = 0.00416620\n",
      "Iteration 112, loss = 0.00408892\n",
      "Iteration 113, loss = 0.00401532\n",
      "Iteration 114, loss = 0.00394329\n",
      "Iteration 115, loss = 0.00387428\n",
      "Iteration 116, loss = 0.00380786\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73189190\n",
      "Iteration 2, loss = 0.65242736\n",
      "Iteration 3, loss = 0.57040758\n",
      "Iteration 4, loss = 0.55844601\n",
      "Iteration 5, loss = 0.60761879\n",
      "Iteration 6, loss = 0.76139735\n",
      "Iteration 7, loss = 0.58167817\n",
      "Iteration 8, loss = 0.56905424\n",
      "Iteration 9, loss = 0.52524296\n",
      "Iteration 10, loss = 0.49058073\n",
      "Iteration 11, loss = 0.50166858\n",
      "Iteration 12, loss = 0.50579623\n",
      "Iteration 13, loss = 0.47132007\n",
      "Iteration 14, loss = 0.44890109\n",
      "Iteration 15, loss = 0.42863694\n",
      "Iteration 16, loss = 0.40770288\n",
      "Iteration 17, loss = 0.40055844\n",
      "Iteration 18, loss = 0.41764180\n",
      "Iteration 19, loss = 0.43930772\n",
      "Iteration 20, loss = 0.37349754\n",
      "Iteration 21, loss = 0.46417123\n",
      "Iteration 22, loss = 0.36746141\n",
      "Iteration 23, loss = 0.42789834\n",
      "Iteration 24, loss = 0.39275485\n",
      "Iteration 25, loss = 0.30582837\n",
      "Iteration 26, loss = 0.33837164\n",
      "Iteration 27, loss = 0.32944204\n",
      "Iteration 28, loss = 0.31028160\n",
      "Iteration 29, loss = 0.29216398\n",
      "Iteration 30, loss = 0.27631303\n",
      "Iteration 31, loss = 0.35733976\n",
      "Iteration 32, loss = 0.27611426\n",
      "Iteration 33, loss = 0.33430269\n",
      "Iteration 34, loss = 0.76807216\n",
      "Iteration 35, loss = 0.36398924\n",
      "Iteration 36, loss = 0.31110321\n",
      "Iteration 37, loss = 0.28428989\n",
      "Iteration 38, loss = 0.26606952\n",
      "Iteration 39, loss = 0.30735963\n",
      "Iteration 40, loss = 0.26182154\n",
      "Iteration 41, loss = 0.28700395\n",
      "Iteration 42, loss = 0.24216224\n",
      "Iteration 43, loss = 0.21381318\n",
      "Iteration 44, loss = 0.19408873\n",
      "Iteration 45, loss = 0.19043402\n",
      "Iteration 46, loss = 0.34914451\n",
      "Iteration 47, loss = 0.18321116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 48, loss = 0.17667715\n",
      "Iteration 49, loss = 0.15797113\n",
      "Iteration 50, loss = 0.13661709\n",
      "Iteration 51, loss = 0.12409408\n",
      "Iteration 52, loss = 0.16009542\n",
      "Iteration 53, loss = 0.19940509\n",
      "Iteration 54, loss = 0.20180715\n",
      "Iteration 55, loss = 0.15169796\n",
      "Iteration 56, loss = 0.14727583\n",
      "Iteration 57, loss = 0.16168252\n",
      "Iteration 58, loss = 0.12005609\n",
      "Iteration 59, loss = 0.15311776\n",
      "Iteration 60, loss = 0.12537707\n",
      "Iteration 61, loss = 0.13317404\n",
      "Iteration 62, loss = 0.11995200\n",
      "Iteration 63, loss = 0.10361590\n",
      "Iteration 64, loss = 0.09684476\n",
      "Iteration 65, loss = 0.10209148\n",
      "Iteration 66, loss = 0.10533965\n",
      "Iteration 67, loss = 0.13205355\n",
      "Iteration 68, loss = 0.09475099\n",
      "Iteration 69, loss = 0.11926657\n",
      "Iteration 70, loss = 0.06684011\n",
      "Iteration 71, loss = 0.24397406\n",
      "Iteration 72, loss = 0.16317823\n",
      "Iteration 73, loss = 0.20464632\n",
      "Iteration 74, loss = 0.15159067\n",
      "Iteration 75, loss = 0.87111606\n",
      "Iteration 76, loss = 0.70593306\n",
      "Iteration 77, loss = 0.21142618\n",
      "Iteration 78, loss = 0.18227845\n",
      "Iteration 79, loss = 1.11033965\n",
      "Iteration 80, loss = 0.40993337\n",
      "Iteration 81, loss = 0.48709848\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69682971\n",
      "Iteration 2, loss = 0.57737315\n",
      "Iteration 3, loss = 0.53029626\n",
      "Iteration 4, loss = 0.50244589\n",
      "Iteration 5, loss = 0.47590203\n",
      "Iteration 6, loss = 0.44975798\n",
      "Iteration 7, loss = 0.42900913\n",
      "Iteration 8, loss = 0.42761840\n",
      "Iteration 9, loss = 0.37924742\n",
      "Iteration 10, loss = 0.40680522\n",
      "Iteration 11, loss = 0.39371567\n",
      "Iteration 12, loss = 0.33239596\n",
      "Iteration 13, loss = 0.31747600\n",
      "Iteration 14, loss = 0.32431795\n",
      "Iteration 15, loss = 0.28735356\n",
      "Iteration 16, loss = 0.31513202\n",
      "Iteration 17, loss = 0.29789250\n",
      "Iteration 18, loss = 0.26295299\n",
      "Iteration 19, loss = 0.24808569\n",
      "Iteration 20, loss = 0.53672511\n",
      "Iteration 21, loss = 0.43350897\n",
      "Iteration 22, loss = 0.25584388\n",
      "Iteration 23, loss = 0.24466511\n",
      "Iteration 24, loss = 0.21736810\n",
      "Iteration 25, loss = 0.19895140\n",
      "Iteration 26, loss = 0.22354004\n",
      "Iteration 27, loss = 0.17154539\n",
      "Iteration 28, loss = 0.16700002\n",
      "Iteration 29, loss = 0.18159520\n",
      "Iteration 30, loss = 0.16373545\n",
      "Iteration 31, loss = 0.14473153\n",
      "Iteration 32, loss = 0.12824403\n",
      "Iteration 33, loss = 0.11927877\n",
      "Iteration 34, loss = 0.11565688\n",
      "Iteration 35, loss = 0.10192625\n",
      "Iteration 36, loss = 0.09355620\n",
      "Iteration 37, loss = 0.10312824\n",
      "Iteration 38, loss = 0.17438693\n",
      "Iteration 39, loss = 0.36450932\n",
      "Iteration 40, loss = 0.31141025\n",
      "Iteration 41, loss = 0.13290946\n",
      "Iteration 42, loss = 0.11848633\n",
      "Iteration 43, loss = 0.10043862\n",
      "Iteration 44, loss = 0.08936293\n",
      "Iteration 45, loss = 0.07375562\n",
      "Iteration 46, loss = 0.07301109\n",
      "Iteration 47, loss = 0.05625656\n",
      "Iteration 48, loss = 0.05435339\n",
      "Iteration 49, loss = 0.05515864\n",
      "Iteration 50, loss = 0.04449916\n",
      "Iteration 51, loss = 0.04150067\n",
      "Iteration 52, loss = 0.03506515\n",
      "Iteration 53, loss = 0.03175332\n",
      "Iteration 54, loss = 0.03049619\n",
      "Iteration 55, loss = 0.03249762\n",
      "Iteration 56, loss = 0.02453100\n",
      "Iteration 57, loss = 0.02444188\n",
      "Iteration 58, loss = 0.02282750\n",
      "Iteration 59, loss = 0.01965054\n",
      "Iteration 60, loss = 0.01788943\n",
      "Iteration 61, loss = 0.01565822\n",
      "Iteration 62, loss = 0.01521760\n",
      "Iteration 63, loss = 0.01620156\n",
      "Iteration 64, loss = 0.01337821\n",
      "Iteration 65, loss = 0.01258253\n",
      "Iteration 66, loss = 0.01086147\n",
      "Iteration 67, loss = 0.01094124\n",
      "Iteration 68, loss = 0.01006051\n",
      "Iteration 69, loss = 0.00936505\n",
      "Iteration 70, loss = 0.00929271\n",
      "Iteration 71, loss = 0.00866258\n",
      "Iteration 72, loss = 0.00816746\n",
      "Iteration 73, loss = 0.00787132\n",
      "Iteration 74, loss = 0.00714773\n",
      "Iteration 75, loss = 0.00696822\n",
      "Iteration 76, loss = 0.00669102\n",
      "Iteration 77, loss = 0.00642933\n",
      "Iteration 78, loss = 0.00636425\n",
      "Iteration 79, loss = 0.00634524\n",
      "Iteration 80, loss = 0.00596688\n",
      "Iteration 81, loss = 0.00561744\n",
      "Iteration 82, loss = 0.00542756\n",
      "Iteration 83, loss = 0.00524201\n",
      "Iteration 84, loss = 0.00501663\n",
      "Iteration 85, loss = 0.00479177\n",
      "Iteration 86, loss = 0.00473298\n",
      "Iteration 87, loss = 0.00465459\n",
      "Iteration 88, loss = 0.00470026\n",
      "Iteration 89, loss = 0.00452293\n",
      "Iteration 90, loss = 0.00420575\n",
      "Iteration 91, loss = 0.00407839\n",
      "Iteration 92, loss = 0.00391747\n",
      "Iteration 93, loss = 0.00382039\n",
      "Iteration 94, loss = 0.00379927\n",
      "Iteration 95, loss = 0.00369237\n",
      "Iteration 96, loss = 0.00355255\n",
      "Iteration 97, loss = 0.00351781\n",
      "Iteration 98, loss = 0.00335172\n",
      "Iteration 99, loss = 0.00332569\n",
      "Iteration 100, loss = 0.00321683\n",
      "Iteration 101, loss = 0.00313004\n",
      "Iteration 102, loss = 0.00306211\n",
      "Iteration 103, loss = 0.00299010\n",
      "Iteration 104, loss = 0.00293524\n",
      "Iteration 105, loss = 0.00289902\n",
      "Iteration 106, loss = 0.00287683\n",
      "Iteration 107, loss = 0.00277508\n",
      "Iteration 108, loss = 0.00271725\n",
      "Iteration 109, loss = 0.00266327\n",
      "Iteration 110, loss = 0.00262821\n",
      "Iteration 111, loss = 0.00258855\n",
      "Iteration 112, loss = 0.00251590\n",
      "Iteration 113, loss = 0.00250313\n",
      "Iteration 114, loss = 0.00251570\n",
      "Iteration 115, loss = 0.00239847\n",
      "Iteration 116, loss = 0.00233868\n",
      "Iteration 117, loss = 0.00234467\n",
      "Iteration 118, loss = 0.00230350\n",
      "Iteration 119, loss = 0.00227009\n",
      "Iteration 120, loss = 0.00220841\n",
      "Iteration 121, loss = 0.00216427\n",
      "Iteration 122, loss = 0.00218325\n",
      "Iteration 123, loss = 0.00211712\n",
      "Iteration 124, loss = 0.00207838\n",
      "Iteration 125, loss = 0.00207593\n",
      "Iteration 126, loss = 0.00202091\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66557955\n",
      "Iteration 2, loss = 0.55720995\n",
      "Iteration 3, loss = 0.51216321\n",
      "Iteration 4, loss = 0.48561234\n",
      "Iteration 5, loss = 0.45795657\n",
      "Iteration 6, loss = 0.43108466\n",
      "Iteration 7, loss = 0.42116692\n",
      "Iteration 8, loss = 0.41474068\n",
      "Iteration 9, loss = 0.36616769\n",
      "Iteration 10, loss = 0.34922262\n",
      "Iteration 11, loss = 0.40482531\n",
      "Iteration 12, loss = 0.37663870\n",
      "Iteration 13, loss = 0.31623509\n",
      "Iteration 14, loss = 0.31155578\n",
      "Iteration 15, loss = 0.27464840\n",
      "Iteration 16, loss = 0.28428798\n",
      "Iteration 17, loss = 0.26015124\n",
      "Iteration 18, loss = 0.23622011\n",
      "Iteration 19, loss = 0.29021486\n",
      "Iteration 20, loss = 0.24731267\n",
      "Iteration 21, loss = 0.22076350\n",
      "Iteration 22, loss = 0.22924699\n",
      "Iteration 23, loss = 0.19057934\n",
      "Iteration 24, loss = 0.16969024\n",
      "Iteration 25, loss = 0.19523218\n",
      "Iteration 26, loss = 0.15875159\n",
      "Iteration 27, loss = 0.19640276\n",
      "Iteration 28, loss = 0.14337830\n",
      "Iteration 29, loss = 0.13483527\n",
      "Iteration 30, loss = 0.14285931\n",
      "Iteration 31, loss = 0.11508841\n",
      "Iteration 32, loss = 0.11049391\n",
      "Iteration 33, loss = 0.14330110\n",
      "Iteration 34, loss = 0.80536682\n",
      "Iteration 35, loss = 0.29486996\n",
      "Iteration 36, loss = 0.24716105\n",
      "Iteration 37, loss = 0.21859018\n",
      "Iteration 38, loss = 0.20576676\n",
      "Iteration 39, loss = 0.16857490\n",
      "Iteration 40, loss = 0.14033976\n",
      "Iteration 41, loss = 0.12747917\n",
      "Iteration 42, loss = 0.11472915\n",
      "Iteration 43, loss = 0.09838378\n",
      "Iteration 44, loss = 0.09307197\n",
      "Iteration 45, loss = 0.08338921\n",
      "Iteration 46, loss = 0.07473139\n",
      "Iteration 47, loss = 0.06746197\n",
      "Iteration 48, loss = 0.06074945\n",
      "Iteration 49, loss = 0.05917833\n",
      "Iteration 50, loss = 0.04850988\n",
      "Iteration 51, loss = 0.04538679\n",
      "Iteration 52, loss = 0.04173996\n",
      "Iteration 53, loss = 0.03902326\n",
      "Iteration 54, loss = 0.03632592\n",
      "Iteration 55, loss = 0.03182549\n",
      "Iteration 56, loss = 0.03057780\n",
      "Iteration 57, loss = 0.02779582\n",
      "Iteration 58, loss = 0.02402829\n",
      "Iteration 59, loss = 0.02265925\n",
      "Iteration 60, loss = 0.02259893\n",
      "Iteration 61, loss = 0.02059517\n",
      "Iteration 62, loss = 0.01988809\n",
      "Iteration 63, loss = 0.01796847\n",
      "Iteration 64, loss = 0.01617007\n",
      "Iteration 65, loss = 0.01446454\n",
      "Iteration 66, loss = 0.01390960\n",
      "Iteration 67, loss = 0.01308136\n",
      "Iteration 68, loss = 0.01225554\n",
      "Iteration 69, loss = 0.01162419\n",
      "Iteration 70, loss = 0.01071429\n",
      "Iteration 71, loss = 0.01020858\n",
      "Iteration 72, loss = 0.00982005\n",
      "Iteration 73, loss = 0.00915924\n",
      "Iteration 74, loss = 0.00876288\n",
      "Iteration 75, loss = 0.00870308\n",
      "Iteration 76, loss = 0.00831186\n",
      "Iteration 77, loss = 0.00793716\n",
      "Iteration 78, loss = 0.00774498\n",
      "Iteration 79, loss = 0.00717043\n",
      "Iteration 80, loss = 0.00682695\n",
      "Iteration 81, loss = 0.00672131\n",
      "Iteration 82, loss = 0.00639991\n",
      "Iteration 83, loss = 0.00627367\n",
      "Iteration 84, loss = 0.00599938\n",
      "Iteration 85, loss = 0.00576930\n",
      "Iteration 86, loss = 0.00559465\n",
      "Iteration 87, loss = 0.00543485\n",
      "Iteration 88, loss = 0.00519161\n",
      "Iteration 89, loss = 0.00511224\n",
      "Iteration 90, loss = 0.00491890\n",
      "Iteration 91, loss = 0.00484859\n",
      "Iteration 92, loss = 0.00469315\n",
      "Iteration 93, loss = 0.00451788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 94, loss = 0.00438773\n",
      "Iteration 95, loss = 0.00431474\n",
      "Iteration 96, loss = 0.00422594\n",
      "Iteration 97, loss = 0.00406997\n",
      "Iteration 98, loss = 0.00398211\n",
      "Iteration 99, loss = 0.00394322\n",
      "Iteration 100, loss = 0.00377924\n",
      "Iteration 101, loss = 0.00368433\n",
      "Iteration 102, loss = 0.00359190\n",
      "Iteration 103, loss = 0.00352861\n",
      "Iteration 104, loss = 0.00340423\n",
      "Iteration 105, loss = 0.00336615\n",
      "Iteration 106, loss = 0.00328074\n",
      "Iteration 107, loss = 0.00324708\n",
      "Iteration 108, loss = 0.00315575\n",
      "Iteration 109, loss = 0.00310514\n",
      "Iteration 110, loss = 0.00306120\n",
      "Iteration 111, loss = 0.00296120\n",
      "Iteration 112, loss = 0.00289662\n",
      "Iteration 113, loss = 0.00285543\n",
      "Iteration 114, loss = 0.00280694\n",
      "Iteration 115, loss = 0.00274317\n",
      "Iteration 116, loss = 0.00272127\n",
      "Iteration 117, loss = 0.00264170\n",
      "Iteration 118, loss = 0.00264895\n",
      "Iteration 119, loss = 0.00254695\n",
      "Iteration 120, loss = 0.00251408\n",
      "Iteration 121, loss = 0.00246034\n",
      "Iteration 122, loss = 0.00242900\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64190317\n",
      "Iteration 2, loss = 0.53929707\n",
      "Iteration 3, loss = 0.50353497\n",
      "Iteration 4, loss = 0.47073629\n",
      "Iteration 5, loss = 0.43372583\n",
      "Iteration 6, loss = 0.41854767\n",
      "Iteration 7, loss = 0.38131671\n",
      "Iteration 8, loss = 0.35525639\n",
      "Iteration 9, loss = 0.38385900\n",
      "Iteration 10, loss = 0.33399859\n",
      "Iteration 11, loss = 0.30977986\n",
      "Iteration 12, loss = 0.33446848\n",
      "Iteration 13, loss = 0.28277592\n",
      "Iteration 14, loss = 0.27774920\n",
      "Iteration 15, loss = 0.23507371\n",
      "Iteration 16, loss = 0.24576061\n",
      "Iteration 17, loss = 0.21371391\n",
      "Iteration 18, loss = 0.23670833\n",
      "Iteration 19, loss = 0.19201832\n",
      "Iteration 20, loss = 0.19501028\n",
      "Iteration 21, loss = 0.17309690\n",
      "Iteration 22, loss = 0.14809725\n",
      "Iteration 23, loss = 0.14131045\n",
      "Iteration 24, loss = 0.18728080\n",
      "Iteration 25, loss = 0.30897886\n",
      "Iteration 26, loss = 0.41805000\n",
      "Iteration 27, loss = 0.22550317\n",
      "Iteration 28, loss = 0.19457744\n",
      "Iteration 29, loss = 0.16555297\n",
      "Iteration 30, loss = 0.15544966\n",
      "Iteration 31, loss = 0.14114403\n",
      "Iteration 32, loss = 0.12158701\n",
      "Iteration 33, loss = 0.11072112\n",
      "Iteration 34, loss = 0.10441621\n",
      "Iteration 35, loss = 0.10439056\n",
      "Iteration 36, loss = 0.10898070\n",
      "Iteration 37, loss = 0.23334215\n",
      "Iteration 38, loss = 0.12902484\n",
      "Iteration 39, loss = 0.11943158\n",
      "Iteration 40, loss = 0.09866490\n",
      "Iteration 41, loss = 0.07519552\n",
      "Iteration 42, loss = 0.06806556\n",
      "Iteration 43, loss = 0.06495367\n",
      "Iteration 44, loss = 0.05324417\n",
      "Iteration 45, loss = 0.04683947\n",
      "Iteration 46, loss = 0.04521569\n",
      "Iteration 47, loss = 0.04117753\n",
      "Iteration 48, loss = 0.05539833\n",
      "Iteration 49, loss = 0.04151318\n",
      "Iteration 50, loss = 0.03182842\n",
      "Iteration 51, loss = 0.02996242\n",
      "Iteration 52, loss = 0.02502658\n",
      "Iteration 53, loss = 0.02162804\n",
      "Iteration 54, loss = 0.01872352\n",
      "Iteration 55, loss = 0.02352378\n",
      "Iteration 56, loss = 0.01924027\n",
      "Iteration 57, loss = 0.01657319\n",
      "Iteration 58, loss = 0.01696135\n",
      "Iteration 59, loss = 0.01311034\n",
      "Iteration 60, loss = 0.01276382\n",
      "Iteration 61, loss = 0.01196583\n",
      "Iteration 62, loss = 0.01222003\n",
      "Iteration 63, loss = 0.01134734\n",
      "Iteration 64, loss = 0.00911690\n",
      "Iteration 65, loss = 0.01063770\n",
      "Iteration 66, loss = 0.01116982\n",
      "Iteration 67, loss = 0.02844848\n",
      "Iteration 68, loss = 0.01989187\n",
      "Iteration 69, loss = 0.01307678\n",
      "Iteration 70, loss = 0.01027735\n",
      "Iteration 71, loss = 0.00886903\n",
      "Iteration 72, loss = 0.00792853\n",
      "Iteration 73, loss = 0.00683561\n",
      "Iteration 74, loss = 0.00617355\n",
      "Iteration 75, loss = 0.00582129\n",
      "Iteration 76, loss = 0.00601109\n",
      "Iteration 77, loss = 0.00545193\n",
      "Iteration 78, loss = 0.00512598\n",
      "Iteration 79, loss = 0.00528566\n",
      "Iteration 80, loss = 0.00446416\n",
      "Iteration 81, loss = 0.00441671\n",
      "Iteration 82, loss = 0.00412651\n",
      "Iteration 83, loss = 0.00383244\n",
      "Iteration 84, loss = 0.00385232\n",
      "Iteration 85, loss = 0.00360263\n",
      "Iteration 86, loss = 0.00349245\n",
      "Iteration 87, loss = 0.00349621\n",
      "Iteration 88, loss = 0.00328967\n",
      "Iteration 89, loss = 0.00329916\n",
      "Iteration 90, loss = 0.00314675\n",
      "Iteration 91, loss = 0.00300526\n",
      "Iteration 92, loss = 0.00295331\n",
      "Iteration 93, loss = 0.00289128\n",
      "Iteration 94, loss = 0.00282920\n",
      "Iteration 95, loss = 0.00272401\n",
      "Iteration 96, loss = 0.00276437\n",
      "Iteration 97, loss = 0.00269271\n",
      "Iteration 98, loss = 0.00256872\n",
      "Iteration 99, loss = 0.00251290\n",
      "Iteration 100, loss = 0.00242127\n",
      "Iteration 101, loss = 0.00258770\n",
      "Iteration 102, loss = 0.00260349\n",
      "Iteration 103, loss = 0.00254115\n",
      "Iteration 104, loss = 0.00232226\n",
      "Iteration 105, loss = 0.00223101\n",
      "Iteration 106, loss = 0.00224244\n",
      "Iteration 107, loss = 0.00213089\n",
      "Iteration 108, loss = 0.00206775\n",
      "Iteration 109, loss = 0.00211281\n",
      "Iteration 110, loss = 0.00204045\n",
      "Iteration 111, loss = 0.00198166\n",
      "Iteration 112, loss = 0.00188471\n",
      "Iteration 113, loss = 0.00188536\n",
      "Iteration 114, loss = 0.00187287\n",
      "Iteration 115, loss = 0.00187333\n",
      "Iteration 116, loss = 0.00176863\n",
      "Iteration 117, loss = 0.00174365\n",
      "Iteration 118, loss = 0.00179647\n",
      "Iteration 119, loss = 0.00170424\n",
      "Iteration 120, loss = 0.00166154\n",
      "Iteration 121, loss = 0.00167045\n",
      "Iteration 122, loss = 0.00168709\n",
      "Iteration 123, loss = 0.00163834\n",
      "Iteration 124, loss = 0.00157387\n",
      "Iteration 125, loss = 0.00157528\n",
      "Iteration 126, loss = 0.00155298\n",
      "Iteration 127, loss = 0.00150493\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.78224735\n",
      "Iteration 2, loss = 0.65452007\n",
      "Iteration 3, loss = 0.61015387\n",
      "Iteration 4, loss = 0.56486717\n",
      "Iteration 5, loss = 0.53884752\n",
      "Iteration 6, loss = 0.52093493\n",
      "Iteration 7, loss = 0.50382971\n",
      "Iteration 8, loss = 0.48712940\n",
      "Iteration 9, loss = 0.47137855\n",
      "Iteration 10, loss = 0.45584490\n",
      "Iteration 11, loss = 0.44017168\n",
      "Iteration 12, loss = 0.42421763\n",
      "Iteration 13, loss = 0.40827670\n",
      "Iteration 14, loss = 0.39236506\n",
      "Iteration 15, loss = 0.37663204\n",
      "Iteration 16, loss = 0.36091992\n",
      "Iteration 17, loss = 0.34501964\n",
      "Iteration 18, loss = 0.32919460\n",
      "Iteration 19, loss = 0.31385474\n",
      "Iteration 20, loss = 0.29891194\n",
      "Iteration 21, loss = 0.28446050\n",
      "Iteration 22, loss = 0.26997602\n",
      "Iteration 23, loss = 0.25570498\n",
      "Iteration 24, loss = 0.24175746\n",
      "Iteration 25, loss = 0.22860568\n",
      "Iteration 26, loss = 0.21576954\n",
      "Iteration 27, loss = 0.20339997\n",
      "Iteration 28, loss = 0.19166931\n",
      "Iteration 29, loss = 0.18040330\n",
      "Iteration 30, loss = 0.16962218\n",
      "Iteration 31, loss = 0.15924032\n",
      "Iteration 32, loss = 0.14938156\n",
      "Iteration 33, loss = 0.14002983\n",
      "Iteration 34, loss = 0.13119671\n",
      "Iteration 35, loss = 0.12291197\n",
      "Iteration 36, loss = 0.11507862\n",
      "Iteration 37, loss = 0.10760564\n",
      "Iteration 38, loss = 0.10067990\n",
      "Iteration 39, loss = 0.09416388\n",
      "Iteration 40, loss = 0.08806500\n",
      "Iteration 41, loss = 0.08224247\n",
      "Iteration 42, loss = 0.07681242\n",
      "Iteration 43, loss = 0.07163861\n",
      "Iteration 44, loss = 0.06694301\n",
      "Iteration 45, loss = 0.06253682\n",
      "Iteration 46, loss = 0.05844779\n",
      "Iteration 47, loss = 0.05469971\n",
      "Iteration 48, loss = 0.05124149\n",
      "Iteration 49, loss = 0.04808889\n",
      "Iteration 50, loss = 0.04496171\n",
      "Iteration 51, loss = 0.04201867\n",
      "Iteration 52, loss = 0.03943156\n",
      "Iteration 53, loss = 0.03692338\n",
      "Iteration 54, loss = 0.03460929\n",
      "Iteration 55, loss = 0.03251161\n",
      "Iteration 56, loss = 0.03053271\n",
      "Iteration 57, loss = 0.02873948\n",
      "Iteration 58, loss = 0.02711689\n",
      "Iteration 59, loss = 0.02553216\n",
      "Iteration 60, loss = 0.02406934\n",
      "Iteration 61, loss = 0.02271432\n",
      "Iteration 62, loss = 0.02147607\n",
      "Iteration 63, loss = 0.02030034\n",
      "Iteration 64, loss = 0.01924547\n",
      "Iteration 65, loss = 0.01824996\n",
      "Iteration 66, loss = 0.01731320\n",
      "Iteration 67, loss = 0.01646397\n",
      "Iteration 68, loss = 0.01565421\n",
      "Iteration 69, loss = 0.01490840\n",
      "Iteration 70, loss = 0.01422202\n",
      "Iteration 71, loss = 0.01356880\n",
      "Iteration 72, loss = 0.01296600\n",
      "Iteration 73, loss = 0.01239958\n",
      "Iteration 74, loss = 0.01186976\n",
      "Iteration 75, loss = 0.01138177\n",
      "Iteration 76, loss = 0.01092116\n",
      "Iteration 77, loss = 0.01048748\n",
      "Iteration 78, loss = 0.01008787\n",
      "Iteration 79, loss = 0.00971213\n",
      "Iteration 80, loss = 0.00935723\n",
      "Iteration 81, loss = 0.00902011\n",
      "Iteration 82, loss = 0.00870892\n",
      "Iteration 83, loss = 0.00841204\n",
      "Iteration 84, loss = 0.00813555\n",
      "Iteration 85, loss = 0.00787455\n",
      "Iteration 86, loss = 0.00762467\n",
      "Iteration 87, loss = 0.00738902\n",
      "Iteration 88, loss = 0.00716881\n",
      "Iteration 89, loss = 0.00695886\n",
      "Iteration 90, loss = 0.00675913\n",
      "Iteration 91, loss = 0.00656992\n",
      "Iteration 92, loss = 0.00639025\n",
      "Iteration 93, loss = 0.00622037\n",
      "Iteration 94, loss = 0.00605738\n",
      "Iteration 95, loss = 0.00590285\n",
      "Iteration 96, loss = 0.00575744\n",
      "Iteration 97, loss = 0.00561521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 98, loss = 0.00548297\n",
      "Iteration 99, loss = 0.00535313\n",
      "Iteration 100, loss = 0.00523234\n",
      "Iteration 101, loss = 0.00511402\n",
      "Iteration 102, loss = 0.00500101\n",
      "Iteration 103, loss = 0.00489387\n",
      "Iteration 104, loss = 0.00478984\n",
      "Iteration 105, loss = 0.00468954\n",
      "Iteration 106, loss = 0.00459445\n",
      "Iteration 107, loss = 0.00450155\n",
      "Iteration 108, loss = 0.00441393\n",
      "Iteration 109, loss = 0.00432798\n",
      "Iteration 110, loss = 0.00424505\n",
      "Iteration 111, loss = 0.00416620\n",
      "Iteration 112, loss = 0.00408892\n",
      "Iteration 113, loss = 0.00401532\n",
      "Iteration 114, loss = 0.00394329\n",
      "Iteration 115, loss = 0.00387428\n",
      "Iteration 116, loss = 0.00380786\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73189190\n",
      "Iteration 2, loss = 0.65242736\n",
      "Iteration 3, loss = 0.57040758\n",
      "Iteration 4, loss = 0.55844601\n",
      "Iteration 5, loss = 0.60761879\n",
      "Iteration 6, loss = 0.76139735\n",
      "Iteration 7, loss = 0.58167817\n",
      "Iteration 8, loss = 0.56905424\n",
      "Iteration 9, loss = 0.52524296\n",
      "Iteration 10, loss = 0.49058073\n",
      "Iteration 11, loss = 0.50166858\n",
      "Iteration 12, loss = 0.50579623\n",
      "Iteration 13, loss = 0.47132007\n",
      "Iteration 14, loss = 0.44890109\n",
      "Iteration 15, loss = 0.42863694\n",
      "Iteration 16, loss = 0.40770288\n",
      "Iteration 17, loss = 0.40055844\n",
      "Iteration 18, loss = 0.41764180\n",
      "Iteration 19, loss = 0.43930772\n",
      "Iteration 20, loss = 0.37349754\n",
      "Iteration 21, loss = 0.46417123\n",
      "Iteration 22, loss = 0.36746141\n",
      "Iteration 23, loss = 0.42789834\n",
      "Iteration 24, loss = 0.39275485\n",
      "Iteration 25, loss = 0.30582837\n",
      "Iteration 26, loss = 0.33837164\n",
      "Iteration 27, loss = 0.32944204\n",
      "Iteration 28, loss = 0.31028160\n",
      "Iteration 29, loss = 0.29216398\n",
      "Iteration 30, loss = 0.27631303\n",
      "Iteration 31, loss = 0.35733976\n",
      "Iteration 32, loss = 0.27611426\n",
      "Iteration 33, loss = 0.33430269\n",
      "Iteration 34, loss = 0.76807216\n",
      "Iteration 35, loss = 0.36398924\n",
      "Iteration 36, loss = 0.31110321\n",
      "Iteration 37, loss = 0.28428989\n",
      "Iteration 38, loss = 0.26606952\n",
      "Iteration 39, loss = 0.30735963\n",
      "Iteration 40, loss = 0.26182154\n",
      "Iteration 41, loss = 0.28700395\n",
      "Iteration 42, loss = 0.24216224\n",
      "Iteration 43, loss = 0.21381318\n",
      "Iteration 44, loss = 0.19408873\n",
      "Iteration 45, loss = 0.19043402\n",
      "Iteration 46, loss = 0.34914451\n",
      "Iteration 47, loss = 0.18321116\n",
      "Iteration 48, loss = 0.17667715\n",
      "Iteration 49, loss = 0.15797113\n",
      "Iteration 50, loss = 0.13661709\n",
      "Iteration 51, loss = 0.12409408\n",
      "Iteration 52, loss = 0.16009542\n",
      "Iteration 53, loss = 0.19940509\n",
      "Iteration 54, loss = 0.20180715\n",
      "Iteration 55, loss = 0.15169796\n",
      "Iteration 56, loss = 0.14727583\n",
      "Iteration 57, loss = 0.16168252\n",
      "Iteration 58, loss = 0.12005609\n",
      "Iteration 59, loss = 0.15311776\n",
      "Iteration 60, loss = 0.12537707\n",
      "Iteration 61, loss = 0.13317404\n",
      "Iteration 62, loss = 0.11995200\n",
      "Iteration 63, loss = 0.10361590\n",
      "Iteration 64, loss = 0.09684476\n",
      "Iteration 65, loss = 0.10209148\n",
      "Iteration 66, loss = 0.10533965\n",
      "Iteration 67, loss = 0.13205355\n",
      "Iteration 68, loss = 0.09475099\n",
      "Iteration 69, loss = 0.11926657\n",
      "Iteration 70, loss = 0.06684011\n",
      "Iteration 71, loss = 0.24397406\n",
      "Iteration 72, loss = 0.16317823\n",
      "Iteration 73, loss = 0.20464632\n",
      "Iteration 74, loss = 0.15159067\n",
      "Iteration 75, loss = 0.87111606\n",
      "Iteration 76, loss = 0.70593306\n",
      "Iteration 77, loss = 0.21142618\n",
      "Iteration 78, loss = 0.18227845\n",
      "Iteration 79, loss = 1.11033965\n",
      "Iteration 80, loss = 0.40993337\n",
      "Iteration 81, loss = 0.48709848\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69353024\n",
      "Iteration 2, loss = 0.58266380\n",
      "Iteration 3, loss = 0.53812364\n",
      "Iteration 4, loss = 0.51044377\n",
      "Iteration 5, loss = 0.48880849\n",
      "Iteration 6, loss = 0.46114294\n",
      "Iteration 7, loss = 0.44663001\n",
      "Iteration 8, loss = 0.44872291\n",
      "Iteration 9, loss = 0.39832860\n",
      "Iteration 10, loss = 0.38019339\n",
      "Iteration 11, loss = 0.42135228\n",
      "Iteration 12, loss = 0.33767615\n",
      "Iteration 13, loss = 0.34173901\n",
      "Iteration 14, loss = 0.33568508\n",
      "Iteration 15, loss = 0.30224971\n",
      "Iteration 16, loss = 0.41754096\n",
      "Iteration 17, loss = 0.30032776\n",
      "Iteration 18, loss = 0.27469532\n",
      "Iteration 19, loss = 0.24607223\n",
      "Iteration 20, loss = 0.30993335\n",
      "Iteration 21, loss = 0.30299575\n",
      "Iteration 22, loss = 0.23429949\n",
      "Iteration 23, loss = 0.20513966\n",
      "Iteration 24, loss = 0.18527467\n",
      "Iteration 25, loss = 0.17266927\n",
      "Iteration 26, loss = 0.58043927\n",
      "Iteration 27, loss = 0.23078852\n",
      "Iteration 28, loss = 0.21645916\n",
      "Iteration 29, loss = 0.20288272\n",
      "Iteration 30, loss = 0.17243267\n",
      "Iteration 31, loss = 0.14406656\n",
      "Iteration 32, loss = 0.12822367\n",
      "Iteration 33, loss = 0.13112606\n",
      "Iteration 34, loss = 0.13275776\n",
      "Iteration 35, loss = 0.11229507\n",
      "Iteration 36, loss = 0.09233192\n",
      "Iteration 37, loss = 0.10100963\n",
      "Iteration 38, loss = 0.08804863\n",
      "Iteration 39, loss = 0.14012404\n",
      "Iteration 40, loss = 0.11607108\n",
      "Iteration 41, loss = 0.07947969\n",
      "Iteration 42, loss = 0.07502504\n",
      "Iteration 43, loss = 0.07235125\n",
      "Iteration 44, loss = 0.06289749\n",
      "Iteration 45, loss = 0.04791176\n",
      "Iteration 46, loss = 0.05341761\n",
      "Iteration 47, loss = 0.03786006\n",
      "Iteration 48, loss = 0.03397136\n",
      "Iteration 49, loss = 0.03509183\n",
      "Iteration 50, loss = 0.03040839\n",
      "Iteration 51, loss = 0.02970333\n",
      "Iteration 52, loss = 0.02522788\n",
      "Iteration 53, loss = 0.02247184\n",
      "Iteration 54, loss = 0.02039883\n",
      "Iteration 55, loss = 0.02438600\n",
      "Iteration 56, loss = 0.01897860\n",
      "Iteration 57, loss = 0.01791598\n",
      "Iteration 58, loss = 0.01636017\n",
      "Iteration 59, loss = 0.01455811\n",
      "Iteration 60, loss = 0.01397530\n",
      "Iteration 61, loss = 0.01314845\n",
      "Iteration 62, loss = 0.01257970\n",
      "Iteration 63, loss = 0.01258313\n",
      "Iteration 64, loss = 0.01071808\n",
      "Iteration 65, loss = 0.01022820\n",
      "Iteration 66, loss = 0.00946159\n",
      "Iteration 67, loss = 0.00963651\n",
      "Iteration 68, loss = 0.00913746\n",
      "Iteration 69, loss = 0.00857672\n",
      "Iteration 70, loss = 0.00838098\n",
      "Iteration 71, loss = 0.00764410\n",
      "Iteration 72, loss = 0.00756190\n",
      "Iteration 73, loss = 0.00721481\n",
      "Iteration 74, loss = 0.00656535\n",
      "Iteration 75, loss = 0.00635944\n",
      "Iteration 76, loss = 0.00616640\n",
      "Iteration 77, loss = 0.00596327\n",
      "Iteration 78, loss = 0.00583244\n",
      "Iteration 79, loss = 0.00572661\n",
      "Iteration 80, loss = 0.00543563\n",
      "Iteration 81, loss = 0.00514757\n",
      "Iteration 82, loss = 0.00507325\n",
      "Iteration 83, loss = 0.00495566\n",
      "Iteration 84, loss = 0.00471381\n",
      "Iteration 85, loss = 0.00448063\n",
      "Iteration 86, loss = 0.00451625\n",
      "Iteration 87, loss = 0.00443210\n",
      "Iteration 88, loss = 0.00443993\n",
      "Iteration 89, loss = 0.00419065\n",
      "Iteration 90, loss = 0.00395692\n",
      "Iteration 91, loss = 0.00384250\n",
      "Iteration 92, loss = 0.00375494\n",
      "Iteration 93, loss = 0.00362579\n",
      "Iteration 94, loss = 0.00359138\n",
      "Iteration 95, loss = 0.00353594\n",
      "Iteration 96, loss = 0.00341670\n",
      "Iteration 97, loss = 0.00338034\n",
      "Iteration 98, loss = 0.00326129\n",
      "Iteration 99, loss = 0.00317568\n",
      "Iteration 100, loss = 0.00312399\n",
      "Iteration 101, loss = 0.00304901\n",
      "Iteration 102, loss = 0.00300348\n",
      "Iteration 103, loss = 0.00292033\n",
      "Iteration 104, loss = 0.00289437\n",
      "Iteration 105, loss = 0.00282585\n",
      "Iteration 106, loss = 0.00276546\n",
      "Iteration 107, loss = 0.00271352\n",
      "Iteration 108, loss = 0.00265137\n",
      "Iteration 109, loss = 0.00260118\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67509460\n",
      "Iteration 2, loss = 0.57044954\n",
      "Iteration 3, loss = 0.53254395\n",
      "Iteration 4, loss = 0.50706700\n",
      "Iteration 5, loss = 0.46885176\n",
      "Iteration 6, loss = 0.44086433\n",
      "Iteration 7, loss = 0.43957151\n",
      "Iteration 8, loss = 0.41160830\n",
      "Iteration 9, loss = 0.37512032\n",
      "Iteration 10, loss = 0.34513050\n",
      "Iteration 11, loss = 0.34160787\n",
      "Iteration 12, loss = 0.31918080\n",
      "Iteration 13, loss = 0.30825047\n",
      "Iteration 14, loss = 0.50587752\n",
      "Iteration 15, loss = 0.31427086\n",
      "Iteration 16, loss = 0.28763509\n",
      "Iteration 17, loss = 0.25077605\n",
      "Iteration 18, loss = 0.23487403\n",
      "Iteration 19, loss = 0.22913141\n",
      "Iteration 20, loss = 0.20907363\n",
      "Iteration 21, loss = 0.20543635\n",
      "Iteration 22, loss = 0.21347739\n",
      "Iteration 23, loss = 0.18164059\n",
      "Iteration 24, loss = 0.15652395\n",
      "Iteration 25, loss = 0.18092495\n",
      "Iteration 26, loss = 0.14995939\n",
      "Iteration 27, loss = 0.38079260\n",
      "Iteration 28, loss = 0.20846224\n",
      "Iteration 29, loss = 0.16379775\n",
      "Iteration 30, loss = 0.13914012\n",
      "Iteration 31, loss = 0.12916217\n",
      "Iteration 32, loss = 0.11780107\n",
      "Iteration 33, loss = 0.11491101\n",
      "Iteration 34, loss = 0.09820066\n",
      "Iteration 35, loss = 0.09326462\n",
      "Iteration 36, loss = 0.08786181\n",
      "Iteration 37, loss = 0.08093789\n",
      "Iteration 38, loss = 0.08848281\n",
      "Iteration 39, loss = 0.08803969\n",
      "Iteration 40, loss = 0.12135952\n",
      "Iteration 41, loss = 0.06578703\n",
      "Iteration 42, loss = 0.06301507\n",
      "Iteration 43, loss = 0.05247356\n",
      "Iteration 44, loss = 0.05436265\n",
      "Iteration 45, loss = 0.04185884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 46, loss = 0.04019484\n",
      "Iteration 47, loss = 0.03359239\n",
      "Iteration 48, loss = 0.05974011\n",
      "Iteration 49, loss = 0.04878598\n",
      "Iteration 50, loss = 0.04091099\n",
      "Iteration 51, loss = 0.02854111\n",
      "Iteration 52, loss = 0.02492274\n",
      "Iteration 53, loss = 0.02331584\n",
      "Iteration 54, loss = 0.01989225\n",
      "Iteration 55, loss = 0.01714150\n",
      "Iteration 56, loss = 0.01684416\n",
      "Iteration 57, loss = 0.01527487\n",
      "Iteration 58, loss = 0.01635528\n",
      "Iteration 59, loss = 0.02063598\n",
      "Iteration 60, loss = 0.01682381\n",
      "Iteration 61, loss = 0.01324650\n",
      "Iteration 62, loss = 0.01090381\n",
      "Iteration 63, loss = 0.00998890\n",
      "Iteration 64, loss = 0.00902757\n",
      "Iteration 65, loss = 0.01685959\n",
      "Iteration 66, loss = 0.01278053\n",
      "Iteration 67, loss = 0.01786421\n",
      "Iteration 68, loss = 0.01576045\n",
      "Iteration 69, loss = 0.01020570\n",
      "Iteration 70, loss = 0.01487239\n",
      "Iteration 71, loss = 0.00853012\n",
      "Iteration 72, loss = 0.00721531\n",
      "Iteration 73, loss = 0.00744956\n",
      "Iteration 74, loss = 0.00628173\n",
      "Iteration 75, loss = 0.00540997\n",
      "Iteration 76, loss = 0.00529570\n",
      "Iteration 77, loss = 0.00520028\n",
      "Iteration 78, loss = 0.00471354\n",
      "Iteration 79, loss = 0.00471564\n",
      "Iteration 80, loss = 0.00437205\n",
      "Iteration 81, loss = 0.00423707\n",
      "Iteration 82, loss = 0.00405495\n",
      "Iteration 83, loss = 0.00402247\n",
      "Iteration 84, loss = 0.00385906\n",
      "Iteration 85, loss = 0.00377221\n",
      "Iteration 86, loss = 0.00367402\n",
      "Iteration 87, loss = 0.00353337\n",
      "Iteration 88, loss = 0.00345707\n",
      "Iteration 89, loss = 0.00334608\n",
      "Iteration 90, loss = 0.00324778\n",
      "Iteration 91, loss = 0.00314507\n",
      "Iteration 92, loss = 0.00311429\n",
      "Iteration 93, loss = 0.00304214\n",
      "Iteration 94, loss = 0.00295102\n",
      "Iteration 95, loss = 0.00290570\n",
      "Iteration 96, loss = 0.00283151\n",
      "Iteration 97, loss = 0.00277001\n",
      "Iteration 98, loss = 0.00271801\n",
      "Iteration 99, loss = 0.00268090\n",
      "Iteration 100, loss = 0.00265437\n",
      "Iteration 101, loss = 0.00256277\n",
      "Iteration 102, loss = 0.00248274\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64857483\n",
      "Iteration 2, loss = 0.55338267\n",
      "Iteration 3, loss = 0.51550742\n",
      "Iteration 4, loss = 0.46366620\n",
      "Iteration 5, loss = 0.44115544\n",
      "Iteration 6, loss = 0.44713877\n",
      "Iteration 7, loss = 0.37829758\n",
      "Iteration 8, loss = 0.36893584\n",
      "Iteration 9, loss = 0.37815040\n",
      "Iteration 10, loss = 0.32965493\n",
      "Iteration 11, loss = 0.30065354\n",
      "Iteration 12, loss = 0.28271287\n",
      "Iteration 13, loss = 0.25664634\n",
      "Iteration 14, loss = 0.23587785\n",
      "Iteration 15, loss = 0.25651398\n",
      "Iteration 16, loss = 0.33993327\n",
      "Iteration 17, loss = 0.22999978\n",
      "Iteration 18, loss = 0.25511019\n",
      "Iteration 19, loss = 0.20699554\n",
      "Iteration 20, loss = 0.19526536\n",
      "Iteration 21, loss = 0.33150312\n",
      "Iteration 22, loss = 0.18211274\n",
      "Iteration 23, loss = 0.16939724\n",
      "Iteration 24, loss = 0.16514780\n",
      "Iteration 25, loss = 0.13417739\n",
      "Iteration 26, loss = 0.12251862\n",
      "Iteration 27, loss = 0.11792021\n",
      "Iteration 28, loss = 0.13068140\n",
      "Iteration 29, loss = 0.10612088\n",
      "Iteration 30, loss = 0.10157623\n",
      "Iteration 31, loss = 0.09603669\n",
      "Iteration 32, loss = 0.09971513\n",
      "Iteration 33, loss = 0.07379196\n",
      "Iteration 34, loss = 0.26777611\n",
      "Iteration 35, loss = 0.13132273\n",
      "Iteration 36, loss = 0.10227330\n",
      "Iteration 37, loss = 0.11185212\n",
      "Iteration 38, loss = 0.06759420\n",
      "Iteration 39, loss = 0.06149842\n",
      "Iteration 40, loss = 0.05753906\n",
      "Iteration 41, loss = 0.04927673\n",
      "Iteration 42, loss = 0.03814685\n",
      "Iteration 43, loss = 0.04003809\n",
      "Iteration 44, loss = 0.06185978\n",
      "Iteration 45, loss = 0.04403024\n",
      "Iteration 46, loss = 0.03908180\n",
      "Iteration 47, loss = 0.03645195\n",
      "Iteration 48, loss = 0.03920014\n",
      "Iteration 49, loss = 0.02722120\n",
      "Iteration 50, loss = 0.02628532\n",
      "Iteration 51, loss = 0.02543450\n",
      "Iteration 52, loss = 0.01916151\n",
      "Iteration 53, loss = 0.01667530\n",
      "Iteration 54, loss = 0.01625661\n",
      "Iteration 55, loss = 0.01869749\n",
      "Iteration 56, loss = 0.01719873\n",
      "Iteration 57, loss = 0.01943630\n",
      "Iteration 58, loss = 0.01295549\n",
      "Iteration 59, loss = 0.01250483\n",
      "Iteration 60, loss = 0.01053579\n",
      "Iteration 61, loss = 0.01338288\n",
      "Iteration 62, loss = 0.01470926\n",
      "Iteration 63, loss = 0.01023041\n",
      "Iteration 64, loss = 0.00867131\n",
      "Iteration 65, loss = 0.00775485\n",
      "Iteration 66, loss = 0.00835315\n",
      "Iteration 67, loss = 0.02994419\n",
      "Iteration 68, loss = 0.01272650\n",
      "Iteration 69, loss = 0.00849709\n",
      "Iteration 70, loss = 0.00784146\n",
      "Iteration 71, loss = 0.00676600\n",
      "Iteration 72, loss = 0.00632413\n",
      "Iteration 73, loss = 0.00733260\n",
      "Iteration 74, loss = 0.00598951\n",
      "Iteration 75, loss = 0.00573557\n",
      "Iteration 76, loss = 0.00664656\n",
      "Iteration 77, loss = 0.00572359\n",
      "Iteration 78, loss = 0.00475069\n",
      "Iteration 79, loss = 0.00548517\n",
      "Iteration 80, loss = 0.00500150\n",
      "Iteration 81, loss = 0.00439543\n",
      "Iteration 82, loss = 0.00398761\n",
      "Iteration 83, loss = 0.00394989\n",
      "Iteration 84, loss = 0.00449554\n",
      "Iteration 85, loss = 0.00389721\n",
      "Iteration 86, loss = 0.00375149\n",
      "Iteration 87, loss = 0.00397490\n",
      "Iteration 88, loss = 0.00988232\n",
      "Iteration 89, loss = 0.00690902\n",
      "Iteration 90, loss = 0.00493811\n",
      "Iteration 91, loss = 0.00364188\n",
      "Iteration 92, loss = 0.00320340\n",
      "Iteration 93, loss = 0.00375507\n",
      "Iteration 94, loss = 0.00316804\n",
      "Iteration 95, loss = 0.00321570\n",
      "Iteration 96, loss = 0.00285173\n",
      "Iteration 97, loss = 0.00318887\n",
      "Iteration 98, loss = 0.00305414\n",
      "Iteration 99, loss = 0.00250075\n",
      "Iteration 100, loss = 0.00236539\n",
      "Iteration 101, loss = 0.00334224\n",
      "Iteration 102, loss = 0.00298742\n",
      "Iteration 103, loss = 0.00306656\n",
      "Iteration 104, loss = 0.00277660\n",
      "Iteration 105, loss = 0.00457758\n",
      "Iteration 106, loss = 0.00465643\n",
      "Iteration 107, loss = 0.00240808\n",
      "Iteration 108, loss = 0.00218408\n",
      "Iteration 109, loss = 0.00254747\n",
      "Iteration 110, loss = 0.00207638\n",
      "Iteration 111, loss = 0.00192028\n",
      "Iteration 112, loss = 0.00181932\n",
      "Iteration 113, loss = 0.00176073\n",
      "Iteration 114, loss = 0.00171618\n",
      "Iteration 115, loss = 0.00193320\n",
      "Iteration 116, loss = 0.00170921\n",
      "Iteration 117, loss = 0.00169790\n",
      "Iteration 118, loss = 0.00162798\n",
      "Iteration 119, loss = 0.00159443\n",
      "Iteration 120, loss = 0.00148332\n",
      "Iteration 121, loss = 0.00152021\n",
      "Iteration 122, loss = 0.00149259\n",
      "Iteration 123, loss = 0.00146496\n",
      "Iteration 124, loss = 0.00142343\n",
      "Iteration 125, loss = 0.00152217\n",
      "Iteration 126, loss = 0.00145602\n",
      "Iteration 127, loss = 0.00141737\n",
      "Iteration 128, loss = 0.00131361\n",
      "Iteration 129, loss = 0.00136098\n",
      "Iteration 130, loss = 0.00150109\n",
      "Iteration 131, loss = 0.00139491\n",
      "Iteration 132, loss = 0.00127782\n",
      "Iteration 133, loss = 0.00137449\n",
      "Iteration 134, loss = 0.00134639\n",
      "Iteration 135, loss = 0.00135959\n",
      "Iteration 136, loss = 0.00129667\n",
      "Iteration 137, loss = 0.00122535\n",
      "Iteration 138, loss = 0.00126953\n",
      "Iteration 139, loss = 0.00119733\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.78224735\n",
      "Iteration 2, loss = 0.65452007\n",
      "Iteration 3, loss = 0.61015387\n",
      "Iteration 4, loss = 0.56486717\n",
      "Iteration 5, loss = 0.53884752\n",
      "Iteration 6, loss = 0.52093493\n",
      "Iteration 7, loss = 0.50382971\n",
      "Iteration 8, loss = 0.48712940\n",
      "Iteration 9, loss = 0.47137855\n",
      "Iteration 10, loss = 0.45584490\n",
      "Iteration 11, loss = 0.44017168\n",
      "Iteration 12, loss = 0.42421763\n",
      "Iteration 13, loss = 0.40827670\n",
      "Iteration 14, loss = 0.39236506\n",
      "Iteration 15, loss = 0.37663204\n",
      "Iteration 16, loss = 0.36091992\n",
      "Iteration 17, loss = 0.34501964\n",
      "Iteration 18, loss = 0.32919460\n",
      "Iteration 19, loss = 0.31385474\n",
      "Iteration 20, loss = 0.29891194\n",
      "Iteration 21, loss = 0.28446050\n",
      "Iteration 22, loss = 0.26997602\n",
      "Iteration 23, loss = 0.25570498\n",
      "Iteration 24, loss = 0.24175746\n",
      "Iteration 25, loss = 0.22860568\n",
      "Iteration 26, loss = 0.21576954\n",
      "Iteration 27, loss = 0.20339997\n",
      "Iteration 28, loss = 0.19166931\n",
      "Iteration 29, loss = 0.18040330\n",
      "Iteration 30, loss = 0.16962218\n",
      "Iteration 31, loss = 0.15924032\n",
      "Iteration 32, loss = 0.14938156\n",
      "Iteration 33, loss = 0.14002983\n",
      "Iteration 34, loss = 0.13119671\n",
      "Iteration 35, loss = 0.12291197\n",
      "Iteration 36, loss = 0.11507862\n",
      "Iteration 37, loss = 0.10760564\n",
      "Iteration 38, loss = 0.10067990\n",
      "Iteration 39, loss = 0.09416388\n",
      "Iteration 40, loss = 0.08806500\n",
      "Iteration 41, loss = 0.08224247\n",
      "Iteration 42, loss = 0.07681242\n",
      "Iteration 43, loss = 0.07163861\n",
      "Iteration 44, loss = 0.06694301\n",
      "Iteration 45, loss = 0.06253682\n",
      "Iteration 46, loss = 0.05844779\n",
      "Iteration 47, loss = 0.05469971\n",
      "Iteration 48, loss = 0.05124149\n",
      "Iteration 49, loss = 0.04808889\n",
      "Iteration 50, loss = 0.04496171\n",
      "Iteration 51, loss = 0.04201867\n",
      "Iteration 52, loss = 0.03943156\n",
      "Iteration 53, loss = 0.03692338\n",
      "Iteration 54, loss = 0.03460929\n",
      "Iteration 55, loss = 0.03251161\n",
      "Iteration 56, loss = 0.03053271\n",
      "Iteration 57, loss = 0.02873948\n",
      "Iteration 58, loss = 0.02711689\n",
      "Iteration 59, loss = 0.02553216\n",
      "Iteration 60, loss = 0.02406934\n",
      "Iteration 61, loss = 0.02271432\n",
      "Iteration 62, loss = 0.02147607\n",
      "Iteration 63, loss = 0.02030034\n",
      "Iteration 64, loss = 0.01924547\n",
      "Iteration 65, loss = 0.01824996\n",
      "Iteration 66, loss = 0.01731320\n",
      "Iteration 67, loss = 0.01646397\n",
      "Iteration 68, loss = 0.01565421\n",
      "Iteration 69, loss = 0.01490840\n",
      "Iteration 70, loss = 0.01422202\n",
      "Iteration 71, loss = 0.01356880\n",
      "Iteration 72, loss = 0.01296600\n",
      "Iteration 73, loss = 0.01239958\n",
      "Iteration 74, loss = 0.01186976\n",
      "Iteration 75, loss = 0.01138177\n",
      "Iteration 76, loss = 0.01092116\n",
      "Iteration 77, loss = 0.01048748\n",
      "Iteration 78, loss = 0.01008787\n",
      "Iteration 79, loss = 0.00971213\n",
      "Iteration 80, loss = 0.00935723\n",
      "Iteration 81, loss = 0.00902011\n",
      "Iteration 82, loss = 0.00870892\n",
      "Iteration 83, loss = 0.00841204\n",
      "Iteration 84, loss = 0.00813555\n",
      "Iteration 85, loss = 0.00787455\n",
      "Iteration 86, loss = 0.00762467\n",
      "Iteration 87, loss = 0.00738902\n",
      "Iteration 88, loss = 0.00716881\n",
      "Iteration 89, loss = 0.00695886\n",
      "Iteration 90, loss = 0.00675913\n",
      "Iteration 91, loss = 0.00656992\n",
      "Iteration 92, loss = 0.00639025\n",
      "Iteration 93, loss = 0.00622037\n",
      "Iteration 94, loss = 0.00605738\n",
      "Iteration 95, loss = 0.00590285\n",
      "Iteration 96, loss = 0.00575744\n",
      "Iteration 97, loss = 0.00561521\n",
      "Iteration 98, loss = 0.00548297\n",
      "Iteration 99, loss = 0.00535313\n",
      "Iteration 100, loss = 0.00523234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 101, loss = 0.00511402\n",
      "Iteration 102, loss = 0.00500101\n",
      "Iteration 103, loss = 0.00489387\n",
      "Iteration 104, loss = 0.00478984\n",
      "Iteration 105, loss = 0.00468954\n",
      "Iteration 106, loss = 0.00459445\n",
      "Iteration 107, loss = 0.00450155\n",
      "Iteration 108, loss = 0.00441393\n",
      "Iteration 109, loss = 0.00432798\n",
      "Iteration 110, loss = 0.00424505\n",
      "Iteration 111, loss = 0.00416620\n",
      "Iteration 112, loss = 0.00408892\n",
      "Iteration 113, loss = 0.00401532\n",
      "Iteration 114, loss = 0.00394329\n",
      "Iteration 115, loss = 0.00387428\n",
      "Iteration 116, loss = 0.00380786\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73189190\n",
      "Iteration 2, loss = 0.65242736\n",
      "Iteration 3, loss = 0.57040758\n",
      "Iteration 4, loss = 0.55844601\n",
      "Iteration 5, loss = 0.60761879\n",
      "Iteration 6, loss = 0.76139735\n",
      "Iteration 7, loss = 0.58167817\n",
      "Iteration 8, loss = 0.56905424\n",
      "Iteration 9, loss = 0.52524296\n",
      "Iteration 10, loss = 0.49058073\n",
      "Iteration 11, loss = 0.50166858\n",
      "Iteration 12, loss = 0.50579623\n",
      "Iteration 13, loss = 0.47132007\n",
      "Iteration 14, loss = 0.44890109\n",
      "Iteration 15, loss = 0.42863694\n",
      "Iteration 16, loss = 0.40770288\n",
      "Iteration 17, loss = 0.40055844\n",
      "Iteration 18, loss = 0.41764180\n",
      "Iteration 19, loss = 0.43930772\n",
      "Iteration 20, loss = 0.37349754\n",
      "Iteration 21, loss = 0.46417123\n",
      "Iteration 22, loss = 0.36746141\n",
      "Iteration 23, loss = 0.42789834\n",
      "Iteration 24, loss = 0.39275485\n",
      "Iteration 25, loss = 0.30582837\n",
      "Iteration 26, loss = 0.33837164\n",
      "Iteration 27, loss = 0.32944204\n",
      "Iteration 28, loss = 0.31028160\n",
      "Iteration 29, loss = 0.29216398\n",
      "Iteration 30, loss = 0.27631303\n",
      "Iteration 31, loss = 0.35733976\n",
      "Iteration 32, loss = 0.27611426\n",
      "Iteration 33, loss = 0.33430269\n",
      "Iteration 34, loss = 0.76807216\n",
      "Iteration 35, loss = 0.36398924\n",
      "Iteration 36, loss = 0.31110321\n",
      "Iteration 37, loss = 0.28428989\n",
      "Iteration 38, loss = 0.26606952\n",
      "Iteration 39, loss = 0.30735963\n",
      "Iteration 40, loss = 0.26182154\n",
      "Iteration 41, loss = 0.28700395\n",
      "Iteration 42, loss = 0.24216224\n",
      "Iteration 43, loss = 0.21381318\n",
      "Iteration 44, loss = 0.19408873\n",
      "Iteration 45, loss = 0.19043402\n",
      "Iteration 46, loss = 0.34914451\n",
      "Iteration 47, loss = 0.18321116\n",
      "Iteration 48, loss = 0.17667715\n",
      "Iteration 49, loss = 0.15797113\n",
      "Iteration 50, loss = 0.13661709\n",
      "Iteration 51, loss = 0.12409408\n",
      "Iteration 52, loss = 0.16009542\n",
      "Iteration 53, loss = 0.19940509\n",
      "Iteration 54, loss = 0.20180715\n",
      "Iteration 55, loss = 0.15169796\n",
      "Iteration 56, loss = 0.14727583\n",
      "Iteration 57, loss = 0.16168252\n",
      "Iteration 58, loss = 0.12005609\n",
      "Iteration 59, loss = 0.15311776\n",
      "Iteration 60, loss = 0.12537707\n",
      "Iteration 61, loss = 0.13317404\n",
      "Iteration 62, loss = 0.11995200\n",
      "Iteration 63, loss = 0.10361590\n",
      "Iteration 64, loss = 0.09684476\n",
      "Iteration 65, loss = 0.10209148\n",
      "Iteration 66, loss = 0.10533965\n",
      "Iteration 67, loss = 0.13205355\n",
      "Iteration 68, loss = 0.09475099\n",
      "Iteration 69, loss = 0.11926657\n",
      "Iteration 70, loss = 0.06684011\n",
      "Iteration 71, loss = 0.24397406\n",
      "Iteration 72, loss = 0.16317823\n",
      "Iteration 73, loss = 0.20464632\n",
      "Iteration 74, loss = 0.15159067\n",
      "Iteration 75, loss = 0.87111606\n",
      "Iteration 76, loss = 0.70593306\n",
      "Iteration 77, loss = 0.21142618\n",
      "Iteration 78, loss = 0.18227845\n",
      "Iteration 79, loss = 1.11033965\n",
      "Iteration 80, loss = 0.40993337\n",
      "Iteration 81, loss = 0.48709848\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69353024\n",
      "Iteration 2, loss = 0.58266380\n",
      "Iteration 3, loss = 0.53812364\n",
      "Iteration 4, loss = 0.51044377\n",
      "Iteration 5, loss = 0.48880849\n",
      "Iteration 6, loss = 0.46114294\n",
      "Iteration 7, loss = 0.44663001\n",
      "Iteration 8, loss = 0.44872291\n",
      "Iteration 9, loss = 0.39832860\n",
      "Iteration 10, loss = 0.38019339\n",
      "Iteration 11, loss = 0.42135228\n",
      "Iteration 12, loss = 0.33767615\n",
      "Iteration 13, loss = 0.34173901\n",
      "Iteration 14, loss = 0.33568508\n",
      "Iteration 15, loss = 0.30224971\n",
      "Iteration 16, loss = 0.41754096\n",
      "Iteration 17, loss = 0.30032776\n",
      "Iteration 18, loss = 0.27469532\n",
      "Iteration 19, loss = 0.24607223\n",
      "Iteration 20, loss = 0.30993335\n",
      "Iteration 21, loss = 0.30299575\n",
      "Iteration 22, loss = 0.23429949\n",
      "Iteration 23, loss = 0.20513966\n",
      "Iteration 24, loss = 0.18527467\n",
      "Iteration 25, loss = 0.17266927\n",
      "Iteration 26, loss = 0.58043927\n",
      "Iteration 27, loss = 0.23078852\n",
      "Iteration 28, loss = 0.21645916\n",
      "Iteration 29, loss = 0.20288272\n",
      "Iteration 30, loss = 0.17243267\n",
      "Iteration 31, loss = 0.14406656\n",
      "Iteration 32, loss = 0.12822367\n",
      "Iteration 33, loss = 0.13112606\n",
      "Iteration 34, loss = 0.13275776\n",
      "Iteration 35, loss = 0.11229507\n",
      "Iteration 36, loss = 0.09233192\n",
      "Iteration 37, loss = 0.10100963\n",
      "Iteration 38, loss = 0.08804863\n",
      "Iteration 39, loss = 0.14012404\n",
      "Iteration 40, loss = 0.11607108\n",
      "Iteration 41, loss = 0.07947969\n",
      "Iteration 42, loss = 0.07502504\n",
      "Iteration 43, loss = 0.07235125\n",
      "Iteration 44, loss = 0.06289749\n",
      "Iteration 45, loss = 0.04791176\n",
      "Iteration 46, loss = 0.05341761\n",
      "Iteration 47, loss = 0.03786006\n",
      "Iteration 48, loss = 0.03397136\n",
      "Iteration 49, loss = 0.03509183\n",
      "Iteration 50, loss = 0.03040839\n",
      "Iteration 51, loss = 0.02970333\n",
      "Iteration 52, loss = 0.02522788\n",
      "Iteration 53, loss = 0.02247184\n",
      "Iteration 54, loss = 0.02039883\n",
      "Iteration 55, loss = 0.02438600\n",
      "Iteration 56, loss = 0.01897860\n",
      "Iteration 57, loss = 0.01791598\n",
      "Iteration 58, loss = 0.01636017\n",
      "Iteration 59, loss = 0.01455811\n",
      "Iteration 60, loss = 0.01397530\n",
      "Iteration 61, loss = 0.01314845\n",
      "Iteration 62, loss = 0.01257970\n",
      "Iteration 63, loss = 0.01258313\n",
      "Iteration 64, loss = 0.01071808\n",
      "Iteration 65, loss = 0.01022820\n",
      "Iteration 66, loss = 0.00946159\n",
      "Iteration 67, loss = 0.00963651\n",
      "Iteration 68, loss = 0.00913746\n",
      "Iteration 69, loss = 0.00857672\n",
      "Iteration 70, loss = 0.00838098\n",
      "Iteration 71, loss = 0.00764410\n",
      "Iteration 72, loss = 0.00756190\n",
      "Iteration 73, loss = 0.00721481\n",
      "Iteration 74, loss = 0.00656535\n",
      "Iteration 75, loss = 0.00635944\n",
      "Iteration 76, loss = 0.00616640\n",
      "Iteration 77, loss = 0.00596327\n",
      "Iteration 78, loss = 0.00583244\n",
      "Iteration 79, loss = 0.00572661\n",
      "Iteration 80, loss = 0.00543563\n",
      "Iteration 81, loss = 0.00514757\n",
      "Iteration 82, loss = 0.00507325\n",
      "Iteration 83, loss = 0.00495566\n",
      "Iteration 84, loss = 0.00471381\n",
      "Iteration 85, loss = 0.00448063\n",
      "Iteration 86, loss = 0.00451625\n",
      "Iteration 87, loss = 0.00443210\n",
      "Iteration 88, loss = 0.00443993\n",
      "Iteration 89, loss = 0.00419065\n",
      "Iteration 90, loss = 0.00395692\n",
      "Iteration 91, loss = 0.00384250\n",
      "Iteration 92, loss = 0.00375494\n",
      "Iteration 93, loss = 0.00362579\n",
      "Iteration 94, loss = 0.00359138\n",
      "Iteration 95, loss = 0.00353594\n",
      "Iteration 96, loss = 0.00341670\n",
      "Iteration 97, loss = 0.00338034\n",
      "Iteration 98, loss = 0.00326129\n",
      "Iteration 99, loss = 0.00317568\n",
      "Iteration 100, loss = 0.00312399\n",
      "Iteration 101, loss = 0.00304901\n",
      "Iteration 102, loss = 0.00300348\n",
      "Iteration 103, loss = 0.00292033\n",
      "Iteration 104, loss = 0.00289437\n",
      "Iteration 105, loss = 0.00282585\n",
      "Iteration 106, loss = 0.00276546\n",
      "Iteration 107, loss = 0.00271352\n",
      "Iteration 108, loss = 0.00265137\n",
      "Iteration 109, loss = 0.00260118\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67844046\n",
      "Iteration 2, loss = 0.57080193\n",
      "Iteration 3, loss = 0.53283735\n",
      "Iteration 4, loss = 0.50775703\n",
      "Iteration 5, loss = 0.47130776\n",
      "Iteration 6, loss = 0.44582314\n",
      "Iteration 7, loss = 0.46331466\n",
      "Iteration 8, loss = 0.40857504\n",
      "Iteration 9, loss = 0.37150572\n",
      "Iteration 10, loss = 0.34000186\n",
      "Iteration 11, loss = 0.34566465\n",
      "Iteration 12, loss = 0.32602839\n",
      "Iteration 13, loss = 0.30299620\n",
      "Iteration 14, loss = 0.42461966\n",
      "Iteration 15, loss = 0.28918130\n",
      "Iteration 16, loss = 0.28074947\n",
      "Iteration 17, loss = 0.24445552\n",
      "Iteration 18, loss = 0.23267342\n",
      "Iteration 19, loss = 0.23250218\n",
      "Iteration 20, loss = 0.20218227\n",
      "Iteration 21, loss = 0.19787590\n",
      "Iteration 22, loss = 0.23149228\n",
      "Iteration 23, loss = 0.18307525\n",
      "Iteration 24, loss = 0.15232672\n",
      "Iteration 25, loss = 0.20065200\n",
      "Iteration 26, loss = 0.50934334\n",
      "Iteration 27, loss = 0.22758981\n",
      "Iteration 28, loss = 0.21754563\n",
      "Iteration 29, loss = 0.17833839\n",
      "Iteration 30, loss = 0.15861112\n",
      "Iteration 31, loss = 0.14222383\n",
      "Iteration 32, loss = 0.13370569\n",
      "Iteration 33, loss = 0.11788072\n",
      "Iteration 34, loss = 0.10859930\n",
      "Iteration 35, loss = 0.09856771\n",
      "Iteration 36, loss = 0.08969796\n",
      "Iteration 37, loss = 0.08101541\n",
      "Iteration 38, loss = 0.07827684\n",
      "Iteration 39, loss = 0.08373732\n",
      "Iteration 40, loss = 0.07966746\n",
      "Iteration 41, loss = 0.05812696\n",
      "Iteration 42, loss = 0.05833191\n",
      "Iteration 43, loss = 0.05103735\n",
      "Iteration 44, loss = 0.05806204\n",
      "Iteration 45, loss = 0.04230944\n",
      "Iteration 46, loss = 0.03931655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 47, loss = 0.03574844\n",
      "Iteration 48, loss = 0.06446310\n",
      "Iteration 49, loss = 0.04839952\n",
      "Iteration 50, loss = 0.04655376\n",
      "Iteration 51, loss = 0.03289472\n",
      "Iteration 52, loss = 0.02554570\n",
      "Iteration 53, loss = 0.02467253\n",
      "Iteration 54, loss = 0.02041262\n",
      "Iteration 55, loss = 0.01967695\n",
      "Iteration 56, loss = 0.01818655\n",
      "Iteration 57, loss = 0.01652812\n",
      "Iteration 58, loss = 0.01616169\n",
      "Iteration 59, loss = 0.01645402\n",
      "Iteration 60, loss = 0.01600085\n",
      "Iteration 61, loss = 0.01225498\n",
      "Iteration 62, loss = 0.01038718\n",
      "Iteration 63, loss = 0.01004854\n",
      "Iteration 64, loss = 0.00882710\n",
      "Iteration 65, loss = 0.01135168\n",
      "Iteration 66, loss = 0.00943068\n",
      "Iteration 67, loss = 0.00894506\n",
      "Iteration 68, loss = 0.00749012\n",
      "Iteration 69, loss = 0.00713099\n",
      "Iteration 70, loss = 0.00706538\n",
      "Iteration 71, loss = 0.00647679\n",
      "Iteration 72, loss = 0.00600345\n",
      "Iteration 73, loss = 0.00571876\n",
      "Iteration 74, loss = 0.00540658\n",
      "Iteration 75, loss = 0.00519039\n",
      "Iteration 76, loss = 0.00519085\n",
      "Iteration 77, loss = 0.00508522\n",
      "Iteration 78, loss = 0.00470041\n",
      "Iteration 79, loss = 0.00459148\n",
      "Iteration 80, loss = 0.00439734\n",
      "Iteration 81, loss = 0.00427455\n",
      "Iteration 82, loss = 0.00408440\n",
      "Iteration 83, loss = 0.00403052\n",
      "Iteration 84, loss = 0.00385472\n",
      "Iteration 85, loss = 0.00379808\n",
      "Iteration 86, loss = 0.00374358\n",
      "Iteration 87, loss = 0.00354479\n",
      "Iteration 88, loss = 0.00355267\n",
      "Iteration 89, loss = 0.00341233\n",
      "Iteration 90, loss = 0.00329134\n",
      "Iteration 91, loss = 0.00318881\n",
      "Iteration 92, loss = 0.00313712\n",
      "Iteration 93, loss = 0.00305725\n",
      "Iteration 94, loss = 0.00299890\n",
      "Iteration 95, loss = 0.00292024\n",
      "Iteration 96, loss = 0.00284738\n",
      "Iteration 97, loss = 0.00279830\n",
      "Iteration 98, loss = 0.00273236\n",
      "Iteration 99, loss = 0.00269016\n",
      "Iteration 100, loss = 0.00266566\n",
      "Iteration 101, loss = 0.00258187\n",
      "Iteration 102, loss = 0.00251102\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65438931\n",
      "Iteration 2, loss = 0.56346906\n",
      "Iteration 3, loss = 0.51616680\n",
      "Iteration 4, loss = 0.47145298\n",
      "Iteration 5, loss = 0.44985079\n",
      "Iteration 6, loss = 0.41647323\n",
      "Iteration 7, loss = 0.39842475\n",
      "Iteration 8, loss = 0.38679051\n",
      "Iteration 9, loss = 0.34310778\n",
      "Iteration 10, loss = 0.37759500\n",
      "Iteration 11, loss = 0.31305299\n",
      "Iteration 12, loss = 0.27933159\n",
      "Iteration 13, loss = 0.26210133\n",
      "Iteration 14, loss = 0.29841591\n",
      "Iteration 15, loss = 0.28193638\n",
      "Iteration 16, loss = 0.21824604\n",
      "Iteration 17, loss = 0.20214634\n",
      "Iteration 18, loss = 0.28922688\n",
      "Iteration 19, loss = 0.20226695\n",
      "Iteration 20, loss = 0.20285020\n",
      "Iteration 21, loss = 0.22815146\n",
      "Iteration 22, loss = 0.15689627\n",
      "Iteration 23, loss = 0.16124321\n",
      "Iteration 24, loss = 0.15359827\n",
      "Iteration 25, loss = 0.12405579\n",
      "Iteration 26, loss = 0.12073728\n",
      "Iteration 27, loss = 0.10892698\n",
      "Iteration 28, loss = 0.11087978\n",
      "Iteration 29, loss = 0.10591265\n",
      "Iteration 30, loss = 0.09419410\n",
      "Iteration 31, loss = 0.10323144\n",
      "Iteration 32, loss = 0.09134730\n",
      "Iteration 33, loss = 0.07911869\n",
      "Iteration 34, loss = 0.12150690\n",
      "Iteration 35, loss = 0.11022300\n",
      "Iteration 36, loss = 0.06491883\n",
      "Iteration 37, loss = 0.05623090\n",
      "Iteration 38, loss = 0.04607664\n",
      "Iteration 39, loss = 0.04120530\n",
      "Iteration 40, loss = 0.05222581\n",
      "Iteration 41, loss = 0.06511480\n",
      "Iteration 42, loss = 0.05453321\n",
      "Iteration 43, loss = 0.08568141\n",
      "Iteration 44, loss = 0.11656381\n",
      "Iteration 45, loss = 0.36353199\n",
      "Iteration 46, loss = 0.78746231\n",
      "Iteration 47, loss = 0.32177441\n",
      "Iteration 48, loss = 0.28548414\n",
      "Iteration 49, loss = 0.21308400\n",
      "Iteration 50, loss = 0.22026957\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "model = MLPClassifier(hidden_layer_sizes=(100,  100), max_iter=1000, alpha=1e-4, solver='sgd', verbose=10, random_state=1, learning_rate_init=.1)\n",
    "scaler = preprocessing.StandardScaler(with_mean=True).fit(X_train)\n",
    "learningFeatures_scaled = scaler.transform(X_train)\n",
    "\n",
    "# Plot learning curve\n",
    "train_sizes, train_scores, test_scores = learning_curve(model, learningFeatures_scaled, learningLabelsStd, cv=5)\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                 color=\"r\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "         label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "         label=\"Cross-validation score\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3185756d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "[CV 1/5; 1/100] START alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=sgd\n",
      "[CV 1/5; 1/100] END alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=sgd;, score=0.500 total time=   0.2s\n",
      "[CV 2/5; 1/100] START alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=sgd\n",
      "[CV 2/5; 1/100] END alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=sgd;, score=0.498 total time=   0.2s\n",
      "[CV 3/5; 1/100] START alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=sgd\n",
      "[CV 3/5; 1/100] END alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=sgd;, score=0.498 total time=   0.2s\n",
      "[CV 4/5; 1/100] START alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=sgd\n",
      "[CV 4/5; 1/100] END alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=sgd;, score=0.505 total time=   0.2s\n",
      "[CV 5/5; 1/100] START alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=sgd\n",
      "[CV 5/5; 1/100] END alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=sgd;, score=0.498 total time=   0.2s\n",
      "[CV 1/5; 2/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam\n",
      "[CV 1/5; 2/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam;, score=0.500 total time=   0.5s\n",
      "[CV 2/5; 2/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam\n",
      "[CV 2/5; 2/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam;, score=0.498 total time=   0.5s\n",
      "[CV 3/5; 2/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam\n",
      "[CV 3/5; 2/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam;, score=0.502 total time=   0.3s\n",
      "[CV 4/5; 2/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam\n",
      "[CV 4/5; 2/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam;, score=0.498 total time=   1.1s\n",
      "[CV 5/5; 2/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam\n",
      "[CV 5/5; 2/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam;, score=0.498 total time=   0.2s\n",
      "[CV 1/5; 3/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 3/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam;, score=0.500 total time=  20.7s\n",
      "[CV 2/5; 3/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 3/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam;, score=0.502 total time=  21.7s\n",
      "[CV 3/5; 3/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 3/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam;, score=0.502 total time=  18.9s\n",
      "[CV 4/5; 3/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 3/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam;, score=0.502 total time=  19.2s\n",
      "[CV 5/5; 3/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam\n",
      "[CV 5/5; 3/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam;, score=0.502 total time=  21.7s\n",
      "[CV 1/5; 4/100] START alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam\n",
      "[CV 1/5; 4/100] END alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam;, score=0.566 total time=   3.0s\n",
      "[CV 2/5; 4/100] START alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam\n",
      "[CV 2/5; 4/100] END alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam;, score=0.502 total time=   0.3s\n",
      "[CV 3/5; 4/100] START alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam\n",
      "[CV 3/5; 4/100] END alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam;, score=0.502 total time=   0.2s\n",
      "[CV 4/5; 4/100] START alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam\n",
      "[CV 4/5; 4/100] END alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam;, score=0.502 total time=   5.2s\n",
      "[CV 5/5; 4/100] START alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam\n",
      "[CV 5/5; 4/100] END alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam;, score=0.498 total time=   0.3s\n",
      "[CV 1/5; 5/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=sgd\n",
      "[CV 1/5; 5/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=sgd;, score=0.500 total time=   0.2s\n",
      "[CV 2/5; 5/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=sgd\n",
      "[CV 2/5; 5/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=sgd;, score=0.505 total time=   0.2s\n",
      "[CV 3/5; 5/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=sgd\n",
      "[CV 3/5; 5/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=sgd;, score=0.502 total time=   0.2s\n",
      "[CV 4/5; 5/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=sgd\n",
      "[CV 4/5; 5/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=sgd;, score=0.502 total time=   0.3s\n",
      "[CV 5/5; 5/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=sgd\n",
      "[CV 5/5; 5/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=sgd;, score=0.502 total time=   0.2s\n",
      "[CV 1/5; 6/100] START alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam\n",
      "[CV 1/5; 6/100] END alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam;, score=0.752 total time=   1.7s\n",
      "[CV 2/5; 6/100] START alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam\n",
      "[CV 2/5; 6/100] END alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam;, score=0.678 total time=   1.3s\n",
      "[CV 3/5; 6/100] START alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam\n",
      "[CV 3/5; 6/100] END alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam;, score=0.760 total time=   3.0s\n",
      "[CV 4/5; 6/100] START alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam\n",
      "[CV 4/5; 6/100] END alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam;, score=0.691 total time=   1.4s\n",
      "[CV 5/5; 6/100] START alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam\n",
      "[CV 5/5; 6/100] END alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam;, score=0.817 total time=   1.5s\n",
      "[CV 1/5; 7/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam\n",
      "[CV 1/5; 7/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam;, score=0.657 total time=   1.5s\n",
      "[CV 2/5; 7/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam\n",
      "[CV 2/5; 7/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam;, score=0.744 total time=   1.9s\n",
      "[CV 3/5; 7/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam\n",
      "[CV 3/5; 7/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam;, score=0.681 total time=   2.2s\n",
      "[CV 4/5; 7/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam\n",
      "[CV 4/5; 7/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam;, score=0.647 total time=   1.3s\n",
      "[CV 5/5; 7/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam\n",
      "[CV 5/5; 7/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam;, score=0.808 total time=   1.4s\n",
      "[CV 1/5; 8/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 8/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs;, score=0.849 total time=  52.4s\n",
      "[CV 2/5; 8/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs\n",
      "[CV 2/5; 8/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs;, score=0.836 total time=  50.7s\n",
      "[CV 3/5; 8/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 8/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs;, score=0.826 total time=  57.9s\n",
      "[CV 4/5; 8/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 8/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs;, score=0.839 total time=  51.5s\n",
      "[CV 5/5; 8/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 8/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs;, score=0.833 total time=  59.8s\n",
      "[CV 1/5; 9/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 9/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=lbfgs;, score=0.836 total time=  13.6s\n",
      "[CV 2/5; 9/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 9/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=lbfgs;, score=0.839 total time=  13.5s\n",
      "[CV 3/5; 9/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 9/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=lbfgs;, score=0.817 total time=  14.4s\n",
      "[CV 4/5; 9/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 9/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=lbfgs;, score=0.795 total time=  13.6s\n",
      "[CV 5/5; 9/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 9/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=lbfgs;, score=0.814 total time=  12.7s\n",
      "[CV 1/5; 10/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd\n",
      "[CV 1/5; 10/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd;, score=0.500 total time=   0.1s\n",
      "[CV 2/5; 10/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd\n",
      "[CV 2/5; 10/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd;, score=0.498 total time=   0.1s\n",
      "[CV 3/5; 10/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd\n",
      "[CV 3/5; 10/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd;, score=0.502 total time=   0.1s\n",
      "[CV 4/5; 10/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd\n",
      "[CV 4/5; 10/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd;, score=0.502 total time=   0.1s\n",
      "[CV 5/5; 10/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd\n",
      "[CV 5/5; 10/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd;, score=0.498 total time=   0.1s\n",
      "[CV 1/5; 11/100] START alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=adam\n",
      "[CV 1/5; 11/100] END alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=adam;, score=0.500 total time=   0.2s\n",
      "[CV 2/5; 11/100] START alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=adam\n",
      "[CV 2/5; 11/100] END alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=adam;, score=0.502 total time=   0.2s\n",
      "[CV 3/5; 11/100] START alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=adam\n",
      "[CV 3/5; 11/100] END alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=adam;, score=0.502 total time=   0.2s\n",
      "[CV 4/5; 11/100] START alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=adam\n",
      "[CV 4/5; 11/100] END alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=adam;, score=0.502 total time=   0.2s\n",
      "[CV 5/5; 11/100] START alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=adam\n",
      "[CV 5/5; 11/100] END alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=adam;, score=0.498 total time=   0.2s\n",
      "[CV 1/5; 12/100] START alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam\n",
      "[CV 1/5; 12/100] END alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam;, score=0.711 total time=   2.0s\n",
      "[CV 2/5; 12/100] START alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam\n",
      "[CV 2/5; 12/100] END alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam;, score=0.814 total time=   3.4s\n",
      "[CV 3/5; 12/100] START alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam\n",
      "[CV 3/5; 12/100] END alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam;, score=0.729 total time=   1.3s\n",
      "[CV 4/5; 12/100] START alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam\n",
      "[CV 4/5; 12/100] END alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam;, score=0.770 total time=   1.8s\n",
      "[CV 5/5; 12/100] START alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam\n",
      "[CV 5/5; 12/100] END alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam;, score=0.801 total time=   1.3s\n",
      "[CV 1/5; 13/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=adam\n",
      "[CV 1/5; 13/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=adam;, score=0.500 total time=   0.4s\n",
      "[CV 2/5; 13/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=adam\n",
      "[CV 2/5; 13/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=adam;, score=0.502 total time=   0.4s\n",
      "[CV 3/5; 13/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=adam\n",
      "[CV 3/5; 13/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=adam;, score=0.502 total time=   0.4s\n",
      "[CV 4/5; 13/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=adam\n",
      "[CV 4/5; 13/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=adam;, score=0.502 total time=   0.4s\n",
      "[CV 5/5; 13/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=adam\n",
      "[CV 5/5; 13/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=adam;, score=0.498 total time=   0.4s\n",
      "[CV 1/5; 14/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=sgd\n",
      "[CV 1/5; 14/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=sgd;, score=0.500 total time=   0.3s\n",
      "[CV 2/5; 14/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=sgd\n",
      "[CV 2/5; 14/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=sgd;, score=0.498 total time=   0.5s\n",
      "[CV 3/5; 14/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=sgd\n",
      "[CV 3/5; 14/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=sgd;, score=0.502 total time=   0.5s\n",
      "[CV 4/5; 14/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=sgd\n",
      "[CV 4/5; 14/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=sgd;, score=0.502 total time=   0.5s\n",
      "[CV 5/5; 14/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=sgd\n",
      "[CV 5/5; 14/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=sgd;, score=0.498 total time=   0.5s\n",
      "[CV 1/5; 15/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam\n",
      "[CV 1/5; 15/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam;, score=0.500 total time=   0.3s\n",
      "[CV 2/5; 15/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam\n",
      "[CV 2/5; 15/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam;, score=0.502 total time=   0.8s\n",
      "[CV 3/5; 15/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam\n",
      "[CV 3/5; 15/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam;, score=0.498 total time=   0.4s\n",
      "[CV 4/5; 15/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam\n",
      "[CV 4/5; 15/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam;, score=0.498 total time=   0.3s\n",
      "[CV 5/5; 15/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam\n",
      "[CV 5/5; 15/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam;, score=0.502 total time=   0.4s\n",
      "[CV 1/5; 16/100] START alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=sgd\n",
      "[CV 1/5; 16/100] END alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=sgd;, score=0.500 total time=   0.4s\n",
      "[CV 2/5; 16/100] START alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=sgd\n",
      "[CV 2/5; 16/100] END alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=sgd;, score=0.502 total time=   0.4s\n",
      "[CV 3/5; 16/100] START alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=sgd\n",
      "[CV 3/5; 16/100] END alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=sgd;, score=0.498 total time=   0.3s\n",
      "[CV 4/5; 16/100] START alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=sgd\n",
      "[CV 4/5; 16/100] END alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=sgd;, score=0.498 total time=   0.4s\n",
      "[CV 5/5; 16/100] START alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=sgd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 16/100] END alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=sgd;, score=0.498 total time=   0.3s\n",
      "[CV 1/5; 17/100] START alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam\n",
      "[CV 1/5; 17/100] END alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam;, score=0.755 total time=   1.5s\n",
      "[CV 2/5; 17/100] START alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam\n",
      "[CV 2/5; 17/100] END alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam;, score=0.751 total time=   1.2s\n",
      "[CV 3/5; 17/100] START alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam\n",
      "[CV 3/5; 17/100] END alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam;, score=0.716 total time=   1.7s\n",
      "[CV 4/5; 17/100] START alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam\n",
      "[CV 4/5; 17/100] END alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam;, score=0.763 total time=   1.6s\n",
      "[CV 5/5; 17/100] START alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam\n",
      "[CV 5/5; 17/100] END alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam;, score=0.697 total time=   0.8s\n",
      "[CV 1/5; 18/100] START alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 18/100] END alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=lbfgs;, score=0.799 total time=  19.0s\n",
      "[CV 2/5; 18/100] START alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 18/100] END alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=lbfgs;, score=0.798 total time=  22.4s\n",
      "[CV 3/5; 18/100] START alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 18/100] END alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=lbfgs;, score=0.795 total time=  18.6s\n",
      "[CV 4/5; 18/100] START alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 18/100] END alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=lbfgs;, score=0.789 total time=  19.7s\n",
      "[CV 5/5; 18/100] START alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 18/100] END alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=lbfgs;, score=0.823 total time=  18.7s\n",
      "[CV 1/5; 19/100] START alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam\n",
      "[CV 1/5; 19/100] END alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam;, score=0.739 total time=   1.3s\n",
      "[CV 2/5; 19/100] START alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam\n",
      "[CV 2/5; 19/100] END alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam;, score=0.694 total time=   1.4s\n",
      "[CV 3/5; 19/100] START alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam\n",
      "[CV 3/5; 19/100] END alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam;, score=0.574 total time=   1.8s\n",
      "[CV 4/5; 19/100] START alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam\n",
      "[CV 4/5; 19/100] END alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam;, score=0.722 total time=   2.4s\n",
      "[CV 5/5; 19/100] START alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam\n",
      "[CV 5/5; 19/100] END alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam;, score=0.656 total time=   1.4s\n",
      "[CV 1/5; 20/100] START alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 20/100] END alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=lbfgs;, score=0.786 total time=  19.9s\n",
      "[CV 2/5; 20/100] START alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 20/100] END alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=lbfgs;, score=0.826 total time=  22.0s\n",
      "[CV 3/5; 20/100] START alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 20/100] END alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=lbfgs;, score=0.808 total time=  21.0s\n",
      "[CV 4/5; 20/100] START alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 20/100] END alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=lbfgs;, score=0.789 total time=  22.8s\n",
      "[CV 5/5; 20/100] START alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 20/100] END alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=lbfgs;, score=0.833 total time=  19.8s\n",
      "[CV 1/5; 21/100] START alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd\n",
      "[CV 1/5; 21/100] END alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd;, score=0.500 total time=   0.5s\n",
      "[CV 2/5; 21/100] START alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd\n",
      "[CV 2/5; 21/100] END alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd;, score=0.498 total time=   0.5s\n",
      "[CV 3/5; 21/100] START alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd\n",
      "[CV 3/5; 21/100] END alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd;, score=0.502 total time=   0.6s\n",
      "[CV 4/5; 21/100] START alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd\n",
      "[CV 4/5; 21/100] END alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd;, score=0.498 total time=   0.5s\n",
      "[CV 5/5; 21/100] START alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd\n",
      "[CV 5/5; 21/100] END alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd;, score=0.498 total time=   0.5s\n",
      "[CV 1/5; 22/100] START alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam\n",
      "[CV 1/5; 22/100] END alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam;, score=0.572 total time=   0.9s\n",
      "[CV 2/5; 22/100] START alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam\n",
      "[CV 2/5; 22/100] END alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam;, score=0.618 total time=   1.4s\n",
      "[CV 3/5; 22/100] START alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam\n",
      "[CV 3/5; 22/100] END alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam;, score=0.644 total time=   1.1s\n",
      "[CV 4/5; 22/100] START alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam\n",
      "[CV 4/5; 22/100] END alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam;, score=0.700 total time=   0.6s\n",
      "[CV 5/5; 22/100] START alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam\n",
      "[CV 5/5; 22/100] END alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam;, score=0.713 total time=   1.0s\n",
      "[CV 1/5; 23/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 23/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=lbfgs;, score=0.836 total time=  49.1s\n",
      "[CV 2/5; 23/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 23/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=lbfgs;, score=0.839 total time=  47.5s\n",
      "[CV 3/5; 23/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 23/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=lbfgs;, score=0.836 total time=  53.5s\n",
      "[CV 4/5; 23/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 23/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=lbfgs;, score=0.855 total time=  56.7s\n",
      "[CV 5/5; 23/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 23/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=lbfgs;, score=0.845 total time=  52.6s\n",
      "[CV 1/5; 24/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=adam\n",
      "[CV 1/5; 24/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=adam;, score=0.500 total time=   0.5s\n",
      "[CV 2/5; 24/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=adam\n",
      "[CV 2/5; 24/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=adam;, score=0.502 total time=   0.5s\n",
      "[CV 3/5; 24/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=adam\n",
      "[CV 3/5; 24/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=adam;, score=0.502 total time=   0.4s\n",
      "[CV 4/5; 24/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=adam\n",
      "[CV 4/5; 24/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=adam;, score=0.498 total time=   0.5s\n",
      "[CV 5/5; 24/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=adam\n",
      "[CV 5/5; 24/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=adam;, score=0.498 total time=   0.8s\n",
      "[CV 1/5; 25/100] START alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam\n",
      "[CV 1/5; 25/100] END alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam;, score=0.500 total time=   0.2s\n",
      "[CV 2/5; 25/100] START alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam\n",
      "[CV 2/5; 25/100] END alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam;, score=0.502 total time=   0.3s\n",
      "[CV 3/5; 25/100] START alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam\n",
      "[CV 3/5; 25/100] END alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam;, score=0.502 total time=   0.5s\n",
      "[CV 4/5; 25/100] START alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam\n",
      "[CV 4/5; 25/100] END alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam;, score=0.502 total time=   0.3s\n",
      "[CV 5/5; 25/100] START alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam\n",
      "[CV 5/5; 25/100] END alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam;, score=0.498 total time=   0.3s\n",
      "[CV 1/5; 26/100] START alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd\n",
      "[CV 1/5; 26/100] END alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd;, score=0.500 total time=   0.4s\n",
      "[CV 2/5; 26/100] START alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd\n",
      "[CV 2/5; 26/100] END alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd;, score=0.498 total time=   0.3s\n",
      "[CV 3/5; 26/100] START alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd\n",
      "[CV 3/5; 26/100] END alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd;, score=0.498 total time=   0.4s\n",
      "[CV 4/5; 26/100] START alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd\n",
      "[CV 4/5; 26/100] END alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd;, score=0.502 total time=   0.4s\n",
      "[CV 5/5; 26/100] START alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd\n",
      "[CV 5/5; 26/100] END alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd;, score=0.498 total time=   0.4s\n",
      "[CV 1/5; 27/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam\n",
      "[CV 1/5; 27/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam;, score=0.726 total time=   1.3s\n",
      "[CV 2/5; 27/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam\n",
      "[CV 2/5; 27/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam;, score=0.593 total time=   2.2s\n",
      "[CV 3/5; 27/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam\n",
      "[CV 3/5; 27/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam;, score=0.722 total time=   1.4s\n",
      "[CV 4/5; 27/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam\n",
      "[CV 4/5; 27/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam;, score=0.719 total time=   1.2s\n",
      "[CV 5/5; 27/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam\n",
      "[CV 5/5; 27/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam;, score=0.697 total time=   1.5s\n",
      "[CV 1/5; 28/100] START alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=adam\n",
      "[CV 1/5; 28/100] END alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=adam;, score=0.500 total time=   0.3s\n",
      "[CV 2/5; 28/100] START alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=adam\n",
      "[CV 2/5; 28/100] END alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=adam;, score=0.502 total time=   0.2s\n",
      "[CV 3/5; 28/100] START alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=adam\n",
      "[CV 3/5; 28/100] END alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=adam;, score=0.498 total time=   0.4s\n",
      "[CV 4/5; 28/100] START alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=adam\n",
      "[CV 4/5; 28/100] END alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=adam;, score=0.498 total time=   0.2s\n",
      "[CV 5/5; 28/100] START alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=adam\n",
      "[CV 5/5; 28/100] END alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=adam;, score=0.498 total time=   0.2s\n",
      "[CV 1/5; 29/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd\n",
      "[CV 1/5; 29/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd;, score=0.500 total time=   0.1s\n",
      "[CV 2/5; 29/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd\n",
      "[CV 2/5; 29/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd;, score=0.502 total time=   0.1s\n",
      "[CV 3/5; 29/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd\n",
      "[CV 3/5; 29/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd;, score=0.498 total time=   0.1s\n",
      "[CV 4/5; 29/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd\n",
      "[CV 4/5; 29/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd;, score=0.498 total time=   0.1s\n",
      "[CV 5/5; 29/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd\n",
      "[CV 5/5; 29/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd;, score=0.502 total time=   0.1s\n",
      "[CV 1/5; 30/100] START alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 30/100] END alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs;, score=0.833 total time=  46.7s\n",
      "[CV 2/5; 30/100] START alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 30/100] END alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs;, score=0.833 total time= 1.4min\n",
      "[CV 3/5; 30/100] START alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 30/100] END alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs;, score=0.820 total time=  49.5s\n",
      "[CV 4/5; 30/100] START alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 30/100] END alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs;, score=0.836 total time=  47.6s\n",
      "[CV 5/5; 30/100] START alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 30/100] END alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs;, score=0.839 total time=  53.8s\n",
      "[CV 1/5; 31/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=sgd\n",
      "[CV 1/5; 31/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=sgd;, score=0.500 total time=   0.2s\n",
      "[CV 2/5; 31/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=sgd\n",
      "[CV 2/5; 31/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=sgd;, score=0.505 total time=   0.2s\n",
      "[CV 3/5; 31/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=sgd\n",
      "[CV 3/5; 31/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=sgd;, score=0.502 total time=   0.2s\n",
      "[CV 4/5; 31/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=sgd\n",
      "[CV 4/5; 31/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=sgd;, score=0.502 total time=   0.1s\n",
      "[CV 5/5; 31/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=sgd\n",
      "[CV 5/5; 31/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=sgd;, score=0.502 total time=   0.1s\n",
      "[CV 1/5; 32/100] START alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=sgd\n",
      "[CV 1/5; 32/100] END alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=sgd;, score=0.500 total time=   0.1s\n",
      "[CV 2/5; 32/100] START alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=sgd\n",
      "[CV 2/5; 32/100] END alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=sgd;, score=0.498 total time=   0.1s\n",
      "[CV 3/5; 32/100] START alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=sgd\n",
      "[CV 3/5; 32/100] END alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=sgd;, score=0.498 total time=   0.2s\n",
      "[CV 4/5; 32/100] START alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=sgd\n",
      "[CV 4/5; 32/100] END alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=sgd;, score=0.505 total time=   0.1s\n",
      "[CV 5/5; 32/100] START alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=sgd\n",
      "[CV 5/5; 32/100] END alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=sgd;, score=0.498 total time=   0.2s\n",
      "[CV 1/5; 33/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 33/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs;, score=0.836 total time=  53.0s\n",
      "[CV 2/5; 33/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 33/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs;, score=0.839 total time= 1.0min\n",
      "[CV 3/5; 33/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 33/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs;, score=0.836 total time=  57.2s\n",
      "[CV 4/5; 33/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 33/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs;, score=0.855 total time= 1.0min\n",
      "[CV 5/5; 33/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 33/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs;, score=0.845 total time= 1.1min\n",
      "[CV 1/5; 34/100] START alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=adam\n",
      "[CV 1/5; 34/100] END alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=adam;, score=0.500 total time=   9.2s\n",
      "[CV 2/5; 34/100] START alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=adam\n",
      "[CV 2/5; 34/100] END alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=adam;, score=0.502 total time=   0.3s\n",
      "[CV 3/5; 34/100] START alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=adam\n",
      "[CV 3/5; 34/100] END alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=adam;, score=0.502 total time=   0.3s\n",
      "[CV 4/5; 34/100] START alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=adam\n",
      "[CV 4/5; 34/100] END alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=adam;, score=0.502 total time=   0.3s\n",
      "[CV 5/5; 34/100] START alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=adam\n",
      "[CV 5/5; 34/100] END alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=adam;, score=0.498 total time=   0.4s\n",
      "[CV 1/5; 35/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=adam\n",
      "[CV 1/5; 35/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=adam;, score=0.500 total time=   7.4s\n",
      "[CV 2/5; 35/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=adam\n",
      "[CV 2/5; 35/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=adam;, score=0.502 total time=   0.6s\n",
      "[CV 3/5; 35/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=adam\n",
      "[CV 3/5; 35/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=adam;, score=0.498 total time=   0.7s\n",
      "[CV 4/5; 35/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=adam\n",
      "[CV 4/5; 35/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=adam;, score=0.502 total time=   4.0s\n",
      "[CV 5/5; 35/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=adam\n",
      "[CV 5/5; 35/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=adam;, score=0.498 total time=   6.6s\n",
      "[CV 1/5; 36/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 36/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs;, score=0.824 total time=  52.8s\n",
      "[CV 2/5; 36/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 36/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs;, score=0.842 total time=  48.1s\n",
      "[CV 3/5; 36/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 36/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs;, score=0.855 total time=  53.1s\n",
      "[CV 4/5; 36/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 36/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs;, score=0.836 total time= 1.1min\n",
      "[CV 5/5; 36/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 36/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs;, score=0.817 total time= 1.1min\n",
      "[CV 1/5; 37/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam\n",
      "[CV 1/5; 37/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam;, score=0.500 total time=   0.3s\n",
      "[CV 2/5; 37/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam\n",
      "[CV 2/5; 37/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam;, score=0.502 total time=   0.3s\n",
      "[CV 3/5; 37/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 37/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam;, score=0.502 total time=  25.9s\n",
      "[CV 4/5; 37/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam\n",
      "[CV 4/5; 37/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam;, score=0.498 total time=   0.3s\n",
      "[CV 5/5; 37/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam\n",
      "[CV 5/5; 37/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam;, score=0.502 total time=   4.2s\n",
      "[CV 1/5; 38/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 38/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs;, score=0.849 total time= 1.1min\n",
      "[CV 2/5; 38/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 38/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs;, score=0.836 total time= 1.1min\n",
      "[CV 3/5; 38/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 38/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs;, score=0.826 total time= 1.0min\n",
      "[CV 4/5; 38/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 38/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs;, score=0.839 total time= 1.0min\n",
      "[CV 5/5; 38/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 38/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs;, score=0.833 total time=  60.0s\n",
      "[CV 1/5; 39/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=sgd\n",
      "[CV 1/5; 39/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=sgd;, score=0.500 total time=   0.5s\n",
      "[CV 2/5; 39/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=sgd\n",
      "[CV 2/5; 39/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=sgd;, score=0.498 total time=   0.6s\n",
      "[CV 3/5; 39/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=sgd\n",
      "[CV 3/5; 39/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=sgd;, score=0.498 total time=   0.6s\n",
      "[CV 4/5; 39/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=sgd\n",
      "[CV 4/5; 39/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=sgd;, score=0.502 total time=   0.6s\n",
      "[CV 5/5; 39/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=sgd\n",
      "[CV 5/5; 39/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=sgd;, score=0.498 total time=   0.6s\n",
      "[CV 1/5; 40/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=sgd\n",
      "[CV 1/5; 40/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=sgd;, score=0.500 total time=   0.6s\n",
      "[CV 2/5; 40/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=sgd\n",
      "[CV 2/5; 40/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=sgd;, score=0.498 total time=   0.5s\n",
      "[CV 3/5; 40/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=sgd\n",
      "[CV 3/5; 40/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=sgd;, score=0.498 total time=   0.5s\n",
      "[CV 4/5; 40/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=sgd\n",
      "[CV 4/5; 40/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=sgd;, score=0.502 total time=   0.5s\n",
      "[CV 5/5; 40/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=sgd\n",
      "[CV 5/5; 40/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=sgd;, score=0.498 total time=   0.6s\n",
      "[CV 1/5; 41/100] START alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam\n",
      "[CV 1/5; 41/100] END alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam;, score=0.733 total time=   1.2s\n",
      "[CV 2/5; 41/100] START alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam\n",
      "[CV 2/5; 41/100] END alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam;, score=0.672 total time=   1.1s\n",
      "[CV 3/5; 41/100] START alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam\n",
      "[CV 3/5; 41/100] END alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam;, score=0.691 total time=   0.6s\n",
      "[CV 4/5; 41/100] START alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam\n",
      "[CV 4/5; 41/100] END alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam;, score=0.700 total time=   0.8s\n",
      "[CV 5/5; 41/100] START alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam\n",
      "[CV 5/5; 41/100] END alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam;, score=0.744 total time=   0.9s\n",
      "[CV 1/5; 42/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 42/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=lbfgs;, score=0.796 total time=  18.8s\n",
      "[CV 2/5; 42/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=lbfgs\n",
      "[CV 2/5; 42/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=lbfgs;, score=0.852 total time=  17.3s\n",
      "[CV 3/5; 42/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 42/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=lbfgs;, score=0.808 total time=  17.7s\n",
      "[CV 4/5; 42/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 42/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=lbfgs;, score=0.789 total time=  16.7s\n",
      "[CV 5/5; 42/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 42/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=lbfgs;, score=0.808 total time=  17.9s\n",
      "[CV 1/5; 43/100] START alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=sgd\n",
      "[CV 1/5; 43/100] END alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=sgd;, score=0.500 total time=   0.2s\n",
      "[CV 2/5; 43/100] START alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=sgd\n",
      "[CV 2/5; 43/100] END alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=sgd;, score=0.502 total time=   0.2s\n",
      "[CV 3/5; 43/100] START alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=sgd\n",
      "[CV 3/5; 43/100] END alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=sgd;, score=0.498 total time=   0.3s\n",
      "[CV 4/5; 43/100] START alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=sgd\n",
      "[CV 4/5; 43/100] END alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=sgd;, score=0.498 total time=   0.2s\n",
      "[CV 5/5; 43/100] START alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=sgd\n",
      "[CV 5/5; 43/100] END alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=sgd;, score=0.498 total time=   0.2s\n",
      "[CV 1/5; 44/100] START alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd\n",
      "[CV 1/5; 44/100] END alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd;, score=0.500 total time=   0.5s\n",
      "[CV 2/5; 44/100] START alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd\n",
      "[CV 2/5; 44/100] END alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd;, score=0.498 total time=   0.5s\n",
      "[CV 3/5; 44/100] START alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd\n",
      "[CV 3/5; 44/100] END alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd;, score=0.498 total time=   0.5s\n",
      "[CV 4/5; 44/100] START alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd\n",
      "[CV 4/5; 44/100] END alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd;, score=0.502 total time=   0.4s\n",
      "[CV 5/5; 44/100] START alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd\n",
      "[CV 5/5; 44/100] END alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd;, score=0.498 total time=   0.4s\n",
      "[CV 1/5; 45/100] START alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 45/100] END alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=lbfgs;, score=0.792 total time=  24.1s\n",
      "[CV 2/5; 45/100] START alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 45/100] END alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=lbfgs;, score=0.801 total time=  21.7s\n",
      "[CV 3/5; 45/100] START alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 45/100] END alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=lbfgs;, score=0.776 total time=  20.9s\n",
      "[CV 4/5; 45/100] START alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 45/100] END alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=lbfgs;, score=0.785 total time=  22.2s\n",
      "[CV 5/5; 45/100] START alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 45/100] END alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=lbfgs;, score=0.836 total time=  29.6s\n",
      "[CV 1/5; 46/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam\n",
      "[CV 1/5; 46/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam;, score=0.695 total time=   2.0s\n",
      "[CV 2/5; 46/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam\n",
      "[CV 2/5; 46/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam;, score=0.634 total time=   2.0s\n",
      "[CV 3/5; 46/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam\n",
      "[CV 3/5; 46/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam;, score=0.751 total time=   2.0s\n",
      "[CV 4/5; 46/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam\n",
      "[CV 4/5; 46/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam;, score=0.565 total time=   1.6s\n",
      "[CV 5/5; 46/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam\n",
      "[CV 5/5; 46/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=adam;, score=0.590 total time=   1.4s\n",
      "[CV 1/5; 47/100] START alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=adam\n",
      "[CV 1/5; 47/100] END alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=adam;, score=0.500 total time=   0.3s\n",
      "[CV 2/5; 47/100] START alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=adam\n",
      "[CV 2/5; 47/100] END alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=adam;, score=0.502 total time=   0.2s\n",
      "[CV 3/5; 47/100] START alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=adam\n",
      "[CV 3/5; 47/100] END alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=adam;, score=0.498 total time=   0.3s\n",
      "[CV 4/5; 47/100] START alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=adam\n",
      "[CV 4/5; 47/100] END alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=adam;, score=0.502 total time=   0.3s\n",
      "[CV 5/5; 47/100] START alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=adam\n",
      "[CV 5/5; 47/100] END alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=adam;, score=0.498 total time=   0.3s\n",
      "[CV 1/5; 48/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 48/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=lbfgs;, score=0.796 total time=  26.0s\n",
      "[CV 2/5; 48/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 48/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=lbfgs;, score=0.852 total time=  18.1s\n",
      "[CV 3/5; 48/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=lbfgs\n",
      "[CV 3/5; 48/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=lbfgs;, score=0.808 total time=  19.0s\n",
      "[CV 4/5; 48/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 48/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=lbfgs;, score=0.789 total time=  18.6s\n",
      "[CV 5/5; 48/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 48/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=lbfgs;, score=0.808 total time=  18.5s\n",
      "[CV 1/5; 49/100] START alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd\n",
      "[CV 1/5; 49/100] END alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd;, score=0.500 total time=   0.6s\n",
      "[CV 2/5; 49/100] START alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd\n",
      "[CV 2/5; 49/100] END alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd;, score=0.502 total time=   0.5s\n",
      "[CV 3/5; 49/100] START alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd\n",
      "[CV 3/5; 49/100] END alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd;, score=0.498 total time=   0.6s\n",
      "[CV 4/5; 49/100] START alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd\n",
      "[CV 4/5; 49/100] END alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd;, score=0.502 total time=   0.6s\n",
      "[CV 5/5; 49/100] START alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd\n",
      "[CV 5/5; 49/100] END alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd;, score=0.508 total time=   0.6s\n",
      "[CV 1/5; 50/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=sgd\n",
      "[CV 1/5; 50/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=sgd;, score=0.500 total time=   0.2s\n",
      "[CV 2/5; 50/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=sgd\n",
      "[CV 2/5; 50/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=sgd;, score=0.502 total time=   0.3s\n",
      "[CV 3/5; 50/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=sgd\n",
      "[CV 3/5; 50/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=sgd;, score=0.502 total time=   0.2s\n",
      "[CV 4/5; 50/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=sgd\n",
      "[CV 4/5; 50/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=sgd;, score=0.502 total time=   0.2s\n",
      "[CV 5/5; 50/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=sgd\n",
      "[CV 5/5; 50/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=sgd;, score=0.498 total time=   0.2s\n",
      "[CV 1/5; 51/100] START alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=adam\n",
      "[CV 1/5; 51/100] END alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=adam;, score=0.748 total time=   5.3s\n",
      "[CV 2/5; 51/100] START alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=adam\n",
      "[CV 2/5; 51/100] END alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=adam;, score=0.757 total time=   7.5s\n",
      "[CV 3/5; 51/100] START alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=adam\n",
      "[CV 3/5; 51/100] END alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=adam;, score=0.760 total time=   4.8s\n",
      "[CV 4/5; 51/100] START alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=adam\n",
      "[CV 4/5; 51/100] END alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=adam;, score=0.770 total time=   6.1s\n",
      "[CV 5/5; 51/100] START alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=adam\n",
      "[CV 5/5; 51/100] END alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=adam;, score=0.833 total time=   8.0s\n",
      "[CV 1/5; 52/100] START alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam\n",
      "[CV 1/5; 52/100] END alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam;, score=0.701 total time=   0.7s\n",
      "[CV 2/5; 52/100] START alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam\n",
      "[CV 2/5; 52/100] END alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam;, score=0.650 total time=   2.3s\n",
      "[CV 3/5; 52/100] START alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam\n",
      "[CV 3/5; 52/100] END alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam;, score=0.678 total time=   1.3s\n",
      "[CV 4/5; 52/100] START alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam\n",
      "[CV 4/5; 52/100] END alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam;, score=0.688 total time=   1.2s\n",
      "[CV 5/5; 52/100] START alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam\n",
      "[CV 5/5; 52/100] END alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam;, score=0.792 total time=   2.5s\n",
      "[CV 1/5; 53/100] START alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 53/100] END alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=lbfgs;, score=0.799 total time=  24.6s\n",
      "[CV 2/5; 53/100] START alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 53/100] END alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=lbfgs;, score=0.808 total time=  13.4s\n",
      "[CV 3/5; 53/100] START alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 53/100] END alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=lbfgs;, score=0.811 total time=  16.5s\n",
      "[CV 4/5; 53/100] START alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 53/100] END alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=lbfgs;, score=0.801 total time=  15.6s\n",
      "[CV 5/5; 53/100] START alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 53/100] END alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=lbfgs;, score=0.820 total time=  16.3s\n",
      "[CV 1/5; 54/100] START alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 54/100] END alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs;, score=0.833 total time=  58.8s\n",
      "[CV 2/5; 54/100] START alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 54/100] END alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs;, score=0.833 total time= 1.1min\n",
      "[CV 3/5; 54/100] START alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 54/100] END alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs;, score=0.820 total time=  58.1s\n",
      "[CV 4/5; 54/100] START alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 54/100] END alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs;, score=0.836 total time=  59.3s\n",
      "[CV 5/5; 54/100] START alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 54/100] END alpha=1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs;, score=0.839 total time=  58.0s\n",
      "[CV 1/5; 55/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd\n",
      "[CV 1/5; 55/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd;, score=0.500 total time=   0.2s\n",
      "[CV 2/5; 55/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd\n",
      "[CV 2/5; 55/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd;, score=0.498 total time=   0.3s\n",
      "[CV 3/5; 55/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd\n",
      "[CV 3/5; 55/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd;, score=0.502 total time=   0.2s\n",
      "[CV 4/5; 55/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd\n",
      "[CV 4/5; 55/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd;, score=0.502 total time=   0.2s\n",
      "[CV 5/5; 55/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd\n",
      "[CV 5/5; 55/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd;, score=0.498 total time=   0.2s\n",
      "[CV 1/5; 56/100] START alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=sgd\n",
      "[CV 1/5; 56/100] END alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=sgd;, score=0.500 total time=   0.3s\n",
      "[CV 2/5; 56/100] START alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=sgd\n",
      "[CV 2/5; 56/100] END alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=sgd;, score=0.498 total time=   0.3s\n",
      "[CV 3/5; 56/100] START alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=sgd\n",
      "[CV 3/5; 56/100] END alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=sgd;, score=0.498 total time=   0.3s\n",
      "[CV 4/5; 56/100] START alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=sgd\n",
      "[CV 4/5; 56/100] END alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=sgd;, score=0.505 total time=   0.3s\n",
      "[CV 5/5; 56/100] START alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=sgd\n",
      "[CV 5/5; 56/100] END alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=sgd;, score=0.502 total time=   0.2s\n",
      "[CV 1/5; 57/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 57/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=lbfgs;, score=0.843 total time=  17.0s\n",
      "[CV 2/5; 57/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 57/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=lbfgs;, score=0.804 total time=  17.3s\n",
      "[CV 3/5; 57/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 57/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=lbfgs;, score=0.792 total time=  16.7s\n",
      "[CV 4/5; 57/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 57/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=lbfgs;, score=0.779 total time=  17.6s\n",
      "[CV 5/5; 57/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 57/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=lbfgs;, score=0.830 total time=  17.4s\n",
      "[CV 1/5; 58/100] START alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd\n",
      "[CV 1/5; 58/100] END alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd;, score=0.500 total time=   0.3s\n",
      "[CV 2/5; 58/100] START alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd\n",
      "[CV 2/5; 58/100] END alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd;, score=0.498 total time=   0.3s\n",
      "[CV 3/5; 58/100] START alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd\n",
      "[CV 3/5; 58/100] END alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd;, score=0.502 total time=   0.2s\n",
      "[CV 4/5; 58/100] START alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd\n",
      "[CV 4/5; 58/100] END alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd;, score=0.502 total time=   0.2s\n",
      "[CV 5/5; 58/100] START alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd\n",
      "[CV 5/5; 58/100] END alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd;, score=0.498 total time=   0.4s\n",
      "[CV 1/5; 59/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam\n",
      "[CV 1/5; 59/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam;, score=0.714 total time=   4.6s\n",
      "[CV 2/5; 59/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam\n",
      "[CV 2/5; 59/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam;, score=0.688 total time=   2.5s\n",
      "[CV 3/5; 59/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam\n",
      "[CV 3/5; 59/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam;, score=0.744 total time=   2.3s\n",
      "[CV 4/5; 59/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam\n",
      "[CV 4/5; 59/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam;, score=0.681 total time=   1.5s\n",
      "[CV 5/5; 59/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam\n",
      "[CV 5/5; 59/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam;, score=0.719 total time=   1.6s\n",
      "[CV 1/5; 60/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 60/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=lbfgs;, score=0.824 total time=  59.4s\n",
      "[CV 2/5; 60/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 60/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=lbfgs;, score=0.842 total time= 1.1min\n",
      "[CV 3/5; 60/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 60/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=lbfgs;, score=0.855 total time=  59.0s\n",
      "[CV 4/5; 60/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 60/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=lbfgs;, score=0.836 total time=  57.5s\n",
      "[CV 5/5; 60/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 60/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=lbfgs;, score=0.817 total time=293.5min\n",
      "[CV 1/5; 61/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 61/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=lbfgs;, score=0.843 total time=   5.9s\n",
      "[CV 2/5; 61/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 61/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=lbfgs;, score=0.804 total time=   5.7s\n",
      "[CV 3/5; 61/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 61/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=lbfgs;, score=0.792 total time=   5.5s\n",
      "[CV 4/5; 61/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 61/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=lbfgs;, score=0.779 total time=   4.8s\n",
      "[CV 5/5; 61/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 61/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=lbfgs;, score=0.830 total time=   5.3s\n",
      "[CV 1/5; 62/100] START alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 62/100] END alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=lbfgs;, score=0.824 total time=  20.6s\n",
      "[CV 2/5; 62/100] START alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 62/100] END alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=lbfgs;, score=0.842 total time=  20.0s\n",
      "[CV 3/5; 62/100] START alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 62/100] END alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=lbfgs;, score=0.836 total time=  19.5s\n",
      "[CV 4/5; 62/100] START alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 62/100] END alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=lbfgs;, score=0.845 total time=  20.5s\n",
      "[CV 5/5; 62/100] START alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 62/100] END alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=lbfgs;, score=0.849 total time=  20.3s\n",
      "[CV 1/5; 63/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam\n",
      "[CV 1/5; 63/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam;, score=0.748 total time=   0.3s\n",
      "[CV 2/5; 63/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam\n",
      "[CV 2/5; 63/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam;, score=0.741 total time=   0.5s\n",
      "[CV 3/5; 63/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam\n",
      "[CV 3/5; 63/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam;, score=0.776 total time=   0.3s\n",
      "[CV 4/5; 63/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam\n",
      "[CV 4/5; 63/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam;, score=0.726 total time=   0.2s\n",
      "[CV 5/5; 63/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam\n",
      "[CV 5/5; 63/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam;, score=0.770 total time=   0.6s\n",
      "[CV 1/5; 64/100] START alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd\n",
      "[CV 1/5; 64/100] END alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd;, score=0.500 total time=   0.0s\n",
      "[CV 2/5; 64/100] START alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd\n",
      "[CV 2/5; 64/100] END alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd;, score=0.502 total time=   0.0s\n",
      "[CV 3/5; 64/100] START alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd\n",
      "[CV 3/5; 64/100] END alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd;, score=0.498 total time=   0.0s\n",
      "[CV 4/5; 64/100] START alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd\n",
      "[CV 4/5; 64/100] END alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd;, score=0.498 total time=   0.0s\n",
      "[CV 5/5; 64/100] START alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd\n",
      "[CV 5/5; 64/100] END alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd;, score=0.502 total time=   0.0s\n",
      "[CV 1/5; 65/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam\n",
      "[CV 1/5; 65/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam;, score=0.682 total time=   0.5s\n",
      "[CV 2/5; 65/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam\n",
      "[CV 2/5; 65/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam;, score=0.722 total time=   0.4s\n",
      "[CV 3/5; 65/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam\n",
      "[CV 3/5; 65/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam;, score=0.662 total time=   0.2s\n",
      "[CV 4/5; 65/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam\n",
      "[CV 4/5; 65/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam;, score=0.650 total time=   0.4s\n",
      "[CV 5/5; 65/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam\n",
      "[CV 5/5; 65/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam;, score=0.640 total time=   0.4s\n",
      "[CV 1/5; 66/100] START alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 66/100] END alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=lbfgs;, score=0.792 total time=   7.4s\n",
      "[CV 2/5; 66/100] START alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 66/100] END alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=lbfgs;, score=0.801 total time=   6.5s\n",
      "[CV 3/5; 66/100] START alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 66/100] END alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=lbfgs;, score=0.776 total time=   7.2s\n",
      "[CV 4/5; 66/100] START alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 66/100] END alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=lbfgs;, score=0.785 total time=   8.0s\n",
      "[CV 5/5; 66/100] START alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 66/100] END alpha=1, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=lbfgs;, score=0.836 total time=   8.4s\n",
      "[CV 1/5; 67/100] START alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=sgd\n",
      "[CV 1/5; 67/100] END alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=sgd;, score=0.500 total time=   0.0s\n",
      "[CV 2/5; 67/100] START alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=sgd\n",
      "[CV 2/5; 67/100] END alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=sgd;, score=0.498 total time=   0.0s\n",
      "[CV 3/5; 67/100] START alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=sgd\n",
      "[CV 3/5; 67/100] END alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=sgd;, score=0.498 total time=   0.0s\n",
      "[CV 4/5; 67/100] START alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=sgd\n",
      "[CV 4/5; 67/100] END alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=sgd;, score=0.498 total time=   0.0s\n",
      "[CV 5/5; 67/100] START alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=sgd\n",
      "[CV 5/5; 67/100] END alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=sgd;, score=0.502 total time=   0.0s\n",
      "[CV 1/5; 68/100] START alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 68/100] END alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs;, score=0.824 total time=  22.9s\n",
      "[CV 2/5; 68/100] START alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 68/100] END alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs;, score=0.842 total time=  22.3s\n",
      "[CV 3/5; 68/100] START alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 68/100] END alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs;, score=0.836 total time=  20.1s\n",
      "[CV 4/5; 68/100] START alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 68/100] END alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs;, score=0.845 total time=  20.2s\n",
      "[CV 5/5; 68/100] START alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 68/100] END alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs;, score=0.849 total time=  19.6s\n",
      "[CV 1/5; 69/100] START alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam\n",
      "[CV 1/5; 69/100] END alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam;, score=0.733 total time=   0.5s\n",
      "[CV 2/5; 69/100] START alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam\n",
      "[CV 2/5; 69/100] END alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam;, score=0.700 total time=   0.4s\n",
      "[CV 3/5; 69/100] START alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam\n",
      "[CV 3/5; 69/100] END alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam;, score=0.713 total time=   0.5s\n",
      "[CV 4/5; 69/100] START alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam\n",
      "[CV 4/5; 69/100] END alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam;, score=0.675 total time=   0.5s\n",
      "[CV 5/5; 69/100] START alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam\n",
      "[CV 5/5; 69/100] END alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam;, score=0.779 total time=   1.0s\n",
      "[CV 1/5; 70/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 70/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=lbfgs;, score=0.843 total time=   5.1s\n",
      "[CV 2/5; 70/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 70/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=lbfgs;, score=0.804 total time=   4.7s\n",
      "[CV 3/5; 70/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 70/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=lbfgs;, score=0.792 total time=   4.7s\n",
      "[CV 4/5; 70/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 70/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=lbfgs;, score=0.779 total time=   5.1s\n",
      "[CV 5/5; 70/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 70/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=lbfgs;, score=0.830 total time=   4.9s\n",
      "[CV 1/5; 71/100] START alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 71/100] END alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=lbfgs;, score=0.799 total time=   4.9s\n",
      "[CV 2/5; 71/100] START alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 71/100] END alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=lbfgs;, score=0.808 total time=   5.1s\n",
      "[CV 3/5; 71/100] START alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 71/100] END alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=lbfgs;, score=0.811 total time=   4.5s\n",
      "[CV 4/5; 71/100] START alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 71/100] END alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=lbfgs;, score=0.801 total time=   4.6s\n",
      "[CV 5/5; 71/100] START alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 71/100] END alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=lbfgs;, score=0.820 total time=   5.3s\n",
      "[CV 1/5; 72/100] START alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam\n",
      "[CV 1/5; 72/100] END alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam;, score=0.500 total time=   0.0s\n",
      "[CV 2/5; 72/100] START alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam\n",
      "[CV 2/5; 72/100] END alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam;, score=0.502 total time=   0.1s\n",
      "[CV 3/5; 72/100] START alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam\n",
      "[CV 3/5; 72/100] END alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam;, score=0.502 total time=   0.1s\n",
      "[CV 4/5; 72/100] START alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam\n",
      "[CV 4/5; 72/100] END alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam;, score=0.498 total time=   0.1s\n",
      "[CV 5/5; 72/100] START alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam\n",
      "[CV 5/5; 72/100] END alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam;, score=0.498 total time=   0.1s\n",
      "[CV 1/5; 73/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=adam\n",
      "[CV 1/5; 73/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=adam;, score=0.723 total time=   1.3s\n",
      "[CV 2/5; 73/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=adam\n",
      "[CV 2/5; 73/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=adam;, score=0.697 total time=   0.5s\n",
      "[CV 3/5; 73/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=adam\n",
      "[CV 3/5; 73/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=adam;, score=0.716 total time=   0.8s\n",
      "[CV 4/5; 73/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=adam\n",
      "[CV 4/5; 73/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=adam;, score=0.767 total time=   1.3s\n",
      "[CV 5/5; 73/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=adam\n",
      "[CV 5/5; 73/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=adam;, score=0.795 total time=   1.1s\n",
      "[CV 1/5; 74/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd\n",
      "[CV 1/5; 74/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd;, score=0.500 total time=   0.0s\n",
      "[CV 2/5; 74/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd\n",
      "[CV 2/5; 74/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd;, score=0.502 total time=   0.0s\n",
      "[CV 3/5; 74/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd\n",
      "[CV 3/5; 74/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd;, score=0.498 total time=   0.0s\n",
      "[CV 4/5; 74/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd\n",
      "[CV 4/5; 74/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd;, score=0.498 total time=   0.0s\n",
      "[CV 5/5; 74/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd\n",
      "[CV 5/5; 74/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd;, score=0.502 total time=   0.0s\n",
      "[CV 1/5; 75/100] START alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd\n",
      "[CV 1/5; 75/100] END alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd;, score=0.500 total time=   0.0s\n",
      "[CV 2/5; 75/100] START alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd\n",
      "[CV 2/5; 75/100] END alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd;, score=0.502 total time=   0.0s\n",
      "[CV 3/5; 75/100] START alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd\n",
      "[CV 3/5; 75/100] END alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd;, score=0.498 total time=   0.0s\n",
      "[CV 4/5; 75/100] START alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd\n",
      "[CV 4/5; 75/100] END alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd;, score=0.498 total time=   0.0s\n",
      "[CV 5/5; 75/100] START alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd\n",
      "[CV 5/5; 75/100] END alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd;, score=0.502 total time=   0.0s\n",
      "[CV 1/5; 76/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 76/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs;, score=0.824 total time=  19.5s\n",
      "[CV 2/5; 76/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 76/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs;, score=0.842 total time=  19.8s\n",
      "[CV 3/5; 76/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 76/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs;, score=0.855 total time=  19.8s\n",
      "[CV 4/5; 76/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 76/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs;, score=0.836 total time=  19.8s\n",
      "[CV 5/5; 76/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 76/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=lbfgs;, score=0.817 total time=  20.1s\n",
      "[CV 1/5; 77/100] START alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 77/100] END alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs;, score=0.824 total time=  19.9s\n",
      "[CV 2/5; 77/100] START alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 77/100] END alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs;, score=0.842 total time=  20.4s\n",
      "[CV 3/5; 77/100] START alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 77/100] END alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs;, score=0.836 total time=  19.3s\n",
      "[CV 4/5; 77/100] START alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 77/100] END alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs;, score=0.845 total time=  19.9s\n",
      "[CV 5/5; 77/100] START alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 77/100] END alpha=0.01, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=lbfgs;, score=0.849 total time=  20.0s\n",
      "[CV 1/5; 78/100] START alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 78/100] END alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=lbfgs;, score=0.799 total time=   7.2s\n",
      "[CV 2/5; 78/100] START alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 78/100] END alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=lbfgs;, score=0.798 total time=   6.6s\n",
      "[CV 3/5; 78/100] START alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 78/100] END alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=lbfgs;, score=0.795 total time=   7.4s\n",
      "[CV 4/5; 78/100] START alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 78/100] END alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=lbfgs;, score=0.789 total time=   6.7s\n",
      "[CV 5/5; 78/100] START alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 78/100] END alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.1, solver=lbfgs;, score=0.823 total time=   7.8s\n",
      "[CV 1/5; 79/100] START alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 79/100] END alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=lbfgs;, score=0.811 total time=   7.7s\n",
      "[CV 2/5; 79/100] START alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 79/100] END alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=lbfgs;, score=0.808 total time=   7.7s\n",
      "[CV 3/5; 79/100] START alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 79/100] END alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=lbfgs;, score=0.826 total time=   7.1s\n",
      "[CV 4/5; 79/100] START alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 79/100] END alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=lbfgs;, score=0.792 total time=   7.4s\n",
      "[CV 5/5; 79/100] START alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 79/100] END alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=lbfgs;, score=0.820 total time=   6.5s\n",
      "[CV 1/5; 80/100] START alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd\n",
      "[CV 1/5; 80/100] END alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd;, score=0.500 total time=   0.0s\n",
      "[CV 2/5; 80/100] START alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd\n",
      "[CV 2/5; 80/100] END alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd;, score=0.498 total time=   0.0s\n",
      "[CV 3/5; 80/100] START alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd\n",
      "[CV 3/5; 80/100] END alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd;, score=0.502 total time=   0.0s\n",
      "[CV 4/5; 80/100] START alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd\n",
      "[CV 4/5; 80/100] END alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd;, score=0.502 total time=   0.0s\n",
      "[CV 5/5; 80/100] START alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd\n",
      "[CV 5/5; 80/100] END alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=sgd;, score=0.498 total time=   0.0s\n",
      "[CV 1/5; 81/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam\n",
      "[CV 1/5; 81/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam;, score=0.500 total time=   5.1s\n",
      "[CV 2/5; 81/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam\n",
      "[CV 2/5; 81/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam;, score=0.502 total time=   4.3s\n",
      "[CV 3/5; 81/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam\n",
      "[CV 3/5; 81/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam;, score=0.502 total time=   0.8s\n",
      "[CV 4/5; 81/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam\n",
      "[CV 4/5; 81/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam;, score=0.498 total time=   3.6s\n",
      "[CV 5/5; 81/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam\n",
      "[CV 5/5; 81/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=adam;, score=0.498 total time=   0.0s\n",
      "[CV 1/5; 82/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd\n",
      "[CV 1/5; 82/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd;, score=0.500 total time=   0.2s\n",
      "[CV 2/5; 82/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd\n",
      "[CV 2/5; 82/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd;, score=0.502 total time=   0.1s\n",
      "[CV 3/5; 82/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd\n",
      "[CV 3/5; 82/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd;, score=0.502 total time=   0.1s\n",
      "[CV 4/5; 82/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd\n",
      "[CV 4/5; 82/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd;, score=0.502 total time=   0.2s\n",
      "[CV 5/5; 82/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd\n",
      "[CV 5/5; 82/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd;, score=0.498 total time=   0.2s\n",
      "[CV 1/5; 83/100] START alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam\n",
      "[CV 1/5; 83/100] END alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam;, score=0.676 total time=   0.8s\n",
      "[CV 2/5; 83/100] START alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam\n",
      "[CV 2/5; 83/100] END alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam;, score=0.770 total time=   0.8s\n",
      "[CV 3/5; 83/100] START alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam\n",
      "[CV 3/5; 83/100] END alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam;, score=0.678 total time=   0.4s\n",
      "[CV 4/5; 83/100] START alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam\n",
      "[CV 4/5; 83/100] END alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam;, score=0.703 total time=   0.8s\n",
      "[CV 5/5; 83/100] START alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam\n",
      "[CV 5/5; 83/100] END alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam;, score=0.713 total time=   0.9s\n",
      "[CV 1/5; 84/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd\n",
      "[CV 1/5; 84/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd;, score=0.500 total time=   0.1s\n",
      "[CV 2/5; 84/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd\n",
      "[CV 2/5; 84/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd;, score=0.498 total time=   0.1s\n",
      "[CV 3/5; 84/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd\n",
      "[CV 3/5; 84/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd;, score=0.498 total time=   0.1s\n",
      "[CV 4/5; 84/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd\n",
      "[CV 4/5; 84/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd;, score=0.502 total time=   0.1s\n",
      "[CV 5/5; 84/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd\n",
      "[CV 5/5; 84/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd;, score=0.498 total time=   0.1s\n",
      "[CV 1/5; 85/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=sgd\n",
      "[CV 1/5; 85/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=sgd;, score=0.500 total time=   0.0s\n",
      "[CV 2/5; 85/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=sgd\n",
      "[CV 2/5; 85/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=sgd;, score=0.498 total time=   0.0s\n",
      "[CV 3/5; 85/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=sgd\n",
      "[CV 3/5; 85/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=sgd;, score=0.502 total time=   0.0s\n",
      "[CV 4/5; 85/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=sgd\n",
      "[CV 4/5; 85/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=sgd;, score=0.502 total time=   0.1s\n",
      "[CV 5/5; 85/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=sgd\n",
      "[CV 5/5; 85/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.2, solver=sgd;, score=0.498 total time=   0.1s\n",
      "[CV 1/5; 86/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd\n",
      "[CV 1/5; 86/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd;, score=0.500 total time=   0.1s\n",
      "[CV 2/5; 86/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd\n",
      "[CV 2/5; 86/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd;, score=0.498 total time=   0.1s\n",
      "[CV 3/5; 86/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd\n",
      "[CV 3/5; 86/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd;, score=0.498 total time=   0.1s\n",
      "[CV 4/5; 86/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd\n",
      "[CV 4/5; 86/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd;, score=0.502 total time=   0.1s\n",
      "[CV 5/5; 86/100] START alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 86/100] END alpha=0.001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd;, score=0.498 total time=   0.1s\n",
      "[CV 1/5; 87/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 87/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=lbfgs;, score=0.796 total time=   5.5s\n",
      "[CV 2/5; 87/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 87/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=lbfgs;, score=0.852 total time=   4.6s\n",
      "[CV 3/5; 87/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 87/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=lbfgs;, score=0.808 total time=   4.5s\n",
      "[CV 4/5; 87/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 87/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=lbfgs;, score=0.789 total time=   5.0s\n",
      "[CV 5/5; 87/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 87/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=lbfgs;, score=0.808 total time=   4.7s\n",
      "[CV 1/5; 88/100] START alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam\n",
      "[CV 1/5; 88/100] END alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam;, score=0.720 total time=   0.3s\n",
      "[CV 2/5; 88/100] START alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam\n",
      "[CV 2/5; 88/100] END alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam;, score=0.732 total time=   0.3s\n",
      "[CV 3/5; 88/100] START alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam\n",
      "[CV 3/5; 88/100] END alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam;, score=0.710 total time=   0.5s\n",
      "[CV 4/5; 88/100] START alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam\n",
      "[CV 4/5; 88/100] END alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam;, score=0.703 total time=   0.1s\n",
      "[CV 5/5; 88/100] START alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam\n",
      "[CV 5/5; 88/100] END alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate_init=0.01, solver=adam;, score=0.609 total time=   0.2s\n",
      "[CV 1/5; 89/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd\n",
      "[CV 1/5; 89/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd;, score=0.500 total time=   0.1s\n",
      "[CV 2/5; 89/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd\n",
      "[CV 2/5; 89/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd;, score=0.498 total time=   0.1s\n",
      "[CV 3/5; 89/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd\n",
      "[CV 3/5; 89/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd;, score=0.498 total time=   0.0s\n",
      "[CV 4/5; 89/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd\n",
      "[CV 4/5; 89/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd;, score=0.502 total time=   0.1s\n",
      "[CV 5/5; 89/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd\n",
      "[CV 5/5; 89/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.01, solver=sgd;, score=0.498 total time=   0.0s\n",
      "[CV 1/5; 90/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd\n",
      "[CV 1/5; 90/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd;, score=0.500 total time=   0.0s\n",
      "[CV 2/5; 90/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd\n",
      "[CV 2/5; 90/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd;, score=0.502 total time=   0.0s\n",
      "[CV 3/5; 90/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd\n",
      "[CV 3/5; 90/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd;, score=0.498 total time=   0.0s\n",
      "[CV 4/5; 90/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd\n",
      "[CV 4/5; 90/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd;, score=0.498 total time=   0.0s\n",
      "[CV 5/5; 90/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd\n",
      "[CV 5/5; 90/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=sgd;, score=0.502 total time=   0.0s\n",
      "[CV 1/5; 91/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=adam\n",
      "[CV 1/5; 91/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=adam;, score=0.500 total time=   0.1s\n",
      "[CV 2/5; 91/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=adam\n",
      "[CV 2/5; 91/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=adam;, score=0.498 total time=   0.1s\n",
      "[CV 3/5; 91/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=adam\n",
      "[CV 3/5; 91/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=adam;, score=0.502 total time=   0.1s\n",
      "[CV 4/5; 91/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=adam\n",
      "[CV 4/5; 91/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=adam;, score=0.498 total time=   0.1s\n",
      "[CV 5/5; 91/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=adam\n",
      "[CV 5/5; 91/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=adam;, score=0.502 total time=   0.1s\n",
      "[CV 1/5; 92/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=sgd\n",
      "[CV 1/5; 92/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=sgd;, score=0.500 total time=   0.0s\n",
      "[CV 2/5; 92/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=sgd\n",
      "[CV 2/5; 92/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=sgd;, score=0.498 total time=   0.1s\n",
      "[CV 3/5; 92/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=sgd\n",
      "[CV 3/5; 92/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=sgd;, score=0.498 total time=   0.1s\n",
      "[CV 4/5; 92/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=sgd\n",
      "[CV 4/5; 92/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=sgd;, score=0.498 total time=   0.1s\n",
      "[CV 5/5; 92/100] START alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=sgd\n",
      "[CV 5/5; 92/100] END alpha=0.1, hidden_layer_sizes=(200, 200), learning_rate_init=0.1, solver=sgd;, score=0.498 total time=   0.1s\n",
      "[CV 1/5; 93/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd\n",
      "[CV 1/5; 93/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd;, score=0.500 total time=   0.2s\n",
      "[CV 2/5; 93/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd\n",
      "[CV 2/5; 93/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd;, score=0.498 total time=   0.1s\n",
      "[CV 3/5; 93/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd\n",
      "[CV 3/5; 93/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd;, score=0.502 total time=   0.2s\n",
      "[CV 4/5; 93/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd\n",
      "[CV 4/5; 93/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd;, score=0.502 total time=   0.2s\n",
      "[CV 5/5; 93/100] START alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd\n",
      "[CV 5/5; 93/100] END alpha=0.0001, hidden_layer_sizes=(200, 200), learning_rate_init=0.001, solver=sgd;, score=0.502 total time=   0.1s\n",
      "[CV 1/5; 94/100] START alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 94/100] END alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=lbfgs;, score=0.808 total time=   7.0s\n",
      "[CV 2/5; 94/100] START alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 94/100] END alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=lbfgs;, score=0.795 total time=   6.5s\n",
      "[CV 3/5; 94/100] START alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 94/100] END alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=lbfgs;, score=0.795 total time=   7.3s\n",
      "[CV 4/5; 94/100] START alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 94/100] END alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=lbfgs;, score=0.804 total time=   6.7s\n",
      "[CV 5/5; 94/100] START alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 94/100] END alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate_init=0.2, solver=lbfgs;, score=0.842 total time=   7.0s\n",
      "[CV 1/5; 95/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam\n",
      "[CV 1/5; 95/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam;, score=0.689 total time=   0.7s\n",
      "[CV 2/5; 95/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam\n",
      "[CV 2/5; 95/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam;, score=0.779 total time=   0.5s\n",
      "[CV 3/5; 95/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam\n",
      "[CV 3/5; 95/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam;, score=0.656 total time=   0.3s\n",
      "[CV 4/5; 95/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam\n",
      "[CV 4/5; 95/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam;, score=0.767 total time=   0.3s\n",
      "[CV 5/5; 95/100] START alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam\n",
      "[CV 5/5; 95/100] END alpha=1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam;, score=0.716 total time=   0.6s\n",
      "[CV 1/5; 96/100] START alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam\n",
      "[CV 1/5; 96/100] END alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam;, score=0.500 total time=   0.0s\n",
      "[CV 2/5; 96/100] START alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam\n",
      "[CV 2/5; 96/100] END alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam;, score=0.502 total time=   0.0s\n",
      "[CV 3/5; 96/100] START alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam\n",
      "[CV 3/5; 96/100] END alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam;, score=0.498 total time=   0.0s\n",
      "[CV 4/5; 96/100] START alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam\n",
      "[CV 4/5; 96/100] END alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam;, score=0.498 total time=   0.0s\n",
      "[CV 5/5; 96/100] START alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam\n",
      "[CV 5/5; 96/100] END alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.2, solver=adam;, score=0.498 total time=   0.0s\n",
      "[CV 1/5; 97/100] START alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=sgd\n",
      "[CV 1/5; 97/100] END alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=sgd;, score=0.500 total time=   0.0s\n",
      "[CV 2/5; 97/100] START alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=sgd\n",
      "[CV 2/5; 97/100] END alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=sgd;, score=0.502 total time=   0.0s\n",
      "[CV 3/5; 97/100] START alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=sgd\n",
      "[CV 3/5; 97/100] END alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=sgd;, score=0.502 total time=   0.0s\n",
      "[CV 4/5; 97/100] START alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=sgd\n",
      "[CV 4/5; 97/100] END alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=sgd;, score=0.502 total time=   0.0s\n",
      "[CV 5/5; 97/100] START alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=sgd\n",
      "[CV 5/5; 97/100] END alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.1, solver=sgd;, score=0.498 total time=   0.0s\n",
      "[CV 1/5; 98/100] START alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam\n",
      "[CV 1/5; 98/100] END alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam;, score=0.692 total time=   0.4s\n",
      "[CV 2/5; 98/100] START alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam\n",
      "[CV 2/5; 98/100] END alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam;, score=0.751 total time=   0.4s\n",
      "[CV 3/5; 98/100] START alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam\n",
      "[CV 3/5; 98/100] END alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam;, score=0.691 total time=   0.3s\n",
      "[CV 4/5; 98/100] START alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam\n",
      "[CV 4/5; 98/100] END alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam;, score=0.744 total time=   0.2s\n",
      "[CV 5/5; 98/100] START alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam\n",
      "[CV 5/5; 98/100] END alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate_init=0.001, solver=adam;, score=0.719 total time=   0.2s\n",
      "[CV 1/5; 99/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam\n",
      "[CV 1/5; 99/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam;, score=0.708 total time=   0.4s\n",
      "[CV 2/5; 99/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam\n",
      "[CV 2/5; 99/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam;, score=0.685 total time=   0.5s\n",
      "[CV 3/5; 99/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam\n",
      "[CV 3/5; 99/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam;, score=0.694 total time=   0.5s\n",
      "[CV 4/5; 99/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam\n",
      "[CV 4/5; 99/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam;, score=0.669 total time=   0.2s\n",
      "[CV 5/5; 99/100] START alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam\n",
      "[CV 5/5; 99/100] END alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.01, solver=adam;, score=0.672 total time=   0.1s\n",
      "[CV 1/5; 100/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam\n",
      "[CV 1/5; 100/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam;, score=0.717 total time=   0.4s\n",
      "[CV 2/5; 100/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam\n",
      "[CV 2/5; 100/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam;, score=0.710 total time=   0.4s\n",
      "[CV 3/5; 100/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam\n",
      "[CV 3/5; 100/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam;, score=0.662 total time=   0.2s\n",
      "[CV 4/5; 100/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam\n",
      "[CV 4/5; 100/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam;, score=0.662 total time=   0.4s\n",
      "[CV 5/5; 100/100] START alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam\n",
      "[CV 5/5; 100/100] END alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate_init=0.001, solver=adam;, score=0.726 total time=   0.6s\n",
      "Best parameters found: {'solver': 'lbfgs', 'learning_rate_init': 0.2, 'hidden_layer_sizes': (200, 200), 'alpha': 0.1}\n",
      "Test Accuracy: 0.8413098236775819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define the parameter space to sample from\n",
    "param_dist = {\"hidden_layer_sizes\": [(50, 50, 50), (100, 100), (200, 200)],\n",
    "              \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "              \"solver\": [\"adam\", \"sgd\", \"lbfgs\"],\n",
    "              \"learning_rate_init\": [0.001, 0.01, 0.1, 0.2]}\n",
    "\n",
    "# Initialize the model\n",
    "mlp = MLPClassifier(max_iter=1000, random_state=42)\n",
    "\n",
    "# Perform the random search\n",
    "random_search = RandomizedSearchCV(mlp, param_distributions=param_dist, n_iter=100, cv=5, random_state=42, verbose=10)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found by the search\n",
    "print(\"Best parameters found:\", random_search.best_params_)\n",
    "\n",
    "# Test the best model\n",
    "best_model = random_search.best_estimator_\n",
    "test_accuracy = best_model.score(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "43fc57cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8790931989924433\n"
     ]
    }
   ],
   "source": [
    "# Separate data in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(learning_features, learning_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the labels\n",
    "labelEncoder = preprocessing.LabelEncoder().fit(y_train)\n",
    "learningLabelsStd = labelEncoder.transform(y_train)\n",
    "testLabelsStd = labelEncoder.transform(y_test)\n",
    "\n",
    "# Learn the model\n",
    "model = svm.SVC(C=30, kernel='rbf', class_weight=None, probability=False)\n",
    "scaler = preprocessing.StandardScaler(with_mean=True).fit(X_train)\n",
    "learningFeatures_scaled = scaler.transform(X_train)\n",
    "\n",
    "model.fit(learningFeatures_scaled, learningLabelsStd)\n",
    "\n",
    "# Test the model\n",
    "testFeatures_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = model.score(testFeatures_scaled, testLabelsStd)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6fbdd5",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33566468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8040c679",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1983, 1586]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_val_score\n\u001b[1;32m----> 3\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearningLabelsStd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(scores)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(scores))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    513\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 515\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:252\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcross_validate\u001b[39m(\n\u001b[0;32m     50\u001b[0m     estimator,\n\u001b[0;32m     51\u001b[0m     X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     63\u001b[0m     error_score\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan,\n\u001b[0;32m     64\u001b[0m ):\n\u001b[0;32m     65\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate metric(s) by cross-validation and also record fit/score times.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <multimetric_cross_validation>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;124;03m    [0.28009951 0.3908844  0.22784907]\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 252\u001b[0m     X, y, groups \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m     cv \u001b[38;5;241m=\u001b[39m check_cv(cv, y, classifier\u001b[38;5;241m=\u001b[39mis_classifier(estimator))\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callable(scoring):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:433\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    415\u001b[0m \n\u001b[0;32m    416\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    432\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 433\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:387\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    385\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 387\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    390\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1983, 1586]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model, learning_features, learningLabelsStd, cv=5)\n",
    "print(scores)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baf47ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
